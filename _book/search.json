[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis with Small Samples",
    "section": "",
    "text": "Preface\nThis book addresses a common challenge in applied research: how to conduct rigorous quantitative analysis when sample sizes are small. Whether you are studying remote communities, rare clinical conditions, pilot educational programs, or exploratory projects with limited resources, the principles and tools in this guide will help you make sound inferences from modest datasets.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Quantitative Analysis with Small Samples",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nThis book is written for undergraduates, taught masters students, and early-career PhD researchers in social sciences, health sciences, business, and education who regularly work with samples of approximately 10 to 100 observations. It is particularly relevant for:\n\nResearchers conducting studies in Small Island Developing States (SIDS) and similar resource-constrained contexts\nEducational practitioners evaluating classroom interventions with small class sizes\nHealth researchers studying rare conditions or conducting pilot clinical trials\nBusiness analysts testing new strategies in small markets or with limited customer bases\nSocial scientists conducting community-based participatory research",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Quantitative Analysis with Small Samples",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nYou will learn to:\n\nRecognise when small-sample methods are necessary and appropriate.\nApply exact tests, resampling methods, and rank-based procedures.\nFit regression models with penalised or Bayesian techniques when classical approaches fail.\nUse multi-criteria decision-making (MCDM) tools for structured evaluation with limited cases.\nReport results transparently, with appropriate uncertainty quantification.\n\nAll methods are implemented in R using a curated set of packages. Every code example is designed to run cleanly in a fresh R session, and datasets are small enough to inspect and understand directly.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Quantitative Analysis with Small Samples",
    "section": "Structure of the Book",
    "text": "Structure of the Book\nPart A: Foundations introduces the rationale for small-sample research and how to frame research questions that suit limited data.\nPart B: Data Collection and Preparation covers sampling strategies, measurement quality, and data checks tailored to small studies.\nPart C: Analysis Methods presents the core toolkit: exact and resampling tests, nonparametric methods, penalised and Bayesian regression, and MCDM techniques.\nPart D: Reporting and Interpretation discusses how to communicate findings, handle uncertainty, and document analytic choices.\nPart E: Worked Projects offers complete case studies that integrate multiple methods from earlier chapters.\n\nConcept Map: How the Parts Connect\nThe following diagram shows how the book’s parts build on one another and how specific chapters feed into the worked projects:\n\n\n\n\n\ngraph TD\n    A[Part A: Foundations&lt;br/&gt;Power & Design] --&gt; B[Part B: Data Collection&lt;br/&gt;Sampling & Quality]\n    B --&gt; C[Part C: Analysis Methods&lt;br/&gt;Ch 3-8]\n    C --&gt; D[Part D: Reporting&lt;br/&gt;Communication & Transparency]\n    A --&gt; C\n    C --&gt; E[Part E: Worked Projects&lt;br/&gt;Integrated Case Studies]\n    D --&gt; E\n    \n    C1[Ch 3: Exact Tests&lt;br/&gt;Fisher's, Binomial] --&gt; E1[Project 1:&lt;br/&gt;Clinical Trial Analysis]\n    C2[Ch 4: Nonparametric&lt;br/&gt;Mann-Whitney, Wilcoxon] --&gt; E1\n    C3[Ch 6: Reliability&lt;br/&gt;Cronbach's Alpha] --&gt; E2[Project 2:&lt;br/&gt;Scale Validation Study]\n    C4[Ch 4: Paired Tests&lt;br/&gt;Signed-Rank] --&gt; E3[Project 3:&lt;br/&gt;Intervention Evaluation]\n    C5[Ch 5: Regression&lt;br/&gt;Firth, Bayesian] --&gt; E3\n    \n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#ffe1f5\n    style D fill:#e1ffe1\n    style E fill:#f5e1ff\n    style E1 fill:#ffd4d4\n    style E2 fill:#ffd4d4\n    style E3 fill:#ffd4d4\n\n\n Book structure showing the flow from foundations through to integrated projects \n\n\n\nReading paths:\n\nLinear path: Work through Parts A → B → C → D → E sequentially for comprehensive coverage\nMethods-focused path: Start with Part C (Analysis Methods) if you’re already familiar with small-sample considerations\nProject-based path: Begin with Part E (Worked Projects) and refer back to specific chapters as needed\n\nEach chapter in Part C includes guided Lab Practicals that apply methods to realistic scenarios across education, business, health sciences, and social sciences contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#software-and-packages",
    "href": "index.html#software-and-packages",
    "title": "Quantitative Analysis with Small Samples",
    "section": "Software and Packages",
    "text": "Software and Packages\nAll analyses use R (version 4.0 or later) and Quarto for reproducible reporting. The following R packages are required:\n\ntidyverse: data manipulation and visualisation\nrstatix: common statistical tests with tidy output\nboot: bootstrap resampling\nexact2x2: exact tests for 2×2 tables\nlogistf: Firth-penalised logistic regression\nglmnet: ridge, lasso, and elastic net regression\nmediation: simple mediation analysis with bootstrap CIs\ngt: publication-ready tables\nperformance: model diagnostics\npsych: reliability and factor analysis\nDescTools: miscellaneous descriptive and test functions\npatchwork: combining plots\nbrms (optional): Bayesian regression with Stan\n\nYou can install all required packages by running:\ninstall.packages(c(\"tidyverse\", \"rstatix\", \"boot\", \"exact2x2\", \n                   \"logistf\", \"glmnet\", \"mediation\", \"gt\", \n                   \"performance\", \"psych\", \"DescTools\", \"patchwork\",\n                   \"brms\"))",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#conventions",
    "href": "index.html#conventions",
    "title": "Quantitative Analysis with Small Samples",
    "section": "Conventions",
    "text": "Conventions\n\nBritish English spelling and punctuation are used throughout.\nCode chunks include library() calls so each example can be run independently.\nRandom number generation uses set.seed(2025) for reproducibility.\nFile paths are relative to the project root.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Quantitative Analysis with Small Samples",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis guide draws on the work of many contributors to small-sample methodology. Core references include Van de Schoot and Miočević (2020), Davison and Hinkley (1997), Good (2005), Conover (1999), Firth (1993), Harrell (2015), Hosmer, Lemeshow and Sturdivant (2013), and Shan (2018). Full citations appear in the reference list.\n\nYou are encouraged to work through the chapters in order, running the code examples in your own R environment. The datasets and helper functions referenced in the text are provided in the data/ and R/ directories of this project.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/part-f-technical-appendices.html",
    "href": "chapters/part-f-technical-appendices.html",
    "title": "6  Part F: Technical Appendices",
    "section": "",
    "text": "7 Part F: Technical Appendices",
    "crumbs": [
      "Part F: Technical Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Part F: Technical Appendices</span>"
    ]
  },
  {
    "objectID": "chapters/part-f-technical-appendices.html#pre-registration-and-registered-reports-for-small-sample-studies",
    "href": "chapters/part-f-technical-appendices.html#pre-registration-and-registered-reports-for-small-sample-studies",
    "title": "6  Part F: Technical Appendices",
    "section": "7.1 Pre-Registration and Registered Reports for Small-Sample Studies",
    "text": "7.1 Pre-Registration and Registered Reports for Small-Sample Studies\n\n7.1.1 Why Pre-Registration Matters Even More with Small Samples\nPre-registration involves specifying research hypotheses, methods, and the analysis plan before collecting or analysing data. This practice is particularly important for small-sample studies because:\n\nLimited power increases temptation for p-hacking: With borderline results (p ~= 0.06), researchers may be tempted to try alternative analyses until significance is achieved.\nResearcher degrees of freedom are amplified: Small samples make results more sensitive to analytic choices (outlier handling, transformations, covariates).\nMultiple testing is common: With limited power, researchers may test many potential predictors or moderators.\nPost-hoc storytelling: It is easier to construct plausible narratives around unexpected findings when samples are small.\n\nPre-registration separates confirmatory from exploratory analyses, making the evidential value of findings transparent.\n\n\n7.1.2 What to Pre-Register\n\n7.1.2.1 Minimum Requirements\n\nResearch questions and hypotheses\n\nPrimary research question(s).\nSpecific hypotheses (directional if appropriate).\nPrimary vs. secondary outcomes.\n\nStudy design\n\nSample size and justification (power analysis or feasibility).\nSampling method.\nRandomisation procedure (if applicable).\nInclusion/exclusion criteria.\n\nVariables and measures\n\nHow each variable will be operationalised.\nScales and scoring procedures.\nDefinition of outliers and handling plan.\n\nAnalysis plan\n\nStatistical tests for each hypothesis.\nPlanned covariates and their justification.\nMultiple comparison corrections (if applicable).\nHandling of missing data.\nSensitivity analyses planned.\n\nDecision rules\n\nWhat constitutes support for hypotheses.\nConditions for stopping data collection (if sequential design).\n\n\n\n\n7.1.2.2 Example Pre-Registration Template for Small-Sample Study\n# Study Title: Effect of Brief Mindfulness Intervention on Test Anxiety\n\n## 1. Research Questions\n**Primary**: Does a 10-minute mindfulness exercise reduce test anxiety \ncompared to control condition in undergraduate students?\n\n**Secondary (Exploratory)**: \n- Does effect differ by prior meditation experience?\n- Does effect differ by baseline anxiety level?\n\n## 2. Hypotheses\n**H1 (Confirmatory)**: Students in mindfulness condition will report lower \ntest anxiety than control group (one-tailed, d ≥ 0.5 expected).\n\n**H2 (Exploratory)**: Effect will be stronger for students with no prior \nmeditation experience (interaction test, not corrected for multiple testing).\n\n## 3. Study Design\n- **Design**: Randomised controlled trial, parallel groups\n- **Sample size**: n = 40 (20 per group)\n  - Justification: 80% power to detect d = 0.9 at α = 0.05 (one-tailed)\n  - Feasible sample given time/resource constraints\n  - Pilot study, not definitive test\n- **Randomisation**: Simple randomisation using random number generator\n- **Inclusion**: Undergraduates enrolled in statistics course\n- **Exclusion**: Regular meditation practice (&gt;1x per week), clinical anxiety diagnosis\n\n## 4. Variables\n**Primary Outcome**: Test anxiety \n- Measure: Test Anxiety Inventory (TAI), 20 items, 4-point Likert\n- Scoring: Sum of items (range 20-80), higher = more anxiety\n- Timing: Administered immediately before exam\n\n**Secondary Measures**:\n- Prior meditation experience: Yes/No\n- Baseline trait anxiety: STAI-T (administered 1 week before)\n\n**Covariates**: None planned (randomisation should balance)\n\n## 5. Analysis Plan\n\n### Primary Analysis (Confirmatory)\n**Test**: Independent samples t-test (or Mann-Whitney U if severely non-normal)\n**Assumptions**: Check normality with Q-Q plots; if skewness &gt; |2|, use Mann-Whitney\n**Outliers**: Values &gt; 3 SD from group mean flagged; primary analysis includes all \ndata, sensitivity analysis excludes outliers\n**Alpha**: 0.05 (one-tailed, directional hypothesis)\n**Effect size**: Cohen's d with 95% CI\n**Missing data**: Complete-case analysis (expect &lt;5% missing)\n\n### Secondary Analyses (Exploratory)\n**Moderation by prior experience**: \n- Test: 2×2 ANOVA (condition × experience) or Kruskal-Wallis\n- No correction for multiple testing (exploratory)\n- Report: \"This analysis was exploratory and not pre-registered as confirmatory\"\n\n**Moderation by baseline anxiety**:\n- Test: Linear regression with interaction term\n- If interaction p &gt; 0.10, report main effects only\n\n### Sensitivity Analyses\n1. Analysis excluding outliers (if any)\n2. Analysis with baseline anxiety as covariate\n3. Complier-average causal effect (if non-compliance occurs)\n\n## 6. Decision Rules\n**Support for H1**: \n- One-tailed p &lt; 0.05 AND Cohen's d &gt; 0.4 (minimum practically important effect)\n- If p &lt; 0.05 but d &lt; 0.4: \"statistically significant but small effect\"\n- If p &gt; 0.05: \"No evidence for effect; study may be underpowered\"\n\n**Deviations from plan**:\nAny deviations from this pre-registration will be documented with justification.\n\n## 7. Timeline\n- Pre-registration posted: [Date]\n- Data collection: [Start] to [End]\n- Analysis: Within 2 weeks of completion\n- Results reported: By [Date]\n\n## 8. Data/Code Availability\n- De-identified data will be posted to OSF upon publication\n- Analysis scripts will be shared on GitHub\n\n\n\n7.1.3 Where to Pre-Register\n\nOpen Science Framework (OSF) - https://osf.io\n\nFree, widely used, time-stamped.\nCan be public or embargoed.\nIntegrated with many platforms.\nBest for: Most small-sample studies.\n\nAsPredicted - https://aspredicted.org\n\nSimpler interface, 9 required questions.\nEmbargoed for up to 4 years.\nQuick to complete (15-20 minutes).\nBest for: Student projects, pilot studies.\n\nClinicalTrials.gov - https://clinicaltrials.gov\n\nRequired for clinical trials in the US.\nMore complex registration.\nPublic database.\nBest for: Clinical or health research.\n\nRegistered Reports (Journal Format)\n\nSubmit protocol before data collection.\nIn-principle acceptance if methodology is sound.\nPublication guaranteed regardless of results.\nBest for: Confirmatory studies with novel questions.\n\n\n\n\n7.1.4 Implementing Pre-Registration: Practical Steps\n\nDraft your pre-registration (before data collection)\n\nSpecify exact hypotheses (include direction if predicted).\nDocument sample size with justification.\nOutline all planned analyses.\nDefine decision rules for interpreting results.\n\nGet feedback\n\nShare with advisors or colleagues.\nIdentify ambiguities or underspecified elements.\nRevise before finalising.\n\nSubmit and time-stamp\n\nUpload to OSF or AsPredicted.\nEmbargo if needed (set a reasonable end date).\nSave confirmation or DOI.\n\nFollow your plan\n\nConduct study as pre-registered.\nDocument any deviations with justification.\nDistinguish confirmatory from exploratory analyses in results.\n\nReport transparently\n\n\n“This study was pre-registered at [URL] on [Date]. All confirmatory analyses followed the pre-registered plan. The following deviations occurred: [list]. Exploratory analyses not included in the pre-registration are clearly labeled.”\n\n\n\n7.1.5 Handling Deviations from Pre-Registration\nDeviations are inevitable and acceptable if reported transparently.\n\n7.1.5.1 Acceptable deviations (with justification)\n\nSample size smaller than planned: e.g., recruitment difficulties.\nDifferent statistical test: e.g., assumptions violated.\nAdditional covariates: reviewer request or new information.\nExclusions: data quality issues not anticipated.\n\n\n\n7.1.5.2 How to report deviations\n**Deviations from Pre-Registration:**\n\n1. **Sample size**: Planned n=40, achieved n=36. Recruitment ended due to \n   course completion. Power reduced to 75% for d=0.9.\n\n2. **Primary analysis**: Pre-registered t-test replaced with Mann-Whitney U \n   due to severe right skew (skewness=2.4). Sensitivity analysis with \n   log-transformation yielded similar results.\n\n3. **Additional analysis**: Added baseline anxiety as covariate per reviewer \n   request. This was not pre-registered but is clearly labeled as post-hoc.\n\n\n\n7.1.6 Benefits of Pre-Registration for Small-Sample Studies\n\nCredibility\n\nReaders know analyses were not data-driven.\nReduces suspicion of p-hacking.\nDistinguishes confirmatory from exploratory work.\n\nProtection against publication bias\n\nRegistered reports guarantee publication regardless of results.\nReduces the file-drawer problem.\n\nConceptual clarity\n\nForces precise hypothesis formulation.\nIdentifies analytic ambiguities early.\nImproves overall research quality.\n\nEfficiency\n\nPre-planned analyses save time.\nReduces post-hoc decision-making stress.\nClear stopping rules prevent endless data collection.\n\n\n\n\n7.1.7 Common Concerns and Responses\n\n“But my study is exploratory…” – Pre-register it as exploratory and specify that you are exploring patterns rather than testing hypotheses.\n“What if I think of a better analysis after seeing the data?” – Conduct both analyses; report the pre-registered analysis as confirmatory and the new one as exploratory.\n“My sample size is already small; strict pre-registration seems excessive.” – Small samples make pre-registration more important because results are more variable.\n“I’m worried I’ll make mistakes in my pre-registration.” – Mistakes are acceptable if documented; deviations do not invalidate your study.\n\n\n\n7.1.8 Pre-Registration Checklist\nBefore submitting your pre-registration, verify:\n\nHypotheses are specific and testable.\nSample size is justified (power analysis or constraints explained).\nAll variables are operationally defined.\nStatistical tests are specified for each hypothesis.\nAlpha level(s) stated explicitly.\nMultiple comparison correction method stated (if applicable).\nOutlier definition and handling specified.\nMissing data approach specified.\nPrimary vs. secondary outcomes distinguished.\nExploratory analyses labeled as such.\nDocument is time-stamped before data collection or analysis.\n\n\n\n7.1.9 Example R Code for a Pre-Registered Analysis\n\n# ==========================================================\n# PRE-REGISTERED ANALYSIS SCRIPT\n# Study: Brief Mindfulness Intervention\n# Pre-registration: osf.io/xxxxx\n# Date: October 15, 2025\n# ==========================================================\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\n# Load data\nstudy_data &lt;- read_csv(\"data/anxiety_study.csv\")\n\n# ==========================================================\n# 1. SAMPLE VERIFICATION\n# ==========================================================\ncat(\"Pre-registered target: n = 40 (20 per group)\\n\")\ncat(\"Achieved sample size: n =\", nrow(study_data), \"\\n\")\ncat(\"By group:\\n\")\nprint(table(study_data$condition))\n\n# Check for protocol violations\ncat(\"\\nExclusions applied per pre-registration:\\n\")\ncat(\"  - Regular meditators:\", \n    sum(study_data$regular_meditation == \"Yes\"), \"excluded\\n\")\n\n# ==========================================================\n# 2. DESCRIPTIVE STATISTICS (PRE-REGISTERED)\n# ==========================================================\ncat(\"\\n=== DESCRIPTIVE STATISTICS ===\\n\")\ndesc_stats &lt;- study_data %&gt;%\n  group_by(condition) %&gt;%\n  summarise(\n    n = n(),\n    Mean_TAI = mean(test_anxiety),\n    SD_TAI = sd(test_anxiety),\n    Median_TAI = median(test_anxiety)\n  )\nprint(desc_stats)\n\n# ==========================================================\n# 3. ASSUMPTION CHECKS (PRE-REGISTERED)\n# ==========================================================\ncat(\"\\n=== ASSUMPTION CHECKS ===\\n\")\n\n# Normality check (per pre-registration: if |skewness| &gt; 2, use Mann-Whitney)\nskewness_vals &lt;- study_data %&gt;%\n  group_by(condition) %&gt;%\n  summarise(Skewness = psych::skew(test_anxiety))\nprint(skewness_vals)\n\nsevere_skew &lt;- any(abs(skewness_vals$Skewness) &gt; 2)\ncat(\"\\nSevere skewness detected:\", severe_skew, \"\\n\")\n\n# Q-Q plots\nggplot(study_data, aes(sample = test_anxiety)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~condition) +\n  labs(title = \"Q-Q Plots by Condition\")\n\n# Outlier detection (per pre-registration: &gt; 3 SD)\noutliers &lt;- study_data %&gt;%\n  group_by(condition) %&gt;%\n  mutate(\n    z_score = (test_anxiety - mean(test_anxiety)) / sd(test_anxiety),\n    is_outlier = abs(z_score) &gt; 3\n  ) %&gt;%\n  filter(is_outlier)\n\nif (nrow(outliers) &gt; 0) {\n  cat(\"\\nOutliers detected (&gt;3 SD):\\n\")\n  print(outliers)\n} else {\n  cat(\"\\nNo outliers detected.\\n\")\n}\n\n# ==========================================================\n# 4. PRIMARY ANALYSIS (PRE-REGISTERED, CONFIRMATORY)\n# ==========================================================\ncat(\"\\n=== PRIMARY ANALYSIS (CONFIRMATORY) ===\\n\")\ncat(\"Pre-registration: One-tailed t-test or Mann-Whitney U\\n\\n\")\n\nif (severe_skew) {\n  cat(\"Using Mann-Whitney U (severe skewness detected)\\n\")\n  primary_test &lt;- wilcox_test(study_data, test_anxiety ~ condition, \n                               alternative = \"less\", detailed = TRUE)\n  effect_size &lt;- wilcox_effsize(study_data, test_anxiety ~ condition)\n} else {\n  cat(\"Using independent samples t-test\\n\")\n  primary_test &lt;- t_test(study_data, test_anxiety ~ condition, \n                         alternative = \"less\", detailed = TRUE)\n  effect_size &lt;- cohens_d(study_data, test_anxiety ~ condition)\n}\n\nprint(primary_test)\nprint(effect_size)\n\n# Decision rule (per pre-registration)\ncat(\"\\n=== DECISION RULE ===\\n\")\ncat(\"Support for H1 requires: p &lt; 0.05 AND d &gt; 0.4\\n\")\ncat(\"Result: p =\", round(primary_test$p, 3), \n    \", effect size =\", round(abs(effect_size$effsize), 2), \"\\n\")\n\nif (primary_test$p &lt; 0.05 & abs(effect_size$effsize) &gt; 0.4) {\n  cat(\"CONCLUSION: Hypothesis supported\\n\")\n} else if (primary_test$p &lt; 0.05 & abs(effect_size$effsize) &lt;= 0.4) {\n  cat(\"CONCLUSION: Statistically significant but small effect\\n\")\n} else {\n  cat(\"CONCLUSION: No evidence for effect (may be underpowered)\\n\")\n}\n\n# ==========================================================\n# 5. SENSITIVITY ANALYSIS (PRE-REGISTERED)\n# ==========================================================\ncat(\"\\n=== SENSITIVITY ANALYSIS ===\\n\")\n\nif (nrow(outliers) &gt; 0) {\n  cat(\"5a. Analysis excluding outliers:\\n\")\n  data_no_outliers &lt;- anti_join(study_data, outliers, by = \"participant_id\")\n  sens_test &lt;- t_test(data_no_outliers, test_anxiety ~ condition, \n                      alternative = \"less\")\n  print(sens_test)\n}\n\n# Covariate adjustment\ncat(\"\\n5b. Adjusted for baseline anxiety:\\n\")\nancova_model &lt;- lm(test_anxiety ~ condition + baseline_anxiety, data = study_data)\nsummary(ancova_model)\n\n# ==========================================================\n# 6. EXPLORATORY ANALYSES (NOT PRE-REGISTERED)\n# ==========================================================\ncat(\"\\n=== EXPLORATORY ANALYSES (POST-HOC) ===\\n\")\ncat(\"WARNING: These were not pre-registered. Interpret cautiously.\\n\\n\")\n\n# Interaction with prior experience\ncat(\"6a. Moderation by prior meditation experience:\\n\")\nkruskal_test(study_data, test_anxiety ~ interaction(condition, prior_experience))\n\n# ==========================================================\n# END OF PRE-REGISTERED ANALYSIS\n# ==========================================================\ncat(\"\\n=== ANALYSIS COMPLETE ===\\n\")\ncat(\"All confirmatory analyses followed pre-registered plan.\\n\")\ncat(\"Deviations documented above.\\n\")\n\n\n\n7.1.10 Key Takeaways\n\nPre-registration protects against p-hacking, which is especially important for small samples.\nDistinguish confirmatory from exploratory analyses in both pre-registration and reporting.\nDeviations are acceptable if documented transparently with justification.\nOSF and AsPredicted offer free, time-stamped pre-registration options, and registered reports provide publication security.\nPre-register even exploratory studies to document your planned approach.\n\n\n\n7.1.11 Further Resources\n\nOSF Pre-registration: https://osf.io/prereg/\nAsPredicted: https://aspredicted.org/\nCenter for Open Science Guide: https://cos.io/prereg/\nNosek et al. (2018): The preregistration revolution. PNAS, 115(11), 2600–2606.\nSimmons et al. (2011): False-positive psychology. Psychological Science, 22(11), 1359–1366.",
    "crumbs": [
      "Part F: Technical Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Part F: Technical Appendices</span>"
    ]
  },
  {
    "objectID": "chapters/part-f-technical-appendices.html#teaching-supplement-quantitative-analysis-with-small-samples",
    "href": "chapters/part-f-technical-appendices.html#teaching-supplement-quantitative-analysis-with-small-samples",
    "title": "6  Part F: Technical Appendices",
    "section": "7.2 Teaching Supplement: Quantitative Analysis with Small Samples",
    "text": "7.2 Teaching Supplement: Quantitative Analysis with Small Samples\n\n7.2.1 Instructor’s Guide and Exercise Bank\n\n\n\n7.2.2 Part 1: Course Integration Suggestions\n\n7.2.2.1 Sample Course Structures\n\n7.2.2.1.1 Option 1: 10-Week Quarter Course — “Quantitative Methods for Small Samples”\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nChapters\nActivities\n\n\n\n\n1\nIntroduction & Foundations\nCh 1–2\nSetup R, generate datasets\n\n\n2\nSampling & Data Collection\nCh 9–10\nDesign a small-sample study\n\n\n3\nData Screening & Missing Data\nCh 11–12\nClean provided messy dataset\n\n\n4\nExact Tests & Resampling\nCh 3\nCompare exact vs. asymptotic\n\n\n5\nNonparametric Methods\nCh 4\nRank-based analyses lab\n\n\n6\nPenalized & Bayesian Regression\nCh 5\nFit Firth & brms models\n\n\n7\nReliability & MCDM\nCh 6–7\nScale development project\n\n\n8\nReporting & Visualization\nCh 13–16\nCreate publication-ready figures\n\n\n9\nWorked Project Analysis\nPart E\nWork through all three projects\n\n\n10\nStudent Presentations\n—\nPresent own small-sample study\n\n\n\nAssessment\n\nWeekly problem sets (40%).\nMid-term take-home exam (20%).\nFinal project: Analyse own small dataset (40%).\n\n\n\n7.2.2.1.2 Option 2: 5-Session Workshop Series — For Graduate Students or Practitioners\n\nSession 1 (3 hours): Foundations & Exact Tests — why small-sample methods matter, Fisher’s exact test, exact binomial/Poisson; hands-on 2×2 analyses.\nSession 2 (3 hours): Nonparametric & Resampling — Mann-Whitney U, Wilcoxon, bootstrap confidence intervals; compare parametric vs. nonparametric.\nSession 3 (3 hours): Regression for Small Samples — Firth logistic regression, Bayesian regression with brms; fit models with sparse data.\nSession 4 (3 hours): Reliability & Measurement — Cronbach’s alpha, omega, scale development; short-scale reliability lab.\nSession 5 (3 hours): Reporting & Integration — effect sizes, transparent reporting; complete a worked project.\n\n\n\n\n\n7.2.3 Part 2: Exercise Bank\nThe following exercises align with the chapters and include answer sketches to support instructors.\n\n7.2.3.1 Chapters 1–2 Exercises\nExercise 1.1 — Identifying Appropriate Methods\nFor each scenario, identify whether standard large-sample methods are appropriate or small-sample methods are needed. Justify your answer.\n\nA psychology study with n = 200 testing differences in reaction time between two groups.\nA pilot study of a new teaching method with n = 15 per class.\nAn evaluation of a rare disease treatment with n = 8 patients total.\nA survey of 500 customers rating satisfaction on a 1–10 scale.\nA comparison of defect rates in 12 manufacturing batches.\n\nAnswer key\n\n\nStandard methods are adequate (n = 200).\n\n\nSmall-sample methods needed (n = 15 per group).\n\n\nSmall-sample methods needed (n = 8 total).\n\n\nStandard methods are adequate (n = 500).\n\n\nSmall-sample methods needed (n = 12, count data).\n\n\nExercise 1.2 — Outcome Selection\nA researcher can measure student performance as binary (pass ≥ 60, fail &lt; 60), ordinal (grades A–F), or continuous (0–100 score). With n = 20 students, which outcome measure provides the most information? Use a simulation to illustrate why preserving granularity improves inference.\n\nset.seed(2025)\nn &lt;- 20\n\n# Simulate continuous scores\nscores &lt;- round(rnorm(n, mean = 68, sd = 12))\nscores &lt;- pmax(0, pmin(100, scores))\n\n# Create binary outcome\npass_fail &lt;- ifelse(scores &gt;= 60, \"Pass\", \"Fail\")\n\n# Create ordinal outcome\ngrades &lt;- cut(scores, breaks = c(0, 60, 70, 80, 90, 100),\n              labels = c(\"F\", \"D\", \"C\", \"B\", \"A\"), right = FALSE)\n\n# Compare information content\nmean(scores)\n\n[1] 70.35\n\ntable(pass_fail)\n\npass_fail\nFail Pass \n   2   18 \n\ntable(grades)\n\ngrades\nF D C B A \n2 8 7 2 1 \n\n\nEncourage students to discuss which summaries are most informative and how much precision is lost after dichotomising or coarsening data.\n\n\n7.2.3.2 Chapters 3–4 Exercises\nExercise 3.1 — Fisher’s Exact Test\nA small clinical trial tests a new therapy (n = 10) vs. standard care (n = 10). Success rates:\n\nNew therapy: 7 successes, 3 failures.\nStandard care: 3 successes, 7 failures.\n\nTasks:\n\nConstruct the 2×2 contingency table.\nCompute Fisher’s exact test in R.\nInterpret the odds ratio and 95% CI.\nCompare with the chi-square test and explain why chi-square is inappropriate.\n\n\ntherapy_success &lt;- c(7, 3)\ntherapy_fail &lt;- c(3, 7)\ntable_data &lt;- matrix(c(7, 3, 3, 7), nrow = 2)\n\n# Fisher's exact test\nfisher.test(table_data)\n\n# Chi-square (shows warning about small expected frequencies)\nchisq.test(table_data)\n\nExercise 4.1 — Choosing the Right Test\nSelect the appropriate test (t-test, Mann-Whitney U, or Wilcoxon signed-rank) for each scenario and justify your choice.\n\n\nCompare blood pressure in 15 patients before and after treatment (paired design, approximately normal).\n\n\nCompare job satisfaction (1–10 ordinal) between 12 remote and 12 office workers (right-skewed data).\n\n\nCompare customer ratings (1–5 stars) for two app versions, n = 20 per group, with a ceiling effect.\n\n\nExpected answers: (a) paired t-test, (b) Mann-Whitney U, (c) Mann-Whitney U.\n\n\n7.2.3.3 Chapters 5–6 Exercises\nExercise 5.1 — When Standard Logistic Regression Fails\nGenerate a dataset with separation to illustrate why Firth regression is helpful.\n\nset.seed(2025)\ndata_sep &lt;- data.frame(\n  x = c(1, 1, 2, 2, 3, 3, 4, 4, 5, 5),\n  y = c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)\n)\n\nglm_fit &lt;- glm(y ~ x, data = data_sep, family = binomial)\nsummary(glm_fit)\n\nlibrary(logistf)\nfirth_fit &lt;- logistf(y ~ x, data = data_sep)\nsummary(firth_fit)\n\nDiscuss:\n\nWhy standard GLM fails (infinite estimates with separation).\nHow Firth regression fixes the problem (penalised likelihood).\nWhen to use Firth regression (small samples, separation, rare events).\n\nExercise 6.1 — Reliability Analysis\nUsing reliability_exercise.csv (n = 25, 3 items):\n\nlibrary(tidyverse)\nlibrary(psych)\n\nitems &lt;- read_csv(\"data/reliability_exercise.csv\")\n\nalpha(items)\n\nQuestions:\n\nWhat is Cronbach’s alpha? Is it acceptable?\nWhich item, if any, should be removed to improve alpha?\nWhat is the 95% CI for alpha?\nHow would alpha change if two more items with similar correlations were added?\n\n\n\n7.2.3.4 Chapters 13–16 Exercises\nExercise 13.1 — Effect Size Interpretation\nA study with n = 15 per group finds a mean difference of 5 points on a 100-point scale, pooled SD = 12, Cohen’s d = 0.42, 95% CI [-2, 12], and p = 0.12. Write a results paragraph emphasising effect size and uncertainty.\nModel answer\n\nThe intervention group scored 5 points higher on average than the control group (95% CI: -2 to 12 points), corresponding to a small-to-medium effect (Cohen’s d = 0.42, 95% CI: -0.15 to 0.98). The confidence interval includes both trivial and moderate effects, indicating substantial uncertainty due to the limited sample size (n = 15 per group). The p-value (p = 0.12) does not provide evidence against the null hypothesis, but the study was underpowered to detect effects smaller than d ~= 1.0. These findings suggest a possible modest benefit that warrants investigation in a larger sample.\n\nExercise 16.1 — Visualisation Critique\n\n# Problematic visualisations\n# BAD: Suppressed zero axis\n# ggplot(data, aes(x = group, y = satisfaction)) +\n#   geom_col() +\n#   coord_cartesian(ylim = c(3.5, 4.5))\n\n# BAD: No error bars\n# ggplot(data, aes(x = group, y = satisfaction)) +\n#   geom_col()\n\n# Improved visualisation\n# ggplot(data, aes(x = group, y = satisfaction)) +\n#   geom_jitter(width = 0.1, alpha = 0.5) +\n#   stat_summary(fun = mean, geom = \"point\", size = 4, color = \"red\") +\n#   stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.2, color = \"red\")\n\nQuestions:\n\nWhat is wrong with the first two plots? (Suppressed axis exaggerates differences; no uncertainty markers.)\nWhy is showing individual points important with small samples? (Highlights distribution and outliers.)\nWhen, if ever, is it acceptable to suppress the zero axis? (Only when the zero point is distant and the focus is on relative differences, with clear justification.)\n\n\n\n\n7.2.4 Part 3: Assessment Materials\nQuiz 1 — Foundations (Chapters 1–2)\nMultiple choice:\n\nWhy might a t-test be inappropriate with n = 8 per group?\n\n\nNot enough degrees of freedom for any inference.\n\n\nCentral Limit Theorem does not guarantee a normal sampling distribution.\n\n\nEffect sizes cannot be computed.\n\n\nConfidence intervals are undefined.\n\nAnswer: b.\n\nWhich outcome type generally provides the most information per observation?\n\n\nBinary (yes/no).\n\n\nOrdinal (1–5 scale).\n\n\nContinuous (0–100 score).\n\n\nAll provide equal information.\n\nAnswer: c.\n\nA study with n = 20 per group has 30% power to detect d = 0.5. What does this mean?\n\n\nThere is a 30% chance the effect exists.\n\n\nIf the true effect is d = 0.5, there is a 30% chance of detecting it.\n\n\nThe study will have a 30% Type I error rate.\n\n\nThe effect size will be underestimated by 30%.\n\nAnswer: b.\n\n\nShort answer prompts:\n\nDescribe when it is acceptable to proceed with a small sample despite limited power (e.g., pilot studies, rare populations, ethical or logistical constraints).\nExplain why dichotomising a 0–100 outcome into high vs. low loses information and reduces statistical power (loss of variability, reduced effect size precision).\n\nQuiz 2 — Methods (Chapters 3–6)\nProblem-solving questions use quiz2_data.csv (ordinal outcome, n = 12 per group).\n\nConduct the appropriate test (likely Mann-Whitney U), compute effect size with CI, and interpret magnitude vs. p-value.\nCritique the statement “Cronbach’s alpha = 0.58 for a 3-item scale with n = 15. This is unacceptable.” Provide a more nuanced interpretation considering sample size, item count, and confidence intervals.\n\nFinal Project Rubric\n\n\n\n\n\n\n\n\nComponent\nPoints\nCriteria\n\n\n\n\nResearch Question\n10\nClear, focused, appropriate for small sample\n\n\nStudy Design\n10\nSampling method, sample size justification\n\n\nData Collection\n10\nMeasures described, reliability addressed\n\n\nData Screening\n10\nOutliers, assumptions, missing data checked\n\n\nPrimary Analysis\n20\nAppropriate method, correct implementation\n\n\nEffect Sizes & CIs\n15\nComputed and interpreted correctly\n\n\nVisualisation\n10\nPublication quality, uncertainty shown\n\n\nTransparent Reporting\n10\nLimitations acknowledged, decisions documented\n\n\nR Code Quality\n5\nReproducible, commented, clean\n\n\nTotal\n100\n\n\n\n\n\n\n7.2.5 Part 4: Common Student Mistakes and How to Address Them\n\n“My p-value is 0.06, so there’s a trend.” Emphasise effect sizes, uncertainty, and pre-registered thresholds rather than arbitrary cut-offs. Use an exercise that alters one data point to show p-value volatility.\nUsing t-tests with ordinal data without justification. Reinforce the assumptions of parametric tests and encourage rank-based alternatives.\n“No significant result means no effect.” Discuss power and the distinction between absence of evidence and evidence of absence. Use confidence intervals to highlight plausible effect ranges.\nAdding covariates after seeing results. Highlight the risk of p-hacking and connect back to pre-registration. Encourage labelling post-hoc analyses transparently.\nReporting only Cronbach’s alpha. Introduce omega and item-total correlations, especially for short scales where alpha assumptions may not hold.\n\n\n\n7.2.6 Part 5: Supplementary Datasets\nProvide students with realistic small-sample datasets for practice across education, business, health sciences, and operations contexts. Run the code below to generate CSV files in the data/ directory, or use source(\"R/make_datasets.R\") which generates all 12 datasets.\n\n7.2.6.1 Education Datasets (NEW)\n\n# Dataset 1: Classroom Reading Intervention (n = 22, paired pre/post)\n# Use case: Paired t-test, Wilcoxon signed-rank, effect sizes\nset.seed(2025)\nn_students &lt;- 22\nbaseline_reading &lt;- round(rnorm(n_students, mean = 65, sd = 12), 1)\nreading_gain &lt;- rnorm(n_students, mean = 8, sd = 5)\npost_reading &lt;- round(pmin(100, pmax(0, baseline_reading + reading_gain)), 1)\n\nclassroom_reading &lt;- tibble(\n  student_id = 1:n_students,\n  grade_level = sample(c(\"3rd\", \"4th\"), n_students, replace = TRUE),\n  pre_reading_score = baseline_reading,\n  post_reading_score = post_reading,\n  improvement = post_reading - baseline_reading,\n  teacher = sample(c(\"Ms. Johnson\", \"Mr. Lee\"), n_students, replace = TRUE)\n)\n\nwrite_csv(classroom_reading, \"data/classroom_reading.csv\")\n\n# Dataset 2: Peer Tutoring Comparison (n = 30, two instructional methods)\n# Use case: Independent t-test, Mann-Whitney U, effect sizes\nn_peer &lt;- 16\nn_teacher &lt;- 14\n\npeer_scores &lt;- round(rnorm(n_peer, mean = 78, sd = 9), 1)\nteacher_scores &lt;- round(rnorm(n_teacher, mean = 73, sd = 10), 1)\n\npeer_tutoring &lt;- tibble(\n  student_id = 1:(n_peer + n_teacher),\n  method = c(rep(\"Peer-Led\", n_peer), rep(\"Teacher-Led\", n_teacher)),\n  test_score = c(peer_scores, teacher_scores),\n  prior_gpa = round(runif(n_peer + n_teacher, min = 2.5, max = 4.0), 2),\n  attendance_rate = round(runif(n_peer + n_teacher, min = 0.75, max = 1.0), 2)\n)\n\nwrite_csv(peer_tutoring, \"data/peer_tutoring.csv\")\n\n# Dataset 3: Quiz Reliability (n = 28, five binary items)\n# Use case: KR-20 reliability, item analysis\nn_quiz &lt;- 28\nlatent_knowledge &lt;- rnorm(n_quiz, mean = 0.7, sd = 0.15)\n\nquiz_reliability &lt;- tibble(\n  student_id = 1:n_quiz,\n  item1 = rbinom(n_quiz, 1, pmin(0.95, pmax(0.05, latent_knowledge + rnorm(n_quiz, 0, 0.15)))),\n  item2 = rbinom(n_quiz, 1, pmin(0.95, pmax(0.05, latent_knowledge + rnorm(n_quiz, 0, 0.15)))),\n  item3 = rbinom(n_quiz, 1, pmin(0.95, pmax(0.05, latent_knowledge + rnorm(n_quiz, 0, 0.18)))),\n  item4 = rbinom(n_quiz, 1, pmin(0.95, pmax(0.05, latent_knowledge + rnorm(n_quiz, 0, 0.15)))),\n  item5 = rbinom(n_quiz, 1, pmin(0.95, pmax(0.05, latent_knowledge + rnorm(n_quiz, 0, 0.20))))\n)\n\nwrite_csv(quiz_reliability, \"data/quiz_reliability.csv\")\n\nExample Applications: - classroom_reading.csv: Evaluate literacy interventions, compare pre/post achievement - peer_tutoring.csv: Compare instructional strategies, assess pedagogical innovations - quiz_reliability.csv: Validate short classroom assessments, item analysis for formative tests - mediation_example.csv: Test mechanisms (e.g., does self-efficacy mediate intervention effects?)\n\n\n\n7.2.6.2 Psychology and Behavioral Sciences\nmediation_example.csv (n = 100)\n\n# Simulated data: Growth mindset intervention study\nset.seed(5678)\nmediation_example &lt;- tibble(\n  participant_id = 1:100,\n  intervention = rep(c(1, 0), each = 50),\n  self_efficacy = case_when(\n    intervention == 1 ~ rnorm(50, mean = 6.9, sd = 0.5),\n    TRUE ~ rnorm(50, mean = 4.2, sd = 0.4)\n  ),\n  exam_score = case_when(\n    intervention == 1 ~ round(65 + 4.5 * self_efficacy + rnorm(50, 0, 3)),\n    TRUE ~ round(65 + 4.5 * self_efficacy + rnorm(50, 0, 3))\n  ),\n  age = sample(20:23, 100, replace = TRUE),\n  gender = rep(c(\"Female\", \"Male\"), 50),\n  prior_gpa = round(rnorm(100, mean = 3.0, sd = 0.3), 1)\n)\n\nwrite_csv(mediation_example, \"data/mediation_example.csv\")\n\nPurpose: Demonstrate simple mediation analysis (X → M → Y) for understanding how interventions work. Tests whether self-efficacy mediates the effect of a growth mindset intervention on exam performance.\nVariables: - intervention: Treatment condition (1 = growth mindset program, 0 = control) - self_efficacy: Post-intervention self-efficacy (1-10 scale) - exam_score: Final exam score (0-100 points) - age, gender, prior_gpa: Covariates\nKey Features: - n = 100 (minimum for mediation analysis with bootstrap) - Temporal ordering: Intervention → Self-Efficacy (Week 6) → Exam (Week 8) - Strong indirect effect (~=79% of total effect mediated) - Used in Part E, Project 5: “Understanding Intervention Mechanisms”\nMethods Demonstrated: - Baron & Kenny approach (paths a, b, c, c’) - Bootstrap confidence intervals for indirect effects (mediation package) - Sensitivity analysis for unmeasured confounding - Covariate-adjusted mediation models\nSample Size Requirement: n ≥ 80-100 for adequate power to detect indirect effects via bootstrapping.\n\nEducation Applications: - classroom_reading.csv: Evaluate literacy interventions, compare pre/post achievement - peer_tutoring.csv: Compare instructional strategies, assess pedagogical innovations - quiz_reliability.csv: Validate short classroom assessments, item analysis for formative tests\n\n\n7.2.6.3 Health Sciences Datasets\n\n# Dataset 4: Hospital Readmissions (n = 25)\nset.seed(2025)\nhospital_data &lt;- tibble(\n  patient_id = 1:25,\n  age = sample(45:85, 25, replace = TRUE),\n  comorbidities = sample(0:4, 25, replace = TRUE),\n  length_of_stay = sample(2:10, 25, replace = TRUE),\n  readmitted_30d = c(1,1,1,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0)\n)\n\nwrite_csv(hospital_data, \"data/hospital_readmissions.csv\")\n\nHealth Applications: - hospital_readmissions.csv: Firth logistic regression for rare events, risk prediction models\n\n\n7.2.6.4 Business/Marketing Datasets\n\n# Dataset 5: Employee Engagement Survey (n = 36, 5 items)\n# Use case: Cronbach's alpha, reliability analysis\nn &lt;- 36\nlatent_engagement &lt;- rnorm(n, mean = 4, sd = 1)\n\nengagement_data &lt;- tibble(\n  employee_id = 1:n,\n  department = sample(c(\"Sales\", \"IT\", \"HR\"), n, replace = TRUE),\n  q1_satisfaction = round(pmin(7, pmax(1, latent_engagement + rnorm(n, 0, 0.9)))),\n  q2_commitment = round(pmin(7, pmax(1, latent_engagement + rnorm(n, 0, 0.8)))),\n  q3_advocacy = round(pmin(7, pmax(1, latent_engagement + rnorm(n, 0, 0.9)))),\n  q4_turnover_intent = round(pmin(7, pmax(1, 8 - latent_engagement + rnorm(n, 0, 1.0)))),\n  q5_performance = round(pmin(7, pmax(1, latent_engagement + rnorm(n, 0, 1.1))))\n)\n\nwrite_csv(engagement_data, \"data/employee_engagement.csv\")\n\n# Dataset 6: A/B Test with Sparse Conversions (n = 40, 4 conversions)\n# Use case: Fisher's exact test, exact binomial tests\nab_test_data &lt;- tibble(\n  user_id = 1:40,\n  variant = rep(c(\"A\", \"B\"), each = 20),\n  converted = c(rep(0, 17), rep(1, 3), rep(0, 18), rep(1, 2))\n)\n\nwrite_csv(ab_test_data, \"data/ab_test_sparse.csv\")\n\nBusiness Applications: - employee_engagement.csv: Scale validation, organizational surveys - ab_test_sparse.csv: Marketing experiments, conversion rate testing\nAdditional Datasets: See R/make_datasets.R for 6 more datasets covering operations (process improvement), service quality, clinical trials, and general exercises.\n\nEach dataset includes accompanying exercises in chapter narratives to reinforce method selection and interpretation across diverse domains: education, health sciences, business, and operations.\n\n\n\n7.2.7 Part 6: Advanced Topics Roadmap\nFor learners who master the core material, offer extension topics:\n\nBayesian Methods in Depth: Prior elicitation, MCMC diagnostics (R-hat, ESS, trace plots), posterior predictive checks, and model comparison with WAIC/LOOIC. (Reference: McElreath, 2020.)\nCausal Inference with Small Samples: Potential outcomes framework, propensity score matching with n &lt; 100, sensitivity analysis for unmeasured confounding, instrumental variables. (Reference: Imbens & Rubin, 2015.)\nMixed Effects Models: Random effects for clustered or longitudinal data, REML estimation, deciding when mixed models are appropriate. (Reference: Gelman & Hill, 2007.)\nMeasurement Error and Reliability: Classical test theory vs. item response theory, attenuation due to measurement error, corrections for unreliability. (Reference: Lord & Novick, 1968.)\nNetwork Analysis for Small Samples: Social network analysis with n &lt; 50, ERGMs, small-world and scale-free properties. (Reference: Lusher, Koskinen & Robins, 2013.)\n\n\n\n7.2.8 Part 7: Answers to Common Instructor Questions\n\nCan this book be used for undergraduates? Yes—advanced undergraduates with prior statistics coursework can engage with most chapters. Skip advanced Bayesian content if needed.\nHow much R background do students need? Basic literacy: loading data, running functions, interpreting tidy output. Provide introductory R sessions before starting.\nIs the book suitable beyond SIDS contexts? Absolutely. The methods apply to any setting where samples are limited (rare diseases, pilot studies, niche populations).\nWhat if students prefer Python? Concepts translate, but code will not run directly. Suggest scipy.stats, statsmodels, and pingouin for similar functionality, or encourage R for course activities.\nWhat if brms/Stan is unavailable? Focus on Firth regression and other penalised methods; use cloud platforms if Bayesian examples are essential.\nHow long does Part E take? Allocate 2–3 hours per project. Activities can be completed in lab sessions or as homework.\n\n\n\n7.2.9 Part 8: Teaching Tips\n\nStart with real examples: Open each session with a scenario where small samples are unavoidable (e.g., rare disease trials, startup A/B tests).\nEmphasise conceptual understanding over formulas: Students should know when and why to use each method.\nUse think–pair–share for interpretation: Build habits of collaborative sense-making.\nAssign code peer review: Students review each other’s scripts for reproducibility, clarity, and correctness.\nCreate a “gallery of bad visualisations”: Critique and improve misleading plots to build critical data-literacy skills.\n\n\n\n7.2.10 Conclusion\nThis teaching supplement provides:\n\nTwo ready-to-use course structures.\n20+ exercises with answer guidance.\nAssessment templates (quizzes, project rubric).\nCommon student misconceptions with interventions.\nSupplementary datasets and advanced topic suggestions.\nInstructor FAQs and practical teaching tips.",
    "crumbs": [
      "Part F: Technical Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Part F: Technical Appendices</span>"
    ]
  },
  {
    "objectID": "chapters/part-f-technical-appendices.html#complete-troubleshooting-guide",
    "href": "chapters/part-f-technical-appendices.html#complete-troubleshooting-guide",
    "title": "6  Part F: Technical Appendices",
    "section": "7.3 Complete Troubleshooting Guide",
    "text": "7.3 Complete Troubleshooting Guide\nQuantitative Analysis with Small Samples — Version 1.0\nLast updated: 15 October 2025\n\n7.3.1 Table of Contents\n\nInstallation Issues\nPackage Loading Errors\nData Import Problems\nCommon Statistical Errors\nVisualisation Issues\nBayesian/brms Specific Problems\nPlatform-Specific Issues\nPerformance Optimisation\n\n\n\n7.3.2 Installation Issues\n\n7.3.2.1 Problem 1.1 — Package installation fails\nWarning: package 'xxx' is not available for this version of R\nFix\n\nR.version.string\ninstall.packages(\"xxx\")\ninstall.packages(\"xxx\", repos = \"https://cran.rstudio.com/\")\n\nUpdate R if your version is below 4.0.\n\n\n7.3.2.2 Problem 1.2 — exact2x2 installation fails\npackage 'exact2x2' is not available for R version X.X.X\nFix\n\ninstall.packages(c(\"ssanv\", \"exactci\"))\ninstall.packages(\"exact2x2\")\n# Fallback: install from archive\nurl &lt;- \"https://cran.r-project.org/src/contrib/Archive/exact2x2/exact2x2_1.6.9.tar.gz\"\ninstall.packages(url, repos = NULL, type = \"source\")\n\n\n\n7.3.2.3 Problem 1.3 — logistf requires compilation (Windows)\ninstallation of package 'logistf' had non-zero exit status\nFix\n\nInstall Rtools: https://cran.r-project.org/bin/windows/Rtools/.\nThen run install.packages(\"logistf\", type = \"binary\").\n\nMac users should install Xcode Command Line Tools via xcode-select --install and then install the package.\n\n\n7.3.2.4 Problem 1.4 — brms/Stan installation issues\nError: CXX14 is not defined\nFix\n\ninstall.packages(\"rstan\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\ninstall.packages(\"brms\")\n\nIf compilation still fails, use RStudio Cloud or Docker images with Stan pre-configured, or treat Bayesian examples as optional.\n\n\n\n7.3.3 Package Loading Errors\n\n7.3.3.1 Problem 2.1 — Missing package during library()\nError in library(xxx) : there is no package called 'xxx'\nFix\n\ninstall.packages(\"xxx\")\nrequired &lt;- c(\"tidyverse\", \"rstatix\", \"boot\", \"exact2x2\", \n              \"logistf\", \"gt\", \"performance\", \"psych\", \"DescTools\")\ninstall.packages(required)\n\n\n\n7.3.3.2 Problem 2.2 — Namespace conflicts\nThe following objects are masked from 'package:stats': filter, lag\nFix\n\nThis is a warning, not an error.\nUse qualified names (e.g., stats::filter()) where needed.\nLoad tidyverse last to use its versions of overlapping functions.\n\n\n\n7.3.3.3 Problem 2.3 — Package version warnings\nWarning: package 'xxx' was built under R version 4.5.1\nFix\n\nUsually safe to ignore — packages are backward compatible.\nRun update.packages(ask = FALSE) or upgrade R to eliminate warnings.\n\n\n\n\n7.3.4 Data Import Problems\n\n7.3.4.1 Problem 3.1 — Missing file\nError: 'data/mini_marketing.csv' does not exist\nFix\n\ngetwd()\nsetwd(\"path/to/smallsamplelab\")\nsource(\"R/make_datasets.R\")\nlist.files(\"data\")\n\nEnsure the working directory is the project root (where smallstat.Rproj resides).\n\n\n7.3.4.2 Problem 3.2 — CSV encoding issues\nWarning: invalid UTF-8 byte sequence detected\nFix\n\nread_csv(\"data/file.csv\", locale = locale(encoding = \"UTF-8\"))\nread_csv(\"data/file.csv\", locale = locale(encoding = \"latin1\"))\n\n\n\n7.3.4.3 Problem 3.3 — Byte order marks\nSymptoms: column names such as ï»¿satisfaction.\nFix\n\nread_csv(\"data/file.csv\", skip_empty_rows = TRUE)\n# or\nlibrary(janitor)\ndata &lt;- read_csv(\"data/file.csv\") %&gt;% clean_names()\n\n\n\n\n7.3.5 Common Statistical Errors\n\n7.3.5.1 Problem 4.1 — “cannot compute exact p-value with ties”\nThis warning stems from discrete data or tied ranks. R automatically switches to a normal approximation.\nFix\n\nwilcox.test(x, y, exact = FALSE)\n\n\n\n7.3.5.2 Problem 4.2 — Firth regression does not converge\nWarning: algorithm did not converge\nFix\n\nlogistf(y ~ x, data = dat, maxit = 1000)\nlogistf(y ~ x, data = dat, pl = FALSE)\n\nInspect cross-tabulations for perfect separation.\n\n\n7.3.5.3 Problem 4.3 — Bootstrap CI fails\nError in boot.ci(...): estimated adjustment 'a' is NA\nFix\n\nboot_result &lt;- boot(data, statistic = fun, R = 5000)\nboot.ci(boot_result, type = \"basic\")\n\nIncrease replications or use a different CI type.\n\n\n7.3.5.4 Problem 4.4 — “system is computationally singular”\nOccurs when predictors are perfectly correlated or constant.\nFix\n\ncor(model_data[, c(\"x1\", \"x2\", \"x3\")])\nsapply(model_data, sd)\n\nRemove or combine collinear predictors, or switch to penalised regression.\n\n\n7.3.5.5 Problem 4.5 — Negative alpha\nA negative Cronbach’s alpha indicates reversed items or weak correlations.\nFix\n\nCheck inter-item correlations.\nConfirm reverse-coded items are recoded (item2_rev &lt;- max_scale + 1 - item2).\nRe-express or remove problematic items.\n\n\n\n\n7.3.6 Visualisation Issues\n\n7.3.6.1 Problem 5.1 — Aesthetic length mismatch\nError: Aesthetics must be either length 1 or the same as the data\nFix\n\ndim(plot_data)\nlength(plot_data$x)\nlength(plot_data$y)\nplot_data &lt;- plot_data %&gt;% filter(!is.na(x), !is.na(y))\n\n\n\n7.3.6.2 Problem 5.2 — Plots do not appear\n\nEnsure the RStudio Plots pane is visible.\nExplicitly print() stored ggplot objects in scripts.\nIn Quarto, set chunk options (fig.width, fig.height).\n\n\n\n7.3.6.3 Problem 5.3 — stat_summary missing\nLoad ggplot2 or tidyverse before using stat_summary.\n\n\n7.3.6.4 Problem 5.4 — Overlapping labels\nUse theme() adjustments to rotate text, reduce size, or expand margins.\n\n\n\n7.3.7 Bayesian/brms Specific Problems\n\n7.3.7.1 Problem 6.1 — No C++ compiler\nSee installation solutions above (Problem 1.4).\n\n\n7.3.7.2 Problem 6.2 — Chains do not converge (R-hat &gt; 1.01)\n\nIncrease iterations and warmup (iter = 4000, warmup = 2000).\nIncrease chains (≥ 4).\nAdjust adapt_delta = 0.95 for divergent transitions.\nInspect trace plots.\n\n\n\n7.3.7.3 Problem 6.3 — brms models are slow\n\nRun short chains for testing (iter = 500).\nUse multiple cores (cores = parallel::detectCores() - 1).\nSimplify models or postpone Bayesian analyses if performance is unacceptable.\n\n\n\n7.3.7.4 Problem 6.4 — Prior mismatch warnings\nUse get_prior() to inspect parameter names and match class/coef arguments.\n\n\n\n7.3.8 Platform-Specific Issues\n\n7.3.8.1 Problem 7.1 — Permission denied (Windows)\nRun RStudio as administrator or set the working directory to a user-controlled location (e.g., Documents).\n\n\n7.3.8.2 Problem 7.2 — File path issues (Windows)\nUse forward slashes (\"C:/Users/...\") or escape backslashes (\"C:\\\\Users\\\\...\").\n\n\n7.3.8.3 Problem 7.3 — Memory errors\nIncrease memory limit on Windows (memory.limit(size = 16000)), reduce bootstrap iterations, or sample smaller subsets for demonstrations.\n\n\n\n7.3.9 Performance Optimisation\n\n7.3.9.1 Tip 8.1 — Speed up bootstrap\n\nlibrary(boot)\nlibrary(parallel)\nboot(data, statistic = fun, R = 2000, parallel = \"multicore\", ncpus = detectCores() - 1)\n\nUse parallel = \"snow\" on Windows.\n\n\n7.3.9.2 Tip 8.2 — Multiple imputation performance\nUse fewer imputations for testing (m = 5, maxit = 5), and increase for final analyses (m = 20).\n\n\n7.3.9.3 Tip 8.3 — Efficient data manipulation\n\nlibrary(data.table)\ndt &lt;- as.data.table(data)\ndt[, mean(x), by = group]\n\n\n\n\n7.3.10 Emergency Fixes\n\n7.3.10.1 Fix 1 — Reset environment\n\nrm(list = ls())\n# Detach non-base packages\nlapply(paste0(\"package:\", names(sessionInfo()$otherPkgs)), detach, character.only = TRUE, unload = TRUE)\n# Restart R in RStudio\ntools::rstudioapi::restartSession()\n\n\n\n7.3.10.2 Fix 2 — Reinstall packages\n\ninstalled &lt;- rownames(installed.packages())\nremove.packages(installed)\nrequired &lt;- c(\"tidyverse\", \"rstatix\", \"boot\", \"exact2x2\", \n              \"logistf\", \"gt\", \"performance\", \"psych\", \"DescTools\")\ninstall.packages(required)\n\n\n\n7.3.10.3 Fix 3 — Capture diagnostics\n\nsink(\"diagnostics.txt\")\nsessionInfo()\nR.version\n.libPaths()\nSys.info()\nsink()\n\n\n\n\n7.3.11 Getting Help\nWhen requesting assistance, share:\n\nThe exact error message.\nA minimal reproducible example.\nSession information (sessionInfo()).\nTroubleshooting steps already attempted.\n\nSuggested resources include the book repository issue tracker, Stack Overflow ([r] tag), RStudio Community, and package documentation.\n\n\n7.3.12 Preventive Measures\n\nUse R projects (.Rproj) to manage working directories.\nDocument R and package versions at the top of scripts.\nConsider renv for reproducible environments.\nTest code in a fresh session (Session &gt; Restart R in RStudio).\nKeep R and packages up to date (update.packages).\n\n\n\n7.3.13 Final Checklist — “My Code Doesn’t Work”\nConfirm the following before escalating:\n\nR version ≥ 4.0 and RStudio current.\nRequired packages installed.\nWorking directory set to project root.\nDatasets present in data/.\nCode runs in a fresh session.\nComplete error message captured.\nSolutions from this guide attempted.\n\n\n\n7.3.14 Appendix — Package Version Matrix (October 2025)\n\n\n\nPackage\nVersion\nCritical?\n\n\n\n\nR\n4.5.0+\nYes\n\n\ntidyverse\n2.0.0+\nYes\n\n\nrstatix\n0.7.2+\nYes\n\n\nboot\n1.3-28+\nYes\n\n\nexact2x2\n1.6.9+\nYes\n\n\nlogistf\n1.26+\nYes\n\n\ngt\n0.10.0+\nYes\n\n\nperformance\n0.11.0+\nYes\n\n\npsych\n2.3.9+\nYes\n\n\nDescTools\n0.99.50+\nYes\n\n\nbrms\n2.20.0+\nOptional\n\n\nmice\n3.16.0+\nOptional\n\n\n\n\nThis appendix compiles pre-registration guidance, teaching resources, troubleshooting tips, and advanced topic pathways so instructors and researchers can implement small-sample methods confidently.",
    "crumbs": [
      "Part F: Technical Appendices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Part F: Technical Appendices</span>"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html",
    "href": "chapters/part-g-lab-practicals.html",
    "title": "Part G: Guided Lab Practicals",
    "section": "",
    "text": "Lab 1: Fisher’s Exact Test (Chapter 3)\nThis section provides hands-on, step-by-step lab walkthroughs for key statistical methods covered in the book. Each lab includes:\nHow to Use These Labs:\nLab Schedule:",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-2-bootstrap-confidence-intervals-chapter-3",
    "href": "chapters/part-g-lab-practicals.html#lab-2-bootstrap-confidence-intervals-chapter-3",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 2: Bootstrap Confidence Intervals (Chapter 3)",
    "text": "Lab 2: Bootstrap Confidence Intervals (Chapter 3)",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-3-mann-whitney-u-test-chapter-4",
    "href": "chapters/part-g-lab-practicals.html#lab-3-mann-whitney-u-test-chapter-4",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 3: Mann-Whitney U Test (Chapter 4)",
    "text": "Lab 3: Mann-Whitney U Test (Chapter 4)",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-4-wilcoxon-signed-rank-test-chapter-4",
    "href": "chapters/part-g-lab-practicals.html#lab-4-wilcoxon-signed-rank-test-chapter-4",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 4: Wilcoxon Signed-Rank Test (Chapter 4)",
    "text": "Lab 4: Wilcoxon Signed-Rank Test (Chapter 4)",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-firth",
    "href": "chapters/part-g-lab-practicals.html#lab-firth",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 5: Firth’s Penalized Logistic Regression",
    "text": "Lab 5: Firth’s Penalized Logistic Regression\nLearning Objectives:\n\nRecognize complete/quasi-complete separation in logistic regression\nApply Firth’s penalized likelihood method to handle separation\nCompare standard and penalized logistic regression results\nInterpret odds ratios and confidence intervals with small samples\nCreate publication-ready tables for logistic regression\n\nScenario: You’re analyzing pilot data (n=18) from a clinical trial where patients received a new treatment (Yes/No) and the outcome is remission (Yes/No). The treatment group has 10 patients (9 remission, 1 no remission), and the control group has 8 patients (2 remission, 6 no remission).\n\nStep 1: Set Up and Load Data\nCreate the dataset and visualize the contingency table:\n\nlibrary(tidyverse)\nlibrary(logistf)  # Firth's logistic regression\nlibrary(broom)\n\n# Create dataset with separation problem\nclinical_data &lt;- tibble(\n  patient_id = 1:18,\n  treatment = rep(c(\"Control\", \"Treatment\"), c(8, 10)),\n  remission = c(0,0,0,0,0,0,1,1,  # Control: 2/8 remission\n                1,1,1,1,1,1,1,1,1,0)  # Treatment: 9/10 remission\n)\n\n# Contingency table\ntable(clinical_data$treatment, clinical_data$remission)\n\n           \n            0 1\n  Control   6 2\n  Treatment 1 9\n\n# Visualization\nclinical_data %&gt;%\n  count(treatment, remission) %&gt;%\n  ggplot(aes(x = treatment, y = n, fill = factor(remission))) +\n  geom_col(position = \"stack\") +\n  labs(title = \"Treatment vs Remission Outcomes\",\n       y = \"Count\", fill = \"Remission\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: You should see near-complete separation (9/10 vs 2/8).\n\n\n\nStep 2: Attempt Standard Logistic Regression\nTry fitting standard logistic regression to see the separation problem:\n\n# Attempt standard logistic regression\nstandard_model &lt;- glm(remission ~ treatment, \n                      data = clinical_data,\n                      family = binomial(link = \"logit\"))\n\n# Check for warning signs\nsummary(standard_model)\n\n\nCall:\nglm(formula = remission ~ treatment, family = binomial(link = \"logit\"), \n    data = clinical_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)          -1.099      0.816   -1.35    0.178  \ntreatmentTreatment    3.296      1.333    2.47    0.013 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 24.057  on 17  degrees of freedom\nResidual deviance: 15.499  on 16  degrees of freedom\nAIC: 19.5\n\nNumber of Fisher Scoring iterations: 4\n\n# Extract coefficients\ntidy(standard_model, exponentiate = TRUE, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term               estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)           0.333     0.816     -1.35  0.178    0.0488      1.45\n2 treatmentTreatment   27.0       1.33       2.47  0.0134   2.71      709.  \n\n\nCheckpoint: Notice extremely large coefficient, very wide confidence interval, or convergence warnings indicating separation.\nDiscussion: What happens to the odds ratio estimate? Why are the standard errors so large?\n\n\n\nStep 3: Apply Firth’s Penalized Logistic Regression\nUse Firth’s method to obtain reliable estimates:\n\n# Firth's penalized logistic regression\nfirth_model &lt;- logistf(remission ~ treatment, \n                       data = clinical_data)\n\n# Summary\nsummary(firth_model)\n\nlogistf(formula = remission ~ treatment, data = clinical_data)\n\nModel fitted by Penalized ML\nCoefficients:\n                      coef se(coef) lower 0.95 upper 0.95 Chisq        p method\n(Intercept)        -0.9555   0.7442    -2.6494     0.4073 1.841 0.174777      2\ntreatmentTreatment  2.8013   1.1514     0.7496     5.4382 7.523 0.006093      2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=7.523 on 1 df, p=0.006093, n=18\nWald test = 6.062 on 1 df, p = 0.01381\n\n# Extract coefficients (manual conversion to OR)\nfirth_coef &lt;- coef(firth_model)[2]\nfirth_or &lt;- exp(firth_coef)\nfirth_ci &lt;- exp(confint(firth_model)[2, ])\n\ncat(\"Firth's Odds Ratio:\", round(firth_or, 2), \"\\n\")\n\nFirth's Odds Ratio: 16.47 \n\ncat(\"95% CI:\", round(firth_ci, 2), \"\\n\")\n\n95% CI: 2.12 230 \n\n\nCheckpoint: Firth’s OR should be more reasonable (e.g., 10-30) with narrower CI compared to standard method.\n\n\n\nStep 4: Compare Methods Side-by-Side\nCreate a comparison table:\n\n# Standard GLM results\nstandard_or &lt;- exp(coef(standard_model)[2])\nstandard_ci &lt;- exp(confint.default(standard_model)[2, ])\n\n# Comparison tibble\ncomparison &lt;- tibble(\n  Method = c(\"Standard GLM\", \"Firth's Penalized\"),\n  OR = c(standard_or, firth_or),\n  CI_Lower = c(standard_ci[1], firth_ci[1]),\n  CI_Upper = c(standard_ci[2], firth_ci[2]),\n  CI_Width = CI_Upper - CI_Lower\n)\n\nprint(comparison)\n\n# A tibble: 2 × 5\n  Method               OR CI_Lower CI_Upper CI_Width\n  &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Standard GLM       27.0     1.98     368.     366.\n2 Firth's Penalized  16.5     2.12     230.     228.\n\n# Visualization\nggplot(comparison, aes(x = Method, y = OR)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper), \n                width = 0.2) +\n  scale_y_log10() +\n  labs(title = \"Standard vs Firth's Logistic Regression\",\n       y = \"Odds Ratio (log scale)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: Firth’s method produces more conservative (lower) OR and much narrower CI.\n\n\n\nStep 5: Assess Model Fit\nCheck diagnostics for the Firth model:\n\n# Profile likelihood ratio test for treatment effect\nfirth_model$prob  # Profile penalized LR p-value\n\n       (Intercept) treatmentTreatment \n          0.174777           0.006093 \n\n# AIC comparison (lower is better)\ncat(\"Standard GLM AIC:\", AIC(standard_model), \"\\n\")\n\nStandard GLM AIC: 19.5 \n\ncat(\"Firth's AIC:\", firth_model$aic, \"\\n\")\n\nFirth's AIC: \n\n# Raw data proportions\nclinical_data %&gt;%\n  group_by(treatment) %&gt;%\n  summarize(\n    n = n(),\n    remission = sum(remission),\n    proportion = mean(remission)\n  )\n\n# A tibble: 2 × 4\n  treatment     n remission proportion\n  &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Control       8         2          2\n2 Treatment    10         9          9\n\n\nCheckpoint: Firth’s p-value should indicate significant treatment effect, with better model fit than standard GLM.\n\n\n\nStep 6: Predicted Probabilities\nCalculate predicted remission probabilities:\n\n# Create prediction data\npred_data &lt;- tibble(treatment = c(\"Control\", \"Treatment\"))\n\n# Firth's predicted probabilities\npred_probs &lt;- predict(firth_model, \n                      newdata = pred_data,\n                      type = \"response\")\n\npred_results &lt;- pred_data %&gt;%\n  mutate(predicted_prob = pred_probs)\n\nprint(pred_results)\n\n# A tibble: 2 × 2\n  treatment predicted_prob\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Control            0.278\n2 Treatment          0.864\n\n# Compare with observed proportions\nclinical_data %&gt;%\n  group_by(treatment) %&gt;%\n  summarize(observed_prob = mean(remission)) %&gt;%\n  left_join(pred_results, by = \"treatment\")\n\n# A tibble: 2 × 3\n  treatment observed_prob predicted_prob\n  &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1 Control            0.25          0.278\n2 Treatment          0.9           0.864\n\n\nCheckpoint: Predicted probabilities should be close to observed proportions (0.25 control, 0.90 treatment).\n\n\n\nStep 7: Create Publication Table\nFormat results for publication:\n\nlibrary(gt)\n\n# Create publication-ready table\npub_table &lt;- tibble(\n  Predictor = \"Treatment (vs Control)\",\n  `Log OR` = sprintf(\"%.2f\", firth_coef),\n  SE = sprintf(\"%.2f\", sqrt(diag(vcov(firth_model)))[2]),\n  `OR` = sprintf(\"%.2f\", firth_or),\n  `95% CI` = sprintf(\"%.2f-%.2f\", firth_ci[1], firth_ci[2]),\n  `p-value` = sprintf(\"%.3f\", firth_model$prob[2])\n)\n\npub_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Firth's Penalized Logistic Regression\",\n    subtitle = \"Predicting Clinical Remission (n=18)\"\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Firth's method used due to quasi-complete separation\",\n    locations = cells_column_labels(columns = \"OR\")\n  )\n\n\n\n\n\n\n\nFirth's Penalized Logistic Regression\n\n\nPredicting Clinical Remission (n=18)\n\n\nPredictor\nLog OR\nSE\nOR1\n95% CI\np-value\n\n\n\n\nTreatment (vs Control)\n2.80\n1.15\n16.47\n2.12-230.04\n0.006\n\n\n\n1 Firth's method used due to quasi-complete separation\n\n\n\n\n\n\n\n\nCheckpoint: Professional table with OR, CI, and p-value formatted for journal submission.\n\n\n\nStep 8: Interpret and Report\nSummarize the clinical findings:\n\n# Effect size interpretation\ncat(\"Interpretation:\\n\")\n\nInterpretation:\n\ncat(\"- Treatment increases remission odds by\", \n    round(firth_or, 1), \"times\\n\")\n\n- Treatment increases remission odds by 16.5 times\n\ncat(\"- 95% CI:\", round(firth_ci, 2), \"\\n\")\n\n- 95% CI: 2.12 230 \n\ncat(\"- Treatment group: 90% remission\\n\")\n\n- Treatment group: 90% remission\n\ncat(\"- Control group: 25% remission\\n\")\n\n- Control group: 25% remission\n\ncat(\"- Risk difference: 65 percentage points\\n\")\n\n- Risk difference: 65 percentage points\n\n# Sample statement\ncat(\"\\nSample Results Statement:\\n\")\n\n\nSample Results Statement:\n\ncat(\"Due to quasi-complete separation in the contingency table,\\n\")\n\nDue to quasi-complete separation in the contingency table,\n\ncat(\"we used Firth's penalized logistic regression. Treatment\\n\")\n\nwe used Firth's penalized logistic regression. Treatment\n\ncat(\"significantly increased remission odds (OR =\", round(firth_or, 2), \n    \", 95% CI:\", paste(round(firth_ci, 2), collapse = \"-\"), \n    \", p =\", round(firth_model$prob[2], 3), \").\\n\")\n\nsignificantly increased remission odds (OR = 16.47 , 95% CI: 2.12-230.04 , p = 0.006 ).\n\n\nCheckpoint: Clear interpretation acknowledging the separation problem and reporting penalized estimates.\n\n\n\nDiscussion Questions\n\nSeparation Detection: How can you identify complete or quasi-complete separation in your data before modeling?\nAnswer: Check contingency tables for cells with 0 counts, examine coefficient estimates for very large values (&gt;10 in absolute value), look for extremely wide confidence intervals, and watch for convergence warnings.\nWhy Firth’s Method: How does Firth’s penalized likelihood reduce bias in small samples?\nAnswer: Firth’s method adds a penalty term that shrinks estimates toward zero, reducing the finite-sample bias and preventing infinite estimates that occur with separation.\nInterpretation: Is the Firth’s OR of 20 (example) more reliable than the standard OR of 500?\nAnswer: Yes, Firth’s OR is more realistic and stable. The standard OR is inflated due to separation and has a very wide, unreliable CI. Firth’s estimate is appropriately conservative.\nClinical vs Statistical: The p-value is significant, but should we conclude the treatment is effective?\nAnswer: The pilot data are promising (90% vs 25% remission), but n=18 is too small for definitive conclusions. A larger confirmatory trial is needed, though Firth’s method provides the best available estimate from these data.\nAlternative Approaches: When might you use exact logistic regression instead of Firth’s method?\nAnswer: Exact logistic regression (e.g., elrm package) is preferred when sample size is very small (n&lt;20 total) and you want exact p-values. Firth’s method is faster and preferred for moderately small samples (n=20-50).\n\n\n\n\nKey Takeaways\n✓ Separation is common in small-sample logistic regression with unbalanced outcomes\n✓ Standard logistic regression fails with separation (infinite estimates, huge CIs)\n✓ Firth’s penalized method provides finite, stable estimates by adding penalty term\n✓ Always check for separation before fitting logistic models in small samples\n✓ Report the method used and acknowledge limitations in pilot data\n✓ Firth’s estimates are more realistic but still require cautious interpretation\n\n\n\nHomework Exercise\nDataset: A pilot study (n=22) examined whether a new surgical technique (Technique = “New” vs “Standard”) predicts successful outcome (Success = 1 for success, 0 for failure).\n\nsurgery_data &lt;- tibble(\n  patient_id = 1:22,\n  technique = rep(c(\"Standard\", \"New\"), c(11, 11)),\n  success = c(1,1,0,1,0,1,0,0,0,1,0,  # Standard: 6/11 success\n              1,1,1,1,1,1,1,1,1,1,0)  # New: 10/11 success\n)\n\nYour Tasks:\n\nCreate a contingency table and visualize the technique×success relationship\nAttempt standard logistic regression and note any separation issues\nFit Firth’s penalized logistic regression\nCreate a comparison table showing standard vs Firth’s OR and 95% CI\nCalculate predicted success probabilities for each technique\nCreate a publication-ready results table\nWrite a 2-3 sentence results statement reporting Firth’s estimates\n\nExpected Results:\n\nQuasi-complete separation (10/11 vs 6/11)\nStandard glm produces inflated OR (possibly &gt;50) with very wide CI\nFirth’s OR should be approximately 5-10 with narrower CI\nPredicted probabilities: ~55% (Standard), ~90% (New)\nSignificant p-value (&lt;0.05) from profile likelihood ratio test\n\nHint: Check firth_model$prob for p-values, and use exp(coef()) and exp(confint()) to convert log-odds to odds ratios.\nBonus: What is the risk difference (percentage point difference in success rates)? How does this compare to the OR interpretation?",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-bayesian",
    "href": "chapters/part-g-lab-practicals.html#lab-bayesian",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 6: Bayesian Regression with Small Samples",
    "text": "Lab 6: Bayesian Regression with Small Samples\nLearning Objectives:\n\nSpecify weakly informative priors for Bayesian regression\nFit Bayesian models using brms package\nInterpret posterior distributions and credible intervals\nAssess MCMC convergence diagnostics (R-hat, effective sample size)\nCompare Bayesian and frequentist approaches for small samples\n\nScenario: You have n=20 participants in a pilot study examining the relationship between study hours (predictor) and exam score (outcome). You’ll use Bayesian regression to estimate this relationship with appropriate uncertainty quantification.\n\nStep 1: Set Up and Load Data\nCreate the dataset and visualize:\n\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidybayes)\n\n# Set seed for reproducibility\nset.seed(2025)\n\n# Small sample data\nstudy_data &lt;- tibble(\n  student_id = 1:20,\n  study_hours = round(runif(20, 5, 25), 1),\n  exam_score = round(50 + 1.5 * study_hours + rnorm(20, 0, 8), 1)\n)\n\n# Descriptive statistics\nstudy_data %&gt;%\n  summarize(\n    n = n(),\n    mean_hours = mean(study_hours),\n    sd_hours = sd(study_hours),\n    mean_score = mean(exam_score),\n    sd_score = sd(exam_score)\n  )\n\n# A tibble: 1 × 5\n      n mean_hours sd_hours mean_score sd_score\n  &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1    20       15.7     5.17       74.7     11.7\n\n# Scatter plot\nggplot(study_data, aes(x = study_hours, y = exam_score)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Study Hours vs Exam Score (n=20)\",\n       x = \"Study Hours\", y = \"Exam Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: You should see n=20 observations with positive relationship between hours and score.\n\n\n\nStep 2: Specify Priors\nDefine weakly informative priors appropriate for the scale of your data:\n\n# Examine data scale to inform priors\nrange(study_data$exam_score)\n\n[1]  57.7 100.0\n\nrange(study_data$study_hours)\n\n[1]  5.5 24.0\n\n# Define priors\npriors &lt;- c(\n  # Intercept: exam scores range 50-100, centered around 70\n  prior(normal(70, 20), class = Intercept),\n  \n  # Slope: expect positive but uncertain, allow ±10 points per hour\n  prior(normal(0, 5), class = b),\n  \n  # Residual SD: exam scores vary, allow reasonable range\n  prior(student_t(3, 0, 10), class = sigma)\n)\n\n# Prior predictive check (simulate data from priors)\nprior_model &lt;- brm(\n  exam_score ~ study_hours,\n  data = study_data,\n  prior = priors,\n  family = gaussian(),\n  sample_prior = \"only\",  # Only sample from priors\n  chains = 2,\n  iter = 2000,\n  refresh = 0  # Suppress iteration messages\n)\n\n# Visualize prior predictions\npp_check(prior_model, ndraws = 50) +\n  labs(title = \"Prior Predictive Check\",\n       subtitle = \"Do priors allow reasonable exam scores?\")\n\n\n\n\n\n\n\n\nCheckpoint: Prior predictions should span 0-100 range but not produce absurd values (e.g., negative scores or scores &gt;150).\nDiscussion: Are your priors too informative or too vague? Adjust if prior predictions are unrealistic.\n\n\n\nStep 3: Fit Bayesian Model\nFit the model with MCMC sampling:\n\n# Fit Bayesian model\nbayes_model &lt;- brm(\n  exam_score ~ study_hours,\n  data = study_data,\n  prior = priors,\n  family = gaussian(),\n  chains = 4,           # Run 4 chains\n  iter = 4000,          # 4000 iterations per chain\n  warmup = 2000,        # 2000 warmup (burn-in)\n  seed = 2025,\n  refresh = 0\n)\n\n# Summary\nsummary(bayes_model)\n\n Family: gaussian \n  Links: mu = identity \nFormula: exam_score ~ study_hours \n   Data: study_data (Number of observations: 20) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      52.66      7.26    38.41    66.96 1.00     7497     5277\nstudy_hours     1.39      0.44     0.51     2.24 1.00     7695     5135\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     9.77      1.67     7.17    13.59 1.00     6733     6053\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCheckpoint: Model should converge (Rhat ≈ 1.00, ESS &gt; 1000). Posterior mean for slope should be positive.\n\n\n\nStep 4: Check MCMC Diagnostics\nAssess convergence and mixing:\n\n# Check convergence summary from brms\nsummary(bayes_model)\n\n Family: gaussian \n  Links: mu = identity \nFormula: exam_score ~ study_hours \n   Data: study_data (Number of observations: 20) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      52.66      7.26    38.41    66.96 1.00     7497     5277\nstudy_hours     1.39      0.44     0.51     2.24 1.00     7695     5135\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     9.77      1.67     7.17    13.59 1.00     6733     6053\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# R-hat values (should be &lt; 1.01) - extract from summary\nrhat_values &lt;- brms::rhat(bayes_model)\nprint(rhat_values)\n\n  b_Intercept b_study_hours         sigma     Intercept        lprior \n       1.0002        0.9997        1.0013        1.0004        1.0013 \n         lp__ \n       1.0010 \n\n# Effective sample size - extract from summary\ness_values &lt;- brms::neff_ratio(bayes_model)\nprint(ess_values)\n\n  b_Intercept b_study_hours         sigma     Intercept        lprior \n       0.6596        0.6419        0.7567        0.6366        0.6984 \n         lp__ \n       0.4583 \n\n# Trace plots (chains should mix well)\nplot(bayes_model, pars = c(\"b_Intercept\", \"b_study_hours\", \"sigma\"))\n\n\n\n\n\n\n\n\nCheckpoint: - All Rhat values &lt; 1.01 (convergence achieved) - ESS ratios &gt; 0.5 (sufficient effective samples) - Trace plots show “hairy caterpillar” (good mixing) - Rank plots show uniform overlap (no divergences)\n\n\n\nStep 5: Extract and Visualize Posteriors\nExamine posterior distributions:\n\n# Extract posterior draws\nposterior_draws &lt;- as_draws_df(bayes_model)\n\n# Visualize posteriors\nmcmc_areas(bayes_model, \n           pars = c(\"b_Intercept\", \"b_study_hours\"),\n           prob = 0.95) +\n  labs(title = \"Posterior Distributions with 95% Credible Intervals\")\n\n\n\n\n\n\n\n# Summary statistics\nposterior_summary(bayes_model, pars = c(\"b_Intercept\", \"b_study_hours\", \"sigma\"))\n\n              Estimate Est.Error    Q2.5  Q97.5\nb_Intercept     52.664    7.2567 38.4094 66.959\nb_study_hours    1.392    0.4382  0.5079  2.237\nsigma            9.774    1.6694  7.1705 13.585\n\n# Interpretation\ncat(\"For each additional study hour, exam scores increase by:\\n\")\n\nFor each additional study hour, exam scores increase by:\n\ncat(\"Median:\", round(median(posterior_draws$b_study_hours), 2), \"points\\n\")\n\nMedian: 1.4 points\n\ncat(\"95% Credible Interval:\", \n    round(quantile(posterior_draws$b_study_hours, c(0.025, 0.975)), 2), \"\\n\")\n\n95% Credible Interval: 0.51 2.24 \n\n\nCheckpoint: Posterior for slope should be clearly positive (95% CI excludes zero), indicating reliable relationship.\n\n\n\nStep 6: Posterior Predictive Checks\nAssess model fit:\n\n# Posterior predictive check\npp_check(bayes_model, ndraws = 50) +\n  labs(title = \"Posterior Predictive Check\",\n       subtitle = \"Does model reproduce observed data distribution?\")\n\n\n\n\n\n\n\n# Individual data points\npp_check(bayes_model, type = \"scatter_avg\") +\n  labs(title = \"Observed vs Predicted Exam Scores\")\n\n\n\n\n\n\n\n# Prediction intervals for new data\nnew_data &lt;- tibble(study_hours = seq(5, 25, by = 5))\npredictions &lt;- posterior_predict(bayes_model, newdata = new_data)\n\n# Visualize with uncertainty\nnew_data %&gt;%\n  mutate(\n    median_score = apply(predictions, 2, median),\n    lower = apply(predictions, 2, quantile, 0.025),\n    upper = apply(predictions, 2, quantile, 0.975)\n  ) %&gt;%\n  ggplot(aes(x = study_hours)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  geom_line(aes(y = median_score), linewidth = 1) +\n  geom_point(data = study_data, aes(y = exam_score), size = 3) +\n  labs(title = \"Bayesian Regression with 95% Prediction Intervals\",\n       x = \"Study Hours\", y = \"Exam Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: Model should reproduce the observed data distribution well. Prediction intervals should capture most observed points.\n\n\n\nStep 7: Compare with Frequentist Approach\nContrast Bayesian and frequentist results:\n\n# Fit frequentist model\nfreq_model &lt;- lm(exam_score ~ study_hours, data = study_data)\n\n# Comparison table\ncomparison &lt;- tibble(\n  Parameter = c(\"Intercept\", \"Slope (study_hours)\", \"Residual SD\"),\n  Frequentist_Estimate = c(coef(freq_model)[1], \n                           coef(freq_model)[2], \n                           sigma(freq_model)),\n  Frequentist_SE = c(summary(freq_model)$coef[1, 2],\n                     summary(freq_model)$coef[2, 2],\n                     NA),\n  Bayesian_Median = c(median(posterior_draws$b_Intercept),\n                      median(posterior_draws$b_study_hours),\n                      median(posterior_draws$sigma)),\n  Bayesian_MAD = c(mad(posterior_draws$b_Intercept),\n                   mad(posterior_draws$b_study_hours),\n                   mad(posterior_draws$sigma))\n)\n\nprint(comparison)\n\n# A tibble: 3 × 5\n  Parameter     Frequentist_Estimate Frequentist_SE Bayesian_Median Bayesian_MAD\n  &lt;chr&gt;                        &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1 Intercept                    52.5           6.92            52.6         7.01 \n2 Slope (study…                 1.41          0.418            1.40        0.431\n3 Residual SD                   9.42         NA                9.58        1.58 \n\n# Credible vs Confidence Intervals\ncat(\"\\n95% Confidence Interval (Frequentist):\\n\")\n\n\n95% Confidence Interval (Frequentist):\n\nprint(confint(freq_model))\n\n              2.5 % 97.5 %\n(Intercept) 38.0148 67.072\nstudy_hours  0.5262  2.284\n\ncat(\"\\n95% Credible Interval (Bayesian):\\n\")\n\n\n95% Credible Interval (Bayesian):\n\ncat(\"Intercept:\", quantile(posterior_draws$b_Intercept, c(0.025, 0.975)), \"\\n\")\n\nIntercept: 38.41 66.96 \n\ncat(\"Slope:\", quantile(posterior_draws$b_study_hours, c(0.025, 0.975)), \"\\n\")\n\nSlope: 0.5079 2.237 \n\n\nCheckpoint: Bayesian and frequentist estimates should be very similar with weakly informative priors. Bayesian approach provides full posterior distribution, not just point estimates.\n\n\n\nStep 8: Interpret and Report\nSummarize findings:\n\n# Calculate probability of positive effect\nprob_positive &lt;- mean(posterior_draws$b_study_hours &gt; 0)\n\ncat(\"Interpretation:\\n\")\n\nInterpretation:\n\ncat(\"- Median slope:\", round(median(posterior_draws$b_study_hours), 2), \n    \"points per hour\\n\")\n\n- Median slope: 1.4 points per hour\n\ncat(\"- 95% Credible Interval:\", \n    round(quantile(posterior_draws$b_study_hours, c(0.025, 0.975)), 2), \"\\n\")\n\n- 95% Credible Interval: 0.51 2.24 \n\ncat(\"- Probability that slope &gt; 0:\", round(prob_positive, 3), \"\\n\")\n\n- Probability that slope &gt; 0: 0.998 \n\ncat(\"- Interpretation: For each additional study hour, exam scores\\n\")\n\n- Interpretation: For each additional study hour, exam scores\n\ncat(\"  increase by approximately\", \n    round(median(posterior_draws$b_study_hours), 1), \n    \"points (95% CI:\", \n    paste(round(quantile(posterior_draws$b_study_hours, c(0.025, 0.975)), 1), \n          collapse = \" to \"), \").\\n\")\n\n  increase by approximately 1.4 points (95% CI: 0.5 to 2.2 ).\n\n# Sample results statement\ncat(\"\\nSample Results Statement:\\n\")\n\n\nSample Results Statement:\n\ncat(\"We used Bayesian linear regression with weakly informative priors\\n\")\n\nWe used Bayesian linear regression with weakly informative priors\n\ncat(\"(n=20, 4 chains, 8000 iterations). MCMC diagnostics indicated adequate\\n\")\n\n(n=20, 4 chains, 8000 iterations). MCMC diagnostics indicated adequate\n\ncat(\"convergence (all Rhat &lt; 1.01, ESS &gt; 1000). Study hours positively\\n\")\n\nconvergence (all Rhat &lt; 1.01, ESS &gt; 1000). Study hours positively\n\ncat(\"predicted exam scores (β =\", round(median(posterior_draws$b_study_hours), 2),\n    \", 95% CrI [\", paste(round(quantile(posterior_draws$b_study_hours, \n                                         c(0.025, 0.975)), 2), collapse = \", \"),\n    \"], P(β &gt; 0) = \", round(prob_positive, 3), \").\\n\")\n\npredicted exam scores (β = 1.4 , 95% CrI [ 0.51, 2.24 ], P(β &gt; 0) =  0.998 ).\n\n\nCheckpoint: Clear interpretation with credible intervals and probability statements that are unique to Bayesian approach.\n\n\n\nDiscussion Questions\n\nPrior Sensitivity: How would results change with more informative priors (e.g., prior(normal(2, 1), class = b))?\nAnswer: More informative priors would pull estimates toward the prior mean (2), resulting in less influence from data. With n=20, data should dominate weakly informative priors, but stronger priors would shrink estimates.\nCredible vs Confidence: What’s the difference between a 95% credible interval and a 95% confidence interval?\nAnswer: A 95% credible interval says “the parameter has 95% probability of being in this range” (Bayesian). A 95% confidence interval says “if we repeated the study many times, 95% of intervals would contain the true parameter” (frequentist). Credible intervals are more intuitive.\nSmall Sample Advantage: Why might Bayesian methods be preferred for small samples?\nAnswer: Bayesian methods explicitly quantify uncertainty, incorporate prior information to stabilize estimates, don’t rely on asymptotic approximations (which fail in small samples), and provide probability statements about parameters.\nMCMC Diagnostics: What would you do if Rhat &gt; 1.01 or ESS &lt; 400?\nAnswer: Increase iterations (e.g., iter=10000), increase warmup (e.g., warmup=5000), check for divergent transitions (indicating prior-likelihood conflict), consider reparameterization, or use stronger priors to improve geometry.\nReporting: What information should you include when reporting Bayesian regression results?\nAnswer: Prior specifications, MCMC settings (chains, iterations, warmup), convergence diagnostics (Rhat, ESS), posterior summaries (median or mean, credible intervals), and interpretation using probability statements.\n\n\n\n\nKey Takeaways\n✓ Bayesian regression provides full uncertainty quantification through posterior distributions\n✓ Weakly informative priors regularize estimates without dominating small-sample data\n✓ MCMC diagnostics are critical: Always check Rhat, ESS, trace plots, and divergences\n✓ Posterior predictive checks assess model fit and identify misspecification\n✓ Credible intervals have intuitive interpretation: Direct probability statements about parameters\n✓ Bayesian methods excel in small samples where frequentist asymptotics break down\n\n\n\nHomework Exercise\nDataset: A pilot study (n=18) examined whether training hours predict performance scores.\n\nset.seed(123)\ntraining_data &lt;- tibble(\n  employee_id = 1:18,\n  training_hours = round(runif(18, 10, 40), 1),\n  performance_score = round(60 + 0.8 * training_hours + rnorm(18, 0, 6), 1)\n)\n\nYour Tasks:\n\nCreate a scatter plot of training hours vs performance score\nSpecify weakly informative priors (justify your choices)\nFit a Bayesian linear regression model with brms (4 chains, 4000 iterations)\nCheck MCMC diagnostics (Rhat, ESS, trace plots)\nVisualize posterior distributions for intercept and slope\nPerform posterior predictive checks\nCompare Bayesian and frequentist estimates\nCalculate P(slope &gt; 0) and interpret\nCreate a plot with median regression line and 95% credible intervals\nWrite a results statement reporting Bayesian estimates\n\nExpected Results:\n\nPositive relationship between training and performance\nSlope posterior: median ≈ 0.7-0.9, 95% CrI roughly [0.4, 1.2]\nAll Rhat &lt; 1.01, ESS &gt; 1000 (good convergence)\nP(slope &gt; 0) &gt; 0.99 (strong evidence for positive effect)\nBayesian and frequentist estimates very similar\n\nHints: - For priors: performance scores range 60-90, training hours range 10-40 - Use prior(normal(75, 20), class = Intercept) and prior(normal(0, 2), class = b) - Extract posteriors with as_draws_df(model) and calculate probabilities with mean(posterior_draws$parameter &gt; value)\nBonus: Re-fit the model with more informative priors (e.g., prior(normal(1, 0.5), class = b) assuming we expect slopes near 1). How much do results change? What does this reveal about data vs prior influence with n=18?",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-reliability",
    "href": "chapters/part-g-lab-practicals.html#lab-reliability",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 7: Reliability Analysis with Cronbach’s Alpha",
    "text": "Lab 7: Reliability Analysis with Cronbach’s Alpha\nLearning Objectives:\n\nCalculate Cronbach’s alpha for internal consistency\nConduct item-total correlation analysis\nInterpret “alpha if item deleted” diagnostics\nIdentify and remove problematic scale items\nReport reliability estimates with confidence intervals\n\nScenario: You’re developing a 6-item Job Satisfaction Scale in a pilot sample (n=25 employees). You need to assess internal consistency, identify any poor items, and refine the scale.\n\nStep 1: Set Up and Load Data\nCreate pilot data with responses to 6 job satisfaction items:\n\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(corrplot)\n\n# Set seed for reproducibility\nset.seed(2025)\n\n# Simulate 6-item scale responses (1=Strongly Disagree, 5=Strongly Agree)\n# Item 3 is intentionally problematic (reverse-coded but not reversed)\njob_data &lt;- tibble(\n  employee_id = 1:25,\n  item1 = sample(2:5, 25, replace = TRUE, prob = c(0.1, 0.3, 0.4, 0.2)),\n  item2 = sample(2:5, 25, replace = TRUE, prob = c(0.1, 0.3, 0.3, 0.3)),\n  item3 = sample(1:4, 25, replace = TRUE, prob = c(0.3, 0.4, 0.2, 0.1)),  # Problematic\n  item4 = sample(2:5, 25, replace = TRUE, prob = c(0.1, 0.2, 0.4, 0.3)),\n  item5 = sample(2:5, 25, replace = TRUE, prob = c(0.1, 0.3, 0.3, 0.3)),\n  item6 = sample(2:5, 25, replace = TRUE, prob = c(0.2, 0.2, 0.3, 0.3))\n)\n\n# Item descriptions\nitem_labels &lt;- c(\n  \"item1\" = \"I enjoy my work tasks\",\n  \"item2\" = \"I feel valued by my employer\",\n  \"item3\" = \"I often feel stressed at work (R)\",  # Reverse-coded\n  \"item4\" = \"I would recommend this workplace\",\n  \"item5\" = \"My work is meaningful\",\n  \"item6\" = \"I have good work-life balance\"\n)\n\n# Descriptive statistics\njob_data %&gt;%\n  select(starts_with(\"item\")) %&gt;%\n  summary()\n\n     item1          item2         item3          item4         item5     \n Min.   :2.00   Min.   :2.0   Min.   :1.00   Min.   :2.0   Min.   :2.00  \n 1st Qu.:3.00   1st Qu.:3.0   1st Qu.:2.00   1st Qu.:3.0   1st Qu.:3.00  \n Median :4.00   Median :4.0   Median :2.00   Median :4.0   Median :5.00  \n Mean   :3.84   Mean   :3.6   Mean   :2.16   Mean   :3.8   Mean   :4.12  \n 3rd Qu.:5.00   3rd Qu.:5.0   3rd Qu.:3.00   3rd Qu.:5.0   3rd Qu.:5.00  \n Max.   :5.00   Max.   :5.0   Max.   :4.00   Max.   :5.0   Max.   :5.00  \n     item6     \n Min.   :2.00  \n 1st Qu.:3.00  \n Median :4.00  \n Mean   :3.64  \n 3rd Qu.:5.00  \n Max.   :5.00  \n\n\nCheckpoint: You should have n=25 employees with responses to 6 items on a 1-5 scale.\n\n\n\nStep 2: Visualize Inter-Item Correlations\nExamine correlation matrix:\n\n# Extract item columns\nitems &lt;- job_data %&gt;% select(starts_with(\"item\"))\n\n# Correlation matrix\ncor_matrix &lt;- cor(items)\nprint(round(cor_matrix, 2))\n\n      item1 item2 item3 item4 item5 item6\nitem1  1.00  0.02 -0.18 -0.04  0.24  0.10\nitem2  0.02  1.00  0.03 -0.34  0.04  0.21\nitem3 -0.18  0.03  1.00 -0.21  0.02 -0.07\nitem4 -0.04 -0.34 -0.21  1.00  0.02 -0.03\nitem5  0.24  0.04  0.02  0.02  1.00  0.38\nitem6  0.10  0.21 -0.07 -0.03  0.38  1.00\n\n# Visualize correlations\ncorrplot(cor_matrix, \n         method = \"color\", \n         type = \"upper\",\n         addCoef.col = \"black\",\n         tl.col = \"black\",\n         tl.srt = 45,\n         title = \"Inter-Item Correlations\",\n         mar = c(0,0,2,0))\n\n\n\n\n\n\n\n# Identify problematic correlations\ncat(\"\\nItems with low/negative correlations:\\n\")\n\n\nItems with low/negative correlations:\n\nwhich(cor_matrix &lt; 0.2 & cor_matrix &lt; 1, arr.ind = TRUE)\n\n      row col\nitem2   2   1\nitem3   3   1\nitem4   4   1\nitem6   6   1\nitem1   1   2\nitem3   3   2\nitem4   4   2\nitem5   5   2\nitem1   1   3\nitem2   2   3\nitem4   4   3\nitem5   5   3\nitem6   6   3\nitem1   1   4\nitem2   2   4\nitem3   3   4\nitem5   5   4\nitem6   6   4\nitem2   2   5\nitem3   3   5\nitem4   4   5\nitem1   1   6\nitem3   3   6\nitem4   4   6\n\n\nCheckpoint: Most items should correlate positively (r &gt; 0.3). Item 3 likely shows weak or negative correlations.\nDiscussion: Why do we want items to be positively correlated? What do low/negative correlations suggest?\n\n\n\nStep 3: Calculate Cronbach’s Alpha\nCompute reliability for the full 6-item scale:\n\n# Cronbach's alpha with detailed output\nalpha_result &lt;- psych::alpha(items)\n\nSome items ( item3 item4 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n# Print full output\nprint(alpha_result)\n\n\nReliability analysis   \nCall: psych::alpha(x = items)\n\n  raw_alpha std.alpha G6(smc) average_r   S/N  ase mean   sd median_r\n      0.13     0.078     0.2     0.014 0.085 0.25  3.5 0.44    0.024\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    -0.53  0.13  0.57\nDuhachek -0.37  0.13  0.63\n\n Reliability if an item is dropped:\n      raw_alpha std.alpha G6(smc) average_r    S/N alpha se var.r   med.r\nitem1     0.087     0.033   0.162    0.0068  0.034     0.27 0.039  0.0240\nitem2     0.172     0.114   0.200    0.0252  0.129     0.25 0.032 -0.0026\nitem3     0.258     0.246   0.310    0.0614  0.327     0.23 0.037  0.0331\nitem4     0.339     0.301   0.327    0.0793  0.430     0.20 0.026  0.0344\nitem5    -0.237    -0.315  -0.128   -0.0504 -0.240     0.37 0.025 -0.0330\nitem6    -0.213    -0.229  -0.057   -0.0387 -0.186     0.37 0.026  0.0202\n\n Item statistics \n       n raw.r std.r  r.cor  r.drop mean   sd\nitem1 25  0.42  0.45  0.211  0.0824  3.8 0.90\nitem2 25  0.42  0.38  0.097 -0.0062  3.6 1.12\nitem3 25  0.17  0.23 -0.272 -0.1490  2.2 0.85\nitem4 25  0.15  0.16 -0.370 -0.2212  3.8 1.00\nitem5 25  0.68  0.68  0.795  0.3605  4.1 1.05\nitem6 25  0.68  0.63  0.685  0.3133  3.6 1.15\n\nNon missing response frequency for each item\n        1    2    3    4    5 miss\nitem1 0.0 0.04 0.36 0.32 0.28    0\nitem2 0.0 0.20 0.28 0.24 0.28    0\nitem3 0.2 0.52 0.20 0.08 0.00    0\nitem4 0.0 0.12 0.24 0.36 0.28    0\nitem5 0.0 0.08 0.24 0.16 0.52    0\nitem6 0.0 0.20 0.28 0.20 0.32    0\n\n# Extract key statistics safely using as.numeric() to ensure atomic values\ncat(\"\\nCronbach's Alpha:\", round(as.numeric(alpha_result$total$raw_alpha), 3), \"\\n\")\n\n\nCronbach's Alpha: 0.131 \n\ncat(\"Standardized Alpha:\", round(as.numeric(alpha_result$total$std.alpha), 3), \"\\n\")\n\nStandardized Alpha: 0.078 \n\ncat(\"Number of items:\", as.numeric(alpha_result$total$nvar), \"\\n\")\n\nNumber of items:  \n\ncat(\"Sample size:\", as.numeric(alpha_result$total$n), \"\\n\")\n\nSample size:  \n\n# 95% Confidence interval for alpha (if available)\nif (!is.null(alpha_result$feldt) && is.data.frame(alpha_result$feldt) && nrow(alpha_result$feldt) &gt; 0) {\n  cat(\"95% CI for Alpha:\", \n      round(as.numeric(alpha_result$feldt$lower.ci), 3), \"-\",\n      round(as.numeric(alpha_result$feldt$upper.ci), 3), \"\\n\")\n}\n\nCheckpoint: Initial alpha should be moderate (0.60-0.75), with wide CI due to small sample.\n\n\n\nStep 4: Item-Total Correlations\nExamine how each item relates to total score:\n\n# Item statistics\nitem_stats &lt;- alpha_result$item.stats\n\nprint(item_stats)\n\n       n  raw.r  std.r    r.cor    r.drop mean     sd\nitem1 25 0.4153 0.4503  0.21130  0.082375 3.84 0.8981\nitem2 25 0.4177 0.3779  0.09653 -0.006213 3.60 1.1180\nitem3 25 0.1737 0.2349 -0.27174 -0.149026 2.16 0.8505\nitem4 25 0.1546 0.1644 -0.36962 -0.221168 3.80 1.0000\nitem5 25 0.6817 0.6760  0.79483  0.360496 4.12 1.0536\nitem6 25 0.6781 0.6300  0.68457  0.313344 3.64 1.1504\n\n# Visualize item-total correlations\nitem_stats %&gt;%\n  rownames_to_column(\"item\") %&gt;%\n  ggplot(aes(x = item, y = r.cor)) +\n  geom_col(fill = \"steelblue\") +\n  geom_hline(yintercept = 0.3, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Corrected Item-Total Correlations\",\n       subtitle = \"Red line = 0.30 threshold\",\n       x = \"Item\", y = \"Correlation with Total\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nCheckpoint: Item 3 should show low corrected item-total correlation (r &lt; 0.30), flagging it as problematic.\nDiscussion: What does a low item-total correlation mean? Should we remove this item?\n\n\n\nStep 5: “Alpha if Item Deleted” Analysis\nSee how alpha changes if each item is removed:\n\n# Extract \"alpha if item deleted\"\nalpha_if_deleted &lt;- alpha_result$alpha.drop\n\nprint(alpha_if_deleted)\n\n      raw_alpha std.alpha  G6(smc) average_r      S/N alpha se   var.r\nitem1   0.08678   0.03325  0.16233  0.006832  0.03439   0.2742 0.03904\nitem2   0.17226   0.11438  0.20042  0.025180  0.12915   0.2467 0.03237\nitem3   0.25783   0.24649  0.30968  0.061407  0.32712   0.2269 0.03740\nitem4   0.33943   0.30090  0.32746  0.079260  0.43041   0.2002 0.02617\nitem5  -0.23699  -0.31525 -0.12752 -0.050351 -0.23969   0.3745 0.02492\nitem6  -0.21349  -0.22895 -0.05668 -0.038702 -0.18630   0.3750 0.02633\n          med.r\nitem1  0.023955\nitem2 -0.002624\nitem3  0.033088\nitem4  0.034370\nitem5 -0.033045\nitem6  0.020163\n\n# Visualize impact\nalpha_if_deleted %&gt;%\n  rownames_to_column(\"item\") %&gt;%\n  ggplot(aes(x = item, y = raw_alpha)) +\n  geom_col(fill = \"coral\") +\n  geom_hline(yintercept = alpha_result$total$raw_alpha, \n             linetype = \"dashed\", color = \"blue\") +\n  labs(title = \"Alpha if Item Deleted\",\n       subtitle = \"Blue line = Current alpha\",\n       x = \"Item\", y = \"Alpha without this item\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Identify items that improve alpha if deleted\nitems_to_consider &lt;- alpha_if_deleted %&gt;%\n  filter(raw_alpha &gt; alpha_result$total$raw_alpha)\n\nif (nrow(items_to_consider) &gt; 0) {\n  cat(\"\\nRemoving these items would INCREASE alpha:\\n\")\n  print(rownames(items_to_consider))\n}\n\n\nRemoving these items would INCREASE alpha:\n[1] \"item2\" \"item3\" \"item4\"\n\n\nCheckpoint: Removing item 3 should increase alpha, suggesting it should be deleted.\n\n\n\nStep 6: Recompute Alpha Without Problematic Item\nRemove item 3 and reassess reliability:\n\n# Remove item 3\nitems_revised &lt;- items %&gt;% select(-item3)\n\n# Recompute alpha\nalpha_revised &lt;- psych::alpha(items_revised)\n\nSome items ( item4 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\nprint(alpha_revised)\n\n\nReliability analysis   \nCall: psych::alpha(x = items_revised)\n\n  raw_alpha std.alpha G6(smc) average_r  S/N  ase mean   sd median_r\n      0.26      0.25    0.31     0.061 0.33 0.23  3.8 0.53    0.033\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    -0.33  0.26  0.63\nDuhachek -0.19  0.26  0.70\n\n Reliability if an item is dropped:\n      raw_alpha std.alpha G6(smc) average_r    S/N alpha se var.r   med.r\nitem1     0.192     0.169   0.257    0.0483  0.203     0.25 0.058  0.0331\nitem2     0.350     0.339   0.329    0.1138  0.514     0.21 0.028  0.0635\nitem4     0.446     0.442   0.410    0.1653  0.792     0.18 0.019  0.1553\nitem5    -0.032    -0.051   0.048   -0.0124 -0.049     0.33 0.034 -0.0062\nitem6    -0.059    -0.033   0.066   -0.0081 -0.032     0.34 0.035  0.0202\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nitem1 25  0.48  0.53  0.28  0.151  3.8 0.9\nitem2 25  0.41  0.37  0.06 -0.016  3.6 1.1\nitem4 25  0.22  0.25 -0.19 -0.160  3.8 1.0\nitem5 25  0.68  0.68  0.63  0.351  4.1 1.1\nitem6 25  0.70  0.67  0.61  0.349  3.6 1.2\n\nNon missing response frequency for each item\n         2    3    4    5 miss\nitem1 0.04 0.36 0.32 0.28    0\nitem2 0.20 0.28 0.24 0.28    0\nitem4 0.12 0.24 0.36 0.28    0\nitem5 0.08 0.24 0.16 0.52    0\nitem6 0.20 0.28 0.20 0.32    0\n\n# Compare original vs revised\ncomparison &lt;- tibble(\n  Version = c(\"Original (6 items)\", \"Revised (5 items)\"),\n  Alpha = c(alpha_result$total$raw_alpha, \n            alpha_revised$total$raw_alpha),\n  CI_Lower = c(alpha_result$feldt$lower.ci,\n               alpha_revised$feldt$lower.ci),\n  CI_Upper = c(alpha_result$feldt$upper.ci,\n               alpha_revised$feldt$upper.ci),\n  Mean_r = c(alpha_result$total$average_r,\n             alpha_revised$total$average_r)\n)\n\nprint(comparison)\n\n# A tibble: 2 × 5\n  Version            Alpha CI_Lower     CI_Upper     Mean_r\n  &lt;chr&gt;              &lt;dbl&gt; &lt;named list&gt; &lt;named list&gt;  &lt;dbl&gt;\n1 Original (6 items) 0.131 &lt;dbl [1]&gt;    &lt;dbl [1]&gt;    0.0139\n2 Revised (5 items)  0.258 &lt;dbl [1]&gt;    &lt;dbl [1]&gt;    0.0614\n\n\nCheckpoint: Revised alpha should improve (e.g., from 0.65 to 0.78), with all item-total correlations now &gt; 0.30.\n\n\n\nStep 7: Final Scale Scoring\nCalculate total scores for the refined 5-item scale:\n\n# Calculate total scores\njob_data_scored &lt;- job_data %&gt;%\n  mutate(\n    total_score_original = rowSums(select(., starts_with(\"item\"))),\n    total_score_revised = rowSums(select(., item1, item2, item4, item5, item6)),\n    mean_score_revised = total_score_revised / 5\n  )\n\n# Descriptive statistics for final scale\njob_data_scored %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(mean_score_revised),\n    sd = sd(mean_score_revised),\n    min = min(mean_score_revised),\n    max = max(mean_score_revised)\n  )\n\n# A tibble: 1 × 5\n      n  mean    sd   min   max\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    25   3.8 0.526   2.6   4.6\n\n# Distribution of scores\nggplot(job_data_scored, aes(x = mean_score_revised)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Job Satisfaction Scores (Revised Scale)\",\n       x = \"Mean Score (1-5)\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: Mean satisfaction score should be in reasonable range (e.g., 2.5-4.0 on 1-5 scale).\n\n\n\nStep 8: Create Publication Table\nSummarize reliability analysis for reporting:\n\nlibrary(gt)\n\n# Item-level summary for revised scale\nitem_summary &lt;- alpha_revised$item.stats %&gt;%\n  rownames_to_column(\"item\") %&gt;%\n  mutate(\n    Item_Label = item_labels[item],\n    M = round(mean, 2),\n    SD = round(sd, 2),\n    `r (item-total)` = round(r.cor, 2)\n  ) %&gt;%\n  select(item, Item_Label, M, SD, `r (item-total)`)\n\nitem_summary %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Job Satisfaction Scale: Item Analysis\",\n    subtitle = \"Revised 5-Item Scale (n=25)\"\n  ) %&gt;%\n  tab_footnote(\n    footnote = sprintf(\"Cronbach's α = %.2f, 95%% CI [%.2f, %.2f]\",\n                       alpha_revised$total$raw_alpha,\n                       alpha_revised$feldt$lower.ci,\n                       alpha_revised$feldt$upper.ci),\n    locations = cells_column_labels(columns = \"r (item-total)\")\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Item 3 removed due to low item-total correlation\",\n    locations = cells_title()\n  )\n\n\n\n\n\n\n\nJob Satisfaction Scale: Item Analysis1\n\n\nRevised 5-Item Scale (n=25)1\n\n\nitem\nItem_Label\nM\nSD\nr (item-total)2\n\n\n\n\nitem1\nI enjoy my work tasks\n3.84\n0.90\n0.28\n\n\nitem2\nI feel valued by my employer\n3.60\n1.12\n0.06\n\n\nitem4\nI would recommend this workplace\n3.80\n1.00\n-0.19\n\n\nitem5\nMy work is meaningful\n4.12\n1.05\n0.63\n\n\nitem6\nI have good work-life balance\n3.64\n1.15\n0.61\n\n\n\n1 Item 3 removed due to low item-total correlation\n\n\n2 Cronbach's α = 0.26, 95% CI [-0.33, 0.63]\n\n\n\n\n\n\n\n\nCheckpoint: Professional table showing item statistics and reliability for the refined scale.\n\n\n\nDiscussion Questions\n\nAcceptability Thresholds: Is α = 0.78 acceptable for a pilot scale with n=25?\nAnswer: Yes. For pilot research, α &gt; 0.70 is acceptable. The value 0.78 suggests adequate internal consistency. However, the wide CI (e.g., 0.65-0.88) reflects uncertainty due to small n.\nItem Deletion: Should you always remove items with low item-total correlations?\nAnswer: Not automatically. Consider (1) theoretical importance of the item, (2) whether it measures a different facet, (3) whether reverse-coding was done correctly, (4) impact on content validity. Sometimes low correlations indicate multidimensionality rather than bad items.\nSample Size: How does n=25 affect reliability estimates?\nAnswer: Small samples produce wider confidence intervals and less stable estimates. Alpha can be inflated or deflated. Ideally, n&gt;100 for reliability studies, but pilot data (n=25-50) provide initial evidence.\nAlpha Limitations: What are weaknesses of Cronbach’s alpha?\nAnswer: Assumes tau-equivalent model (equal item-total correlations), sensitive to number of items (more items → higher alpha), doesn’t detect multidimensionality, can be high even with low inter-item correlations if many items.\nReporting: What should you include when reporting reliability?\nAnswer: Cronbach’s alpha, 95% CI, sample size, number of items, mean inter-item correlation, item-total correlations, and whether any items were deleted (with justification).\n\n\n\n\nKey Takeaways\n✓ Cronbach’s alpha assesses internal consistency (typical goal: α &gt; 0.70)\n✓ Item-total correlations identify problematic items (threshold: r &gt; 0.30)\n✓ “Alpha if deleted” analysis guides scale refinement decisions\n✓ Small samples (n&lt;50) produce wide CIs for reliability estimates\n✓ Item deletion must be justified theoretically, not just statistically\n✓ Always report confidence intervals and sample size with alpha\n\n\n\nHomework Exercise\nDataset: A 5-item Self-Efficacy Scale piloted with n=30 students.\n\nset.seed(456)\nefficacy_data &lt;- tibble(\n  student_id = 1:30,\n  item1 = sample(2:5, 30, replace = TRUE, prob = c(0.1, 0.3, 0.4, 0.2)),\n  item2 = sample(2:5, 30, replace = TRUE, prob = c(0.1, 0.2, 0.4, 0.3)),\n  item3 = sample(2:5, 30, replace = TRUE, prob = c(0.1, 0.3, 0.3, 0.3)),\n  item4 = sample(1:5, 30, replace = TRUE, prob = c(0.2, 0.3, 0.3, 0.1, 0.1)),  # Potentially problematic\n  item5 = sample(2:5, 30, replace = TRUE, prob = c(0.1, 0.2, 0.3, 0.4))\n)\n\nYour Tasks:\n\nCreate a correlation matrix heatmap for the 5 items\nCalculate Cronbach’s alpha for the full 5-item scale\nReport alpha with 95% confidence interval\nCreate a bar chart of corrected item-total correlations\nIdentify items with r &lt; 0.30\nExamine “alpha if item deleted” – would removing any item increase alpha?\nIf needed, recompute alpha with problematic item(s) removed\nCreate a comparison table (original vs revised scale)\nCalculate mean self-efficacy scores for the final scale\nCreate a publication-ready table with item statistics and alpha\n\nExpected Results:\n\nOriginal alpha approximately 0.72-0.78\nItem 4 may show lower item-total correlation (r ≈ 0.25-0.35)\nRemoving item 4 might increase alpha to ~0.80-0.85\nMean self-efficacy score: 3.0-3.5 on 1-5 scale\n\nHints: - Use psych::alpha(items) for comprehensive output - Check alpha_result$item.stats for item-total correlations - Check alpha_result$alpha.drop for “alpha if deleted” - Use corrplot() for visualization\nBonus: Calculate standardized alpha vs raw alpha. When are they different, and which should you report? How does scale length (number of items) affect alpha?",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-power",
    "href": "chapters/part-g-lab-practicals.html#lab-power",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 8: Power Analysis for Study Planning",
    "text": "Lab 8: Power Analysis for Study Planning\nLearning Objectives:\n\nCalculate statistical power for t-tests and ANOVA\nDetermine required sample size for desired power\nCreate power curves visualizing power-sample size trade-offs\nJustify sample size decisions in research proposals\nUnderstand effect size estimation and Cohen’s benchmarks\n\nScenario: You’re planning a randomized trial comparing a new therapy (treatment) vs standard care (control) on depression scores. You need to determine the required sample size to detect a meaningful effect with 80% power.\n\nStep 1: Set Up and Define Parameters\nSpecify effect size and significance level:\n\nlibrary(tidyverse)\nlibrary(pwr)\n\n# Define study parameters\nalpha &lt;- 0.05        # Significance level (two-tailed)\npower_target &lt;- 0.80  # Desired power (80%)\neffect_size &lt;- 0.5    # Cohen's d = 0.5 (medium effect)\n\ncat(\"Study Planning Parameters:\\n\")\n\nStudy Planning Parameters:\n\ncat(\"- Significance level (α):\", alpha, \"\\n\")\n\n- Significance level (α): 0.05 \n\ncat(\"- Desired power (1-β):\", power_target, \"\\n\")\n\n- Desired power (1-β): 0.8 \n\ncat(\"- Expected effect size (Cohen's d):\", effect_size, \"\\n\")\n\n- Expected effect size (Cohen's d): 0.5 \n\ncat(\"- Test type: Two-sample t-test (two-tailed)\\n\")\n\n- Test type: Two-sample t-test (two-tailed)\n\n# Cohen's effect size benchmarks\ntibble(\n  Magnitude = c(\"Small\", \"Medium\", \"Large\"),\n  `Cohen's d` = c(0.2, 0.5, 0.8),\n  `Cohen's f (ANOVA)` = c(0.10, 0.25, 0.40),\n  `R² (Regression)` = c(0.01, 0.09, 0.25)\n)\n\n# A tibble: 3 × 4\n  Magnitude `Cohen's d` `Cohen's f (ANOVA)` `R² (Regression)`\n  &lt;chr&gt;           &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1 Small             0.2                0.1               0.01\n2 Medium            0.5                0.25              0.09\n3 Large             0.8                0.4               0.25\n\n\nCheckpoint: You’ve defined α=0.05, power=0.80, and d=0.5 for planning.\n\n\n\nStep 2: Calculate Required Sample Size\nDetermine n per group for target power:\n\n# Power analysis for two-sample t-test\npower_result &lt;- pwr.t.test(\n  d = effect_size,\n  sig.level = alpha,\n  power = power_target,\n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\n\nprint(power_result)\n\n\n     Two-sample t test power calculation \n\n              n = 63.77\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n# Extract required n per group\nn_per_group &lt;- ceiling(power_result$n)\ntotal_n &lt;- n_per_group * 2\n\ncat(\"\\nRequired Sample Size:\\n\")\n\n\nRequired Sample Size:\n\ncat(\"- Per group:\", n_per_group, \"\\n\")\n\n- Per group: 64 \n\ncat(\"- Total:\", total_n, \"\\n\")\n\n- Total: 128 \n\ncat(sprintf(\"- To detect d=%.1f with %.0f%% power at α=%.2f\\n\",\n            effect_size, power_target*100, alpha))\n\n- To detect d=0.5 with 80% power at α=0.05\n\n\nCheckpoint: For d=0.5, you should need approximately n=64 per group (128 total).\n\n\n\nStep 3: Create Power Curves\nVisualize power as a function of sample size:\n\n# Generate power curves for different effect sizes\nsample_sizes &lt;- seq(10, 150, by = 5)\neffect_sizes &lt;- c(0.2, 0.5, 0.8)\n\npower_curves &lt;- expand_grid(\n  n = sample_sizes,\n  d = effect_sizes\n) %&gt;%\n  mutate(\n    power = map2_dbl(d, n, ~pwr.t.test(\n      d = .x, n = .y, sig.level = alpha,\n      type = \"two.sample\", alternative = \"two.sided\"\n    )$power)\n  )\n\n# Plot power curves\nggplot(power_curves, aes(x = n, y = power, color = factor(d))) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = n_per_group, linetype = \"dotted\", color = \"blue\") +\n  scale_color_manual(\n    values = c(\"green\", \"blue\", \"purple\"),\n    labels = c(\"d=0.2 (small)\", \"d=0.5 (medium)\", \"d=0.8 (large)\")\n  ) +\n  labs(\n    title = \"Power Curves for Two-Sample t-Test\",\n    subtitle = sprintf(\"α=%.2f, two-tailed\", alpha),\n    x = \"Sample Size (per group)\",\n    y = \"Statistical Power (1-β)\",\n    color = \"Effect Size\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nCheckpoint: Power increases with larger n and larger effect sizes. Red line shows 80% power threshold.\n\n\n\nStep 4: Explore Effect of Alpha on Power\nCompare power with different significance levels:\n\n# Power for different alpha levels\nalphas &lt;- c(0.01, 0.05, 0.10)\n\nalpha_comparison &lt;- map_df(alphas, function(a) {\n  result &lt;- pwr.t.test(\n    d = effect_size,\n    sig.level = a,\n    power = power_target,\n    type = \"two.sample\"\n  )\n  tibble(\n    alpha = a,\n    required_n = ceiling(result$n),\n    total_n = ceiling(result$n) * 2\n  )\n})\n\nprint(alpha_comparison)\n\n# A tibble: 3 × 3\n  alpha required_n total_n\n  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1  0.01         96     192\n2  0.05         64     128\n3  0.1          51     102\n\n# Visualization\nggplot(alpha_comparison, aes(x = factor(alpha), y = required_n)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = required_n), vjust = -0.5) +\n  labs(\n    title = \"Required Sample Size vs Significance Level\",\n    subtitle = sprintf(\"For d=%.1f and power=%.0f%%\", \n                      effect_size, power_target*100),\n    x = \"Significance Level (α)\",\n    y = \"Required n per Group\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: More stringent α (e.g., 0.01) requires larger samples to maintain 80% power.\n\n\n\nStep 5: ANOVA Power Analysis\nCalculate power for one-way ANOVA with multiple groups:\n\n# Scenario: Compare 3 groups (Control, Treatment A, Treatment B)\nk_groups &lt;- 3\neffect_size_anova &lt;- 0.25  # Cohen's f = 0.25 (medium effect for ANOVA)\n\n# Power analysis for ANOVA\nanova_result &lt;- pwr.anova.test(\n  k = k_groups,\n  f = effect_size_anova,\n  sig.level = alpha,\n  power = power_target\n)\n\nprint(anova_result)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.4\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nn_per_group_anova &lt;- ceiling(anova_result$n)\ntotal_n_anova &lt;- n_per_group_anova * k_groups\n\ncat(\"\\nANOVA Sample Size:\\n\")\n\n\nANOVA Sample Size:\n\ncat(\"- Groups:\", k_groups, \"\\n\")\n\n- Groups: 3 \n\ncat(\"- Per group:\", n_per_group_anova, \"\\n\")\n\n- Per group: 53 \n\ncat(\"- Total:\", total_n_anova, \"\\n\")\n\n- Total: 159 \n\n\nCheckpoint: For f=0.25 and 3 groups, you should need approximately n=52 per group (156 total).\n\n\n\nStep 6: Post-Hoc Power Analysis (Observed Effect)\nSuppose you conducted a pilot study (n=20 per group) and observed d=0.45. What was achieved power?\n\n# Pilot study parameters\npilot_n &lt;- 20\nobserved_d &lt;- 0.45\n\n# Calculate achieved power\nachieved_power &lt;- pwr.t.test(\n  d = observed_d,\n  n = pilot_n,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\n\nprint(achieved_power)\n\n\n     Two-sample t test power calculation \n\n              n = 20\n              d = 0.45\n      sig.level = 0.05\n          power = 0.2838\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\ncat(\"\\nPilot Study Power:\\n\")\n\n\nPilot Study Power:\n\ncat(\"- Sample size:\", pilot_n, \"per group\\n\")\n\n- Sample size: 20 per group\n\ncat(\"- Observed effect:\", observed_d, \"\\n\")\n\n- Observed effect: 0.45 \n\ncat(\"- Achieved power:\", round(achieved_power$power, 3), \"\\n\")\n\n- Achieved power: 0.284 \n\ncat(\"- Interpretation:\", \n    ifelse(achieved_power$power &lt; 0.80, \n           \"Underpowered - consider larger sample for main study\",\n           \"Adequately powered\"), \"\\n\")\n\n- Interpretation: Underpowered - consider larger sample for main study \n\n\nCheckpoint: With n=20 and d=0.45, power is only ~0.35 (35%), indicating the pilot was underpowered.\n\n\n\nStep 7: Sensitivity Analysis\nFor a fixed sample size, what is the minimum detectable effect (MDE)?\n\n# Available sample: n=40 per group\navailable_n &lt;- 40\n\n# Minimum detectable effect with 80% power\nmde_result &lt;- pwr.t.test(\n  n = available_n,\n  sig.level = alpha,\n  power = power_target,\n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\n\nprint(mde_result)\n\n\n     Two-sample t test power calculation \n\n              n = 40\n              d = 0.6343\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\ncat(\"\\nSensitivity Analysis:\\n\")\n\n\nSensitivity Analysis:\n\ncat(\"- Available sample:\", available_n, \"per group\\n\")\n\n- Available sample: 40 per group\n\ncat(\"- Minimum detectable effect (80% power):\", round(mde_result$d, 2), \"\\n\")\n\n- Minimum detectable effect (80% power): 0.63 \n\ncat(\"- Interpretation: Can detect effects of d ≥\", \n    round(mde_result$d, 2), \"\\n\")\n\n- Interpretation: Can detect effects of d ≥ 0.63 \n\n\nCheckpoint: With n=40 per group, you can detect effects of d≥0.64 with 80% power (between medium and large).\n\n\n\nStep 8: Create Sample Size Justification\nWrite a formal justification for grant/ethics applications:\n\n# Final recommended sample size\nrecommended_n &lt;- ceiling(power_result$n) * 1.15  # Add 15% for attrition\n\ncat(\"SAMPLE SIZE JUSTIFICATION\\n\")\n\nSAMPLE SIZE JUSTIFICATION\n\ncat(\"===========================\\n\\n\")\n\n===========================\n\ncat(\"Research Question: Does the new therapy reduce depression scores\\n\")\n\nResearch Question: Does the new therapy reduce depression scores\n\ncat(\"compared to standard care?\\n\\n\")\n\ncompared to standard care?\n\ncat(\"Statistical Test: Independent samples t-test (two-tailed)\\n\\n\")\n\nStatistical Test: Independent samples t-test (two-tailed)\n\ncat(\"Effect Size Estimation:\\n\")\n\nEffect Size Estimation:\n\ncat(\"- Based on meta-analysis (Smith et al., 2023), similar therapies\\n\")\n\n- Based on meta-analysis (Smith et al., 2023), similar therapies\n\ncat(\"  produce medium effects (d=0.50, range: 0.35-0.65)\\n\")\n\n  produce medium effects (d=0.50, range: 0.35-0.65)\n\ncat(\"- We conservatively assume d=0.50\\n\\n\")\n\n- We conservatively assume d=0.50\n\ncat(\"Power Analysis:\\n\")\n\nPower Analysis:\n\ncat(sprintf(\"- Significance level: α=%.2f\\n\", alpha))\n\n- Significance level: α=0.05\n\ncat(sprintf(\"- Desired power: %.0f%%\\n\", power_target*100))\n\n- Desired power: 80%\n\ncat(sprintf(\"- Effect size: d=%.2f\\n\", effect_size))\n\n- Effect size: d=0.50\n\ncat(sprintf(\"- Required sample: n=%d per group (%d total)\\n\", \n            n_per_group, total_n))\n\n- Required sample: n=64 per group (128 total)\n\ncat(sprintf(\"- With 15%% attrition: n=%d per group (%d total)\\n\\n\",\n            ceiling(recommended_n), ceiling(recommended_n)*2))\n\n- With 15% attrition: n=74 per group (148 total)\n\ncat(\"Justification:\\n\")\n\nJustification:\n\ncat(\"This sample size provides 80% power to detect a medium effect (d=0.50)\\n\")\n\nThis sample size provides 80% power to detect a medium effect (d=0.50)\n\ncat(\"at the conventional α=0.05 significance level. We will recruit\\n\")\n\nat the conventional α=0.05 significance level. We will recruit\n\ncat(sprintf(\"n=%d per group (accounting for expected 15%% dropout) to ensure\\n\",\n            ceiling(recommended_n)))\n\nn=74 per group (accounting for expected 15% dropout) to ensure\n\ncat(\"adequate power for the primary analysis.\\n\")\n\nadequate power for the primary analysis.\n\n\nCheckpoint: Professional justification suitable for research proposals.\n\n\n\nDiscussion Questions\n\nA Priori vs Post-Hoc: Why is a priori power analysis preferred over post-hoc power analysis?\nAnswer: A priori analysis plans sample size before data collection, maximizing power while minimizing resources. Post-hoc analysis is circular (uses observed effect), uninformative (you already know if the result is significant), and can mislead (low power with significant result suggests lucky sample).\nEffect Size Estimation: How should you choose an effect size for planning?\nAnswer: Base on (1) prior literature/meta-analyses, (2) pilot data, (3) smallest effect of practical interest (SESOI), (4) expert judgment. Be conservative – overestimating effect size leads to underpowered studies.\nTrade-offs: If you can only recruit n=30 per group (not the required 64), what should you do?\nAnswer: Options: (1) acknowledge lower power (report achieved power), (2) increase effect size claims (detect only larger effects), (3) use more powerful design (repeated measures), (4) collaborate to increase n, (5) delay until sufficient resources available.\nMultiple Comparisons: How does power change with multiple outcome tests?\nAnswer: Multiple comparisons require adjusted alpha (e.g., Bonferroni), which decreases power for each test. Need larger samples to maintain power when testing multiple hypotheses.\nReporting: What power information should you include in a paper?\nAnswer: For planning: a priori power analysis with effect size justification. For results: achieved sample size, observed effect size with CI. Do NOT report post-hoc power for non-significant results (it’s always low).\n\n\n\n\nKey Takeaways\n✓ A priori power analysis guides sample size planning before data collection\n✓ Power depends on: effect size, sample size, significance level, and test type\n✓ 80% power is conventional minimum; 90% preferred when feasible\n✓ Effect size estimation is critical – base on literature, not wishful thinking\n✓ Power curves visualize trade-offs between n, effect size, and power\n✓ Account for attrition by recruiting 10-20% more than calculated n\n✓ Underpowered studies are unethical – they waste resources and participant time\n\n\n\nHomework Exercise\nScenario: You’re planning a within-subjects study where n=25 participants complete a task before and after an intervention. You expect a medium effect (d=0.50) and want to know if this sample provides adequate power.\nYour Tasks:\n\nCalculate power for a paired t-test with n=25, d=0.50, α=0.05\nDetermine the required sample size for 80% power\nCreate a power curve showing power vs n (n=10 to 50) for d=0.3, 0.5, 0.8\nCalculate the minimum detectable effect with n=25 and 80% power\nPerform a sensitivity analysis: What power do you have to detect d=0.40 with n=25?\nCompare power for paired vs independent t-tests (use n=25 per group for independent)\nCreate a table comparing required n for 70%, 80%, and 90% power (d=0.50)\nWrite a 3-4 sentence sample size justification for a grant proposal\n\nExpected Results:\n\nWith n=25 paired, d=0.50: power ≈ 0.85 (adequately powered)\nRequired n for 80% power: approximately 27-34 (paired t-test)\nMinimum detectable effect: d ≈ 0.57-0.58\nPaired design is more powerful than independent (requires ~60% fewer participants)\n\nHints: - Use pwr.t.test() with type = \"paired\" for within-subjects - Use map_df() or loops to create power curves over multiple n values - For sensitivity, solve for d given n and power\nBonus: Suppose you can only recruit n=20 (not 27). Should you proceed? Calculate the achieved power and interpret what this means for your ability to detect the expected effect. What alternatives might you consider?",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-mice",
    "href": "chapters/part-g-lab-practicals.html#lab-mice",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 9: Multiple Imputation with MICE",
    "text": "Lab 9: Multiple Imputation with MICE\nLearning Objectives:\n\nIdentify missing data patterns and mechanisms (MCAR, MAR, MNAR)\nPerform multiple imputation using the MICE algorithm\nAssess convergence through trace plots and diagnostics\nPool results across imputed datasets\nCompare complete-case vs multiple imputation analyses\nReport imputation procedures appropriately\n\nScenario: You have survey data from n=50 employees measuring job satisfaction, stress, and performance. However, 20% of values are missing due to survey non-response. You’ll use multiple imputation to handle missing data appropriately.\n\nStep 1: Set Up and Create Data with Missing Values\nCreate realistic dataset with missing data:\n\nlibrary(tidyverse)\nlibrary(mice)\nlibrary(naniar)  # For missing data visualization\nlibrary(VIM)     # For missing data patterns\n\n# Set seed for reproducibility\nset.seed(2025)\n\n# Complete data (for comparison later)\ncomplete_data &lt;- tibble(\n  employee_id = 1:50,\n  age = round(rnorm(50, 35, 8)),\n  tenure_years = round(runif(50, 1, 15), 1),\n  job_satisfaction = round(rnorm(50, 70, 12)),\n  stress_level = round(rnorm(50, 55, 15)),\n  performance_score = round(60 + 0.3 * job_satisfaction - 0.2 * stress_level + rnorm(50, 0, 8))\n)\n\n# Introduce missing data (MAR mechanism)\n# Higher stress predicts missingness in job_satisfaction\n# Lower performance predicts missingness in stress_level\nsurvey_data &lt;- complete_data %&gt;%\n  mutate(\n    job_satisfaction = if_else(\n      stress_level &gt; 60 & runif(50) &lt; 0.3, \n      NA_real_, \n      job_satisfaction\n    ),\n    stress_level = if_else(\n      performance_score &lt; 65 & runif(50) &lt; 0.25,\n      NA_real_,\n      stress_level\n    ),\n    performance_score = if_else(\n      runif(50) &lt; 0.15,\n      NA_real_,\n      performance_score\n    )\n  )\n\n# Missing data summary\ncat(\"Missing Data Summary:\\n\")\n\nMissing Data Summary:\n\nsummary(survey_data)\n\n  employee_id        age        tenure_years   job_satisfaction  stress_level \n Min.   : 1.0   Min.   :21.0   Min.   : 1.10   Min.   :47.0     Min.   :26.0  \n 1st Qu.:13.2   1st Qu.:32.0   1st Qu.: 4.58   1st Qu.:57.2     1st Qu.:44.0  \n Median :25.5   Median :35.5   Median :10.00   Median :66.5     Median :54.0  \n Mean   :25.5   Mean   :36.2   Mean   : 8.58   Mean   :67.7     Mean   :54.8  \n 3rd Qu.:37.8   3rd Qu.:41.0   3rd Qu.:11.30   3rd Qu.:76.8     3rd Qu.:68.0  \n Max.   :50.0   Max.   :58.0   Max.   :14.50   Max.   :88.0     Max.   :89.0  \n                                               NA's   :8        NA's   :5     \n performance_score\n Min.   :47.0     \n 1st Qu.:63.2     \n Median :71.0     \n Mean   :70.3     \n 3rd Qu.:77.0     \n Max.   :89.0     \n NA's   :8        \n\n# Proportion missing per variable\nmiss_var_summary(survey_data)\n\n# A tibble: 6 × 3\n  variable          n_miss pct_miss\n  &lt;chr&gt;              &lt;int&gt;    &lt;num&gt;\n1 job_satisfaction       8       16\n2 performance_score      8       16\n3 stress_level           5       10\n4 employee_id            0        0\n5 age                    0        0\n6 tenure_years           0        0\n\n\nCheckpoint: You should have ~20% missing data across job_satisfaction, stress_level, and performance_score.\n\n\n\nStep 2: Visualize Missing Data Patterns\nExamine patterns and mechanisms:\n\n# Missing data pattern visualization\ngg_miss_var(survey_data) +\n  labs(title = \"Missing Data by Variable\")\n\n\n\n\n\n\n\n# Missing data pattern combinations\nmd_pattern &lt;- md.pattern(survey_data, rotate.names = TRUE)\n\n\n\n\n\n\n\n# Upset plot showing missing combinations\ngg_miss_upset(survey_data)\n\n\n\n\n\n\n\n# Check relationships between missingness and observed values\n# Is missingness related to other variables? (MAR test)\nsurvey_data %&gt;%\n  mutate(\n    satisfaction_missing = is.na(job_satisfaction),\n    stress_missing = is.na(stress_level)\n  ) %&gt;%\n  group_by(satisfaction_missing) %&gt;%\n  summarize(\n    n = n(),\n    mean_stress_observed = mean(stress_level, na.rm = TRUE),\n    mean_age = mean(age, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 4\n  satisfaction_missing     n mean_stress_observed mean_age\n  &lt;lgl&gt;                &lt;int&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1 FALSE                   42                 51.8     35.6\n2 TRUE                     8                 70.9     39.2\n\n\nCheckpoint: You should see that missingness is not completely random (MAR mechanism confirmed).\nDiscussion: What’s the difference between MCAR (Missing Completely at Random), MAR (Missing at Random), and MNAR (Missing Not at Random)?\n\n\n\nStep 3: Perform Multiple Imputation\nRun MICE algorithm with m=20 imputations:\n\n# Set up imputation\n# Exclude employee_id from imputation model\nimpute_vars &lt;- survey_data %&gt;% select(-employee_id)\n\n# Run MICE\n# m = 20 imputed datasets\n# maxit = 20 iterations\n# method = \"pmm\" (predictive mean matching) for continuous variables\nmice_result &lt;- mice(\n  impute_vars,\n  m = 20,\n  maxit = 20,\n  method = \"pmm\",\n  seed = 2025,\n  print = FALSE\n)\n\n# Summary of imputation\nprint(mice_result)\n\nClass: mids\nNumber of multiple imputations:  20 \nImputation methods:\n              age      tenure_years  job_satisfaction      stress_level \n               \"\"                \"\"             \"pmm\"             \"pmm\" \nperformance_score \n            \"pmm\" \nPredictorMatrix:\n                  age tenure_years job_satisfaction stress_level\nage                 0            1                1            1\ntenure_years        1            0                1            1\njob_satisfaction    1            1                0            1\nstress_level        1            1                1            0\nperformance_score   1            1                1            1\n                  performance_score\nage                               1\ntenure_years                      1\njob_satisfaction                  1\nstress_level                      1\nperformance_score                 0\n\n# Check imputation methods used\nmice_result$method\n\n              age      tenure_years  job_satisfaction      stress_level \n               \"\"                \"\"             \"pmm\"             \"pmm\" \nperformance_score \n            \"pmm\" \n\n\nCheckpoint: MICE should complete 20 imputations with 20 iterations each. Method “pmm” used for continuous variables.\n\n\n\nStep 4: Check Convergence Diagnostics\nAssess whether MICE algorithm converged:\n\n# Trace plots (should show mixing without trends)\nplot(mice_result, c(\"job_satisfaction\", \"stress_level\", \"performance_score\"))\n\n\n\n\n\n\n\n# Alternative: ggplot version for publication\nconvergence_data &lt;- mice_result$chainMean %&gt;%\n  as.data.frame() %&gt;%\n  mutate(iteration = row_number()) %&gt;%\n  pivot_longer(-iteration, names_to = \"variable\", values_to = \"mean\")\n\nggplot(convergence_data, aes(x = iteration, y = mean, color = variable)) +\n  geom_line() +\n  facet_wrap(~variable, scales = \"free_y\") +\n  labs(\n    title = \"MICE Convergence: Trace Plots of Imputed Means\",\n    subtitle = \"Lines should stabilize and mix (no trends)\",\n    x = \"Iteration\", y = \"Mean of Imputed Values\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nCheckpoint: Trace plots should show stable streams without upward/downward trends (convergence achieved).\nDiscussion: What would non-convergence look like? What should you do if chains don’t converge?\n\n\n\nStep 5: Inspect Imputed Values\nCheck plausibility of imputed data:\n\n# Compare distributions: observed vs imputed\ndensityplot(mice_result, ~job_satisfaction + stress_level + performance_score)\n\n\n\n\n\n\n\n# Scatterplot: observed (blue) vs imputed (red)\nxyplot(mice_result, \n       job_satisfaction ~ stress_level + performance_score,\n       pch = c(1, 20), \n       cex = c(1, 1.5))\n\n\n\n\n\n\n\n# Numerical comparison\n# Extract first imputed dataset\nimputed_data_1 &lt;- complete(mice_result, 1)\n\n# Compare observed vs imputed distributions\ncomparison_summary &lt;- tibble(\n  Variable = c(\"job_satisfaction\", \"stress_level\", \"performance_score\"),\n  Observed_Mean = c(\n    mean(survey_data$job_satisfaction, na.rm = TRUE),\n    mean(survey_data$stress_level, na.rm = TRUE),\n    mean(survey_data$performance_score, na.rm = TRUE)\n  ),\n  Imputed_Mean = c(\n    mean(imputed_data_1$job_satisfaction),\n    mean(imputed_data_1$stress_level),\n    mean(imputed_data_1$performance_score)\n  ),\n  Observed_SD = c(\n    sd(survey_data$job_satisfaction, na.rm = TRUE),\n    sd(survey_data$stress_level, na.rm = TRUE),\n    sd(survey_data$performance_score, na.rm = TRUE)\n  ),\n  Imputed_SD = c(\n    sd(imputed_data_1$job_satisfaction),\n    sd(imputed_data_1$stress_level),\n    sd(imputed_data_1$performance_score)\n  )\n)\n\nprint(comparison_summary)\n\n# A tibble: 3 × 5\n  Variable          Observed_Mean Imputed_Mean Observed_SD Imputed_SD\n  &lt;chr&gt;                     &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 job_satisfaction           67.7         69.1       11.7        11.8\n2 stress_level               54.8         54.9       15.6        15.3\n3 performance_score          70.3         69.4        9.56       10.1\n\n\nCheckpoint: Imputed values should be plausible (within observed range) and distributions should be similar to observed data.\n\n\n\nStep 6: Analyze Imputed Data and Pool Results\nFit regression model across all imputed datasets:\n\n# Research question: Does job satisfaction predict performance,\n# controlling for stress and age?\n\n# Fit model to each imputed dataset\nfitted_models &lt;- with(mice_result, \n                      lm(performance_score ~ job_satisfaction + stress_level + age))\n\n# Pool results using Rubin's rules\npooled_results &lt;- pool(fitted_models)\n\n# Summary of pooled estimates\nsummary(pooled_results, conf.int = TRUE)\n\n              term estimate std.error statistic    df    p.value   2.5 %\n1      (Intercept)  62.3151  11.20395    5.5619 31.75 0.00000397 39.4865\n2 job_satisfaction   0.4007   0.11897    3.3682 25.67 0.00239443  0.1560\n3     stress_level  -0.2742   0.08975   -3.0546 22.27 0.00575630 -0.4602\n4              age  -0.1111   0.17250   -0.6441 24.93 0.52537872 -0.4664\n    97.5 % conf.low conf.high\n1 85.14378  39.4865  85.14378\n2  0.64542   0.1560   0.64542\n3 -0.08815  -0.4602  -0.08815\n4  0.24421  -0.4664   0.24421\n\n# Extract key statistics\npooled_summary &lt;- summary(pooled_results, conf.int = TRUE) %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    term = term,\n    estimate = round(estimate, 3),\n    std.error = round(std.error, 3),\n    statistic = round(statistic, 2),\n    p.value = round(p.value, 4),\n    `95% CI` = sprintf(\"[%.2f, %.2f]\", `2.5 %`, `97.5 %`)\n  ) %&gt;%\n  select(term, estimate, std.error, statistic, p.value, `95% CI`)\n\nprint(pooled_summary)\n\n# A tibble: 4 × 6\n  term             estimate std.error statistic p.value `95% CI`      \n  &lt;fct&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         \n1 (Intercept)        62.3      11.2        5.56  0      [39.49, 85.14]\n2 job_satisfaction    0.401     0.119      3.37  0.0024 [0.16, 0.65]  \n3 stress_level       -0.274     0.09      -3.05  0.0058 [-0.46, -0.09]\n4 age                -0.111     0.172     -0.64  0.525  [-0.47, 0.24] \n\n# Fraction of missing information (FMI)\n# Higher FMI = more uncertainty due to missing data\npool.r.squared(fitted_models)\n\n      est  lo 95  hi 95   fmi\nR^2 0.416 0.1384 0.6647 0.432\n\n\nCheckpoint: Pooled regression results should show relationship between job satisfaction and performance, with appropriate standard errors accounting for imputation uncertainty.\n\n\n\nStep 7: Compare with Complete-Case Analysis\nContrast MI results with naive complete-case deletion:\n\n# Complete-case analysis (list-wise deletion)\ncomplete_case_data &lt;- survey_data %&gt;% \n  select(-employee_id) %&gt;%\n  na.omit()\n\ncat(\"Sample sizes:\\n\")\n\nSample sizes:\n\ncat(\"- Complete-case analysis:\", nrow(complete_case_data), \"\\n\")\n\n- Complete-case analysis: 32 \n\ncat(\"- Multiple imputation:\", nrow(survey_data), \"\\n\")\n\n- Multiple imputation: 50 \n\ncat(\"- Cases lost:\", nrow(survey_data) - nrow(complete_case_data), \"\\n\")\n\n- Cases lost: 18 \n\n# Fit complete-case model\ncc_model &lt;- lm(performance_score ~ job_satisfaction + stress_level + age,\n               data = complete_case_data)\n\n# Compare estimates\ncomparison_table &lt;- tibble(\n  Predictor = c(\"Intercept\", \"Job Satisfaction\", \"Stress Level\", \"Age\"),\n  CC_Estimate = round(coef(cc_model), 3),\n  CC_SE = round(summary(cc_model)$coef[, 2], 3),\n  MI_Estimate = round(summary(pooled_results)$estimate, 3),\n  MI_SE = round(summary(pooled_results)$std.error, 3),\n  Difference = round(abs(coef(cc_model) - summary(pooled_results)$estimate), 3)\n)\n\nprint(comparison_table)\n\n# A tibble: 4 × 6\n  Predictor        CC_Estimate  CC_SE MI_Estimate  MI_SE Difference\n  &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Intercept             69.2   11.1        62.3   11.2        6.91 \n2 Job Satisfaction       0.424  0.113       0.401  0.119      0.023\n3 Stress Level          -0.32   0.09       -0.274  0.09       0.046\n4 Age                   -0.256  0.168      -0.111  0.172      0.145\n\n# Visualization\ncomparison_long &lt;- comparison_table %&gt;%\n  select(Predictor, CC_Estimate, MI_Estimate) %&gt;%\n  filter(Predictor != \"Intercept\") %&gt;%\n  pivot_longer(-Predictor, names_to = \"Method\", values_to = \"Estimate\") %&gt;%\n  mutate(Method = recode(Method, \n                        \"CC_Estimate\" = \"Complete Case\",\n                        \"MI_Estimate\" = \"Multiple Imputation\"))\n\nggplot(comparison_long, aes(x = Predictor, y = Estimate, fill = Method)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Complete-Case vs Multiple Imputation\",\n    subtitle = \"Regression Coefficient Estimates\",\n    y = \"Coefficient Estimate\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nCheckpoint: Complete-case analysis loses power (smaller n) and may produce biased estimates if data are MAR. MI preserves full sample and accounts for uncertainty.\n\n\n\nStep 8: Report Imputation Procedures\nCreate publication-ready summary:\n\nlibrary(gt)\n\n# Publication table\npooled_summary %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Multiple Imputation Regression Results\",\n    subtitle = \"Predicting Employee Performance (N=50, m=20 imputations)\"\n  ) %&gt;%\n  cols_label(\n    term = \"Predictor\",\n    estimate = \"B\",\n    std.error = \"SE\",\n    statistic = \"t\",\n    p.value = \"p\"\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Multiple imputation performed using MICE (predictive mean matching)\",\n    locations = cells_column_labels(columns = \"estimate\")\n  ) %&gt;%\n  tab_footnote(\n    footnote = sprintf(\"Missing data: job_satisfaction (%.0f%%), stress_level (%.0f%%), performance_score (%.0f%%)\",\n                       mean(is.na(survey_data$job_satisfaction))*100,\n                       mean(is.na(survey_data$stress_level))*100,\n                       mean(is.na(survey_data$performance_score))*100),\n    locations = cells_title()\n  )\n\n\n\n\n\n\n\nMultiple Imputation Regression Results1\n\n\nPredicting Employee Performance (N=50, m=20 imputations)1\n\n\nPredictor\nB2\nSE\nt\np\n95% CI\n\n\n\n\n(Intercept)\n62.315\n11.204\n5.56\n0.0000\n[39.49, 85.14]\n\n\njob_satisfaction\n0.401\n0.119\n3.37\n0.0024\n[0.16, 0.65]\n\n\nstress_level\n-0.274\n0.090\n-3.05\n0.0058\n[-0.46, -0.09]\n\n\nage\n-0.111\n0.172\n-0.64\n0.5254\n[-0.47, 0.24]\n\n\n\n1 Missing data: job_satisfaction (16%), stress_level (10%), performance_score (16%)\n\n\n2 Multiple imputation performed using MICE (predictive mean matching)\n\n\n\n\n\n\n\n# Sample methods statement\ncat(\"\\nSample Methods Statement:\\n\")\n\n\nSample Methods Statement:\n\ncat(\"Missing data were handled using multiple imputation (MI) with the\\n\")\n\nMissing data were handled using multiple imputation (MI) with the\n\ncat(\"MICE algorithm (van Buuren & Groothuis-Oudshoorn, 2011). Variables\\n\")\n\nMICE algorithm (van Buuren & Groothuis-Oudshoorn, 2011). Variables\n\ncat(\"ranged from\", round(min(miss_var_summary(survey_data)$pct_miss), 1), \n    \"% to\", round(max(miss_var_summary(survey_data)$pct_miss), 1), \n    \"% missing.\\n\")\n\nranged from 0 % to 16 % missing.\n\ncat(\"We generated m=20 imputed datasets using predictive mean matching\\n\")\n\nWe generated m=20 imputed datasets using predictive mean matching\n\ncat(\"(PMM) with 20 iterations. Convergence diagnostics (trace plots)\\n\")\n\n(PMM) with 20 iterations. Convergence diagnostics (trace plots)\n\ncat(\"indicated adequate convergence. Regression analyses were performed\\n\")\n\nindicated adequate convergence. Regression analyses were performed\n\ncat(\"on each imputed dataset, and results were pooled using Rubin's rules.\\n\")\n\non each imputed dataset, and results were pooled using Rubin's rules.\n\ncat(\"Complete-case analysis (n=\", nrow(complete_case_data), \n    \") yielded similar results,\\n\")\n\nComplete-case analysis (n= 32 ) yielded similar results,\n\ncat(\"but MI retained the full sample (N=50) and appropriately quantified\\n\")\n\nbut MI retained the full sample (N=50) and appropriately quantified\n\ncat(\"uncertainty due to missing data.\\n\")\n\nuncertainty due to missing data.\n\n\nCheckpoint: Professional reporting following MI best practices.\n\n\n\nDiscussion Questions\n\nMissing Data Mechanisms: How can you determine if data are MCAR, MAR, or MNAR?\nAnswer: Use Little’s MCAR test (null hypothesis: data are MCAR), examine correlations between missingness indicators and observed variables (MAR if related), and consider theory (MNAR if missingness depends on unobserved values). MNAR cannot be tested statistically and requires assumptions.\nNumber of Imputations: Why use m=20 imputations instead of the traditional m=5?\nAnswer: Modern recommendations suggest m ≥ 20 (or equal to percentage missing) for more stable estimates and better power. More imputations improve precision and reproducibility, especially with high missingness or small samples.\nPMM vs Other Methods: Why use predictive mean matching (pmm) instead of regression imputation?\nAnswer: PMM imputes actual observed values (not predictions), preserving data distribution and avoiding impossible values (e.g., negative scores). Regression imputation can produce out-of-range values and underestimate variability.\nWhen Not to Use MI: Are there situations where you shouldn’t use multiple imputation?\nAnswer: (1) MNAR data (requires specialized models), (2) &gt;40-50% missing (insufficient information), (3) key variables completely missing for subgroups, (4) missing data on outcome only (can bias estimates), (5) when missingness IS the research question.\nComplete-Case Bias: Why might complete-case analysis produce biased results even if data are MAR?\nAnswer: Complete-case deletion excludes systematically different cases (e.g., those with high stress), reducing generalizability and potentially biasing estimates. MI uses all available information and maintains proper statistical relationships.\n\n\n\n\nKey Takeaways\n✓ Missing data are common in small samples – handle appropriately, don’t ignore\n✓ Multiple imputation preserves sample size and accounts for uncertainty\n✓ MICE is flexible: handles different variable types and missing patterns\n✓ Check convergence: Trace plots must show stable streams without trends\n✓ Imputed values should be plausible – visually inspect distributions\n✓ Pool results using Rubin’s rules – don’t analyze single imputed dataset\n✓ Always report imputation procedures: m, iterations, methods, convergence\n\n\n\nHomework Exercise\nDataset: A quality improvement study (n=35) measuring patient satisfaction, wait time, and treatment effectiveness. Data have ~25% missingness.\n\nset.seed(789)\npatient_data_complete &lt;- tibble(\n  patient_id = 1:35,\n  age = round(rnorm(35, 55, 12)),\n  wait_time_mins = round(runif(35, 10, 90)),\n  satisfaction_score = round(rnorm(35, 75, 15)),\n  effectiveness_score = round(70 + 0.2 * satisfaction_score - 0.1 * wait_time_mins + rnorm(35, 0, 10))\n)\n\n# Introduce MAR missingness\npatient_data &lt;- patient_data_complete %&gt;%\n  mutate(\n    satisfaction_score = if_else(wait_time_mins &gt; 60 & runif(35) &lt; 0.35, NA_real_, satisfaction_score),\n    effectiveness_score = if_else(satisfaction_score &lt; 65 & runif(35) &lt; 0.30, NA_real_, effectiveness_score),\n    wait_time_mins = if_else(runif(35) &lt; 0.20, NA_real_, wait_time_mins)\n  )\n\nYour Tasks:\n\nCalculate the proportion of missing data for each variable\nVisualize missing data patterns using gg_miss_upset() and md.pattern()\nPerform multiple imputation with m=25 imputations, 25 iterations\nCreate trace plots and assess convergence\nInspect imputed values using densityplot() – are they plausible?\nFit a regression model predicting effectiveness from satisfaction and wait time\nPool results across imputed datasets\nCompare MI results with complete-case analysis (report both n’s and estimates)\nCreate a publication-ready table of pooled regression results\nWrite a methods paragraph describing the imputation procedure\n\nExpected Results:\n\n~25-30% missing across variables\nGood convergence (stable trace plots)\nSatisfaction positively predicts effectiveness (β ≈ 0.15-0.25)\nWait time negatively predicts effectiveness (β ≈ -0.08 to -0.15)\nComplete-case n ≈ 20-25 (vs N=35 for MI)\nMI standard errors slightly larger than complete-case (accounting for imputation uncertainty)\n\nHints: - Use miss_var_summary(data) for missing percentages - Check mice_result$loggedEvents for any imputation warnings - Extract pooled R² with pool.r.squared(fitted_models) - Compare FMI (fraction of missing information) across predictors\nBonus: Perform a sensitivity analysis by re-running MICE with different methods (e.g., “norm” for Bayesian linear regression instead of “pmm”). Do results substantively change? What does this reveal about robustness of findings?",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-screening",
    "href": "chapters/part-g-lab-practicals.html#lab-screening",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 10: Data Screening and Outlier Detection",
    "text": "Lab 10: Data Screening and Outlier Detection\nLearning Objectives:\n\nScreen small-sample data for outliers using multiple methods\nApply univariate detection (z-scores, boxplots, Tukey fences)\nUse multivariate detection (Mahalanobis distance, Cook’s D)\nAssess normality assumptions with Q-Q plots and tests\nMake principled decisions about outlier treatment\nDocument data screening procedures for transparency\n\nScenario: You have collected cognitive performance data (n=30) from a pilot intervention study. Before analysis, you need to screen for outliers, assess normality, and check for influential cases that might distort results.\n\nStep 1: Set Up and Load Data\nCreate dataset with some outliers:\n\nlibrary(tidyverse)\nlibrary(car)        # For outlier tests\nlibrary(performance) # For check_outliers()\nlibrary(MVN)        # For multivariate normality\n\n# Set seed for reproducibility\nset.seed(2025)\n\n# Create data with outliers\ncognitive_data &lt;- tibble(\n  participant_id = 1:30,\n  age = c(round(rnorm(28, 25, 4)), 45, 50),  # 2 age outliers\n  baseline_score = round(rnorm(30, 50, 8)),\n  post_score = round(rnorm(30, 55, 9)),\n  improvement = post_score - baseline_score\n) %&gt;%\n  mutate(\n    # Add one multivariate outlier (high age, low scores)\n    baseline_score = if_else(participant_id == 29, 25, baseline_score),\n    post_score = if_else(participant_id == 29, 28, post_score),\n    improvement = post_score - baseline_score\n  )\n\n# Descriptive statistics\ncognitive_data %&gt;%\n  select(-participant_id) %&gt;%\n  summary()\n\n      age       baseline_score   post_score    improvement    \n Min.   :18.0   Min.   :25.0   Min.   :28.0   Min.   :-14.00  \n 1st Qu.:24.0   1st Qu.:43.5   1st Qu.:47.2   1st Qu.: -5.75  \n Median :25.5   Median :51.5   Median :50.5   Median :  2.00  \n Mean   :27.3   Mean   :49.8   Mean   :52.0   Mean   :  2.20  \n 3rd Qu.:28.0   3rd Qu.:57.0   3rd Qu.:58.5   3rd Qu.: 11.00  \n Max.   :50.0   Max.   :65.0   Max.   :70.0   Max.   : 28.00  \n\n# Visualize raw data\ncognitive_data %&gt;%\n  select(age, baseline_score, post_score, improvement) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = variable, y = value)) +\n  geom_boxplot(fill = \"lightblue\") +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Distribution of Variables (Boxplots)\",\n       x = \"Variable\", y = \"Value\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nCheckpoint: You should see n=30 participants with visible outliers in age and possibly in scores.\n\n\n\nStep 2: Univariate Outlier Detection with Z-Scores\nIdentify outliers using standardized scores:\n\n# Calculate z-scores for all numeric variables\ncognitive_zscores &lt;- cognitive_data %&gt;%\n  mutate(\n    z_age = scale(age)[,1],\n    z_baseline = scale(baseline_score)[,1],\n    z_post = scale(post_score)[,1],\n    z_improvement = scale(improvement)[,1]\n  )\n\n# Flag outliers (|z| &gt; 3.0 for small samples, or &gt; 2.5)\nthreshold &lt;- 2.5\noutliers_zscore &lt;- cognitive_zscores %&gt;%\n  mutate(\n    outlier_age = abs(z_age) &gt; threshold,\n    outlier_baseline = abs(z_baseline) &gt; threshold,\n    outlier_post = abs(z_post) &gt; threshold,\n    outlier_improvement = abs(z_improvement) &gt; threshold,\n    any_outlier = outlier_age | outlier_baseline | outlier_post | outlier_improvement\n  )\n\n# Summary of outliers\ncat(\"Z-Score Outliers (|z| &gt;\", threshold, \"):\\n\")\n\nZ-Score Outliers (|z| &gt; 2.5 ):\n\noutliers_zscore %&gt;%\n  filter(any_outlier) %&gt;%\n  select(participant_id, age, z_age, baseline_score, z_baseline, \n         post_score, z_post, improvement, z_improvement)\n\n# A tibble: 2 × 9\n  participant_id   age z_age baseline_score z_baseline post_score z_post\n           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1             29    45  2.63             25     -2.74          28 -2.35 \n2             30    50  3.37             43     -0.754         46 -0.590\n# ℹ 2 more variables: improvement &lt;dbl&gt;, z_improvement &lt;dbl&gt;\n\n# Count outliers per variable\noutliers_zscore %&gt;%\n  summarize(\n    age_outliers = sum(outlier_age),\n    baseline_outliers = sum(outlier_baseline),\n    post_outliers = sum(outlier_post),\n    improvement_outliers = sum(outlier_improvement),\n    total_flagged = sum(any_outlier)\n  )\n\n# A tibble: 1 × 5\n  age_outliers baseline_outliers post_outliers improvement_outliers\n         &lt;int&gt;             &lt;int&gt;         &lt;int&gt;                &lt;int&gt;\n1            2                 1             0                    0\n# ℹ 1 more variable: total_flagged &lt;int&gt;\n\n\nCheckpoint: Participants 29 and 30 should be flagged as outliers on age; participant 29 also on baseline/post scores.\n\n\n\nStep 3: Tukey Fences (Boxplot Method)\nApply IQR-based outlier detection:\n\n# Function to calculate Tukey fences\ntukey_outliers &lt;- function(x) {\n  Q1 &lt;- quantile(x, 0.25)\n  Q3 &lt;- quantile(x, 0.75)\n  IQR &lt;- Q3 - Q1\n  lower_fence &lt;- Q1 - 1.5 * IQR\n  upper_fence &lt;- Q3 + 1.5 * IQR\n  return(x &lt; lower_fence | x &gt; upper_fence)\n}\n\n# Identify Tukey outliers\ncognitive_tukey &lt;- cognitive_data %&gt;%\n  mutate(\n    tukey_age = tukey_outliers(age),\n    tukey_baseline = tukey_outliers(baseline_score),\n    tukey_post = tukey_outliers(post_score),\n    tukey_improvement = tukey_outliers(improvement),\n    any_tukey_outlier = tukey_age | tukey_baseline | tukey_post | tukey_improvement\n  )\n\n# Summary\ncat(\"Tukey Fence Outliers:\\n\")\n\nTukey Fence Outliers:\n\ncognitive_tukey %&gt;%\n  filter(any_tukey_outlier) %&gt;%\n  select(participant_id, age, tukey_age, baseline_score, tukey_baseline,\n         post_score, tukey_post)\n\n# A tibble: 4 × 7\n  participant_id   age tukey_age baseline_score tukey_baseline post_score\n           &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt;              &lt;dbl&gt; &lt;lgl&gt;               &lt;dbl&gt;\n1             20    35 TRUE                  51 FALSE                  69\n2             23    36 TRUE                  53 FALSE                  45\n3             29    45 TRUE                  25 FALSE                  28\n4             30    50 TRUE                  43 FALSE                  46\n# ℹ 1 more variable: tukey_post &lt;lgl&gt;\n\n# Visualize with outliers highlighted\ncognitive_tukey %&gt;%\n  ggplot(aes(x = age, y = improvement, color = any_tukey_outlier)) +\n  geom_point(size = 3) +\n  geom_text(aes(label = if_else(any_tukey_outlier, \n                                 as.character(participant_id), \"\")),\n            vjust = -0.8, size = 3) +\n  scale_color_manual(values = c(\"black\", \"red\"), \n                     labels = c(\"Normal\", \"Outlier\")) +\n  labs(title = \"Age vs Improvement: Tukey Outliers Highlighted\",\n       x = \"Age\", y = \"Improvement Score\", color = \"\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: Tukey method should identify similar outliers as z-scores, particularly on age variable.\n\n\n\nStep 4: Multivariate Outlier Detection (Mahalanobis Distance)\nDetect cases that are outlying in multivariate space:\n\n# Calculate Mahalanobis distance\n# Include age, baseline, post scores\noutlier_vars &lt;- cognitive_data %&gt;%\n  select(age, baseline_score, post_score) %&gt;%\n  as.matrix()\n\n# Center and covariance\ncenter &lt;- colMeans(outlier_vars)\ncov_matrix &lt;- cov(outlier_vars)\n\n# Mahalanobis distances\nmahal_dist &lt;- mahalanobis(outlier_vars, center, cov_matrix)\n\n# Chi-square critical value (df = number of variables)\n# For 3 variables, chi-sq(3, 0.001) ≈ 13.8\ndf &lt;- ncol(outlier_vars)\ncritical_value &lt;- qchisq(0.999, df)  # p &lt; 0.001\n\n# Flag multivariate outliers\ncognitive_mahal &lt;- cognitive_data %&gt;%\n  mutate(\n    mahal_distance = mahal_dist,\n    mahal_outlier = mahal_dist &gt; critical_value,\n    mahal_p = pchisq(mahal_dist, df, lower.tail = FALSE)\n  )\n\n# Summary\ncat(\"Mahalanobis Distance Outliers (χ² p &lt; 0.001):\\n\")\n\nMahalanobis Distance Outliers (χ² p &lt; 0.001):\n\ncognitive_mahal %&gt;%\n  filter(mahal_outlier) %&gt;%\n  select(participant_id, age, baseline_score, post_score, \n         mahal_distance, mahal_p) %&gt;%\n  arrange(desc(mahal_distance))\n\n# A tibble: 0 × 6\n# ℹ 6 variables: participant_id &lt;int&gt;, age &lt;dbl&gt;, baseline_score &lt;dbl&gt;,\n#   post_score &lt;dbl&gt;, mahal_distance &lt;dbl&gt;, mahal_p &lt;dbl&gt;\n\n# Visualize\ncognitive_mahal %&gt;%\n  ggplot(aes(x = participant_id, y = mahal_distance, \n             color = mahal_outlier)) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = critical_value, linetype = \"dashed\", \n             color = \"red\") +\n  geom_text(aes(label = if_else(mahal_outlier, \n                                 as.character(participant_id), \"\")),\n            vjust = -0.8, size = 3) +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  labs(title = \"Mahalanobis Distance: Multivariate Outliers\",\n       subtitle = sprintf(\"Red line = critical value (χ²(%.0f) = %.2f)\", \n                         df, critical_value),\n       x = \"Participant ID\", y = \"Mahalanobis Distance\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nCheckpoint: Participant 29 should be flagged as multivariate outlier (unusual combination of high age + low scores).\n\n\n\nStep 5: Influential Cases (Cook’s Distance)\nIdentify cases with high influence on regression results:\n\n# Fit regression model (predicting improvement from age and baseline)\nmodel &lt;- lm(improvement ~ age + baseline_score, data = cognitive_data)\n\n# Calculate Cook's distance\ncooks_d &lt;- cooks.distance(model)\n\n# Flag influential cases (Cook's D &gt; 4/n threshold)\nthreshold_cooks &lt;- 4 / nrow(cognitive_data)\n\ncognitive_cooks &lt;- cognitive_data %&gt;%\n  mutate(\n    cooks_d = cooks_d,\n    influential = cooks_d &gt; threshold_cooks\n  )\n\n# Summary\ncat(\"Influential Cases (Cook's D &gt; 4/n =\", round(threshold_cooks, 3), \"):\\n\")\n\nInfluential Cases (Cook's D &gt; 4/n = 0.133 ):\n\ncognitive_cooks %&gt;%\n  filter(influential) %&gt;%\n  select(participant_id, age, baseline_score, improvement, cooks_d) %&gt;%\n  arrange(desc(cooks_d))\n\n# A tibble: 1 × 5\n  participant_id   age baseline_score improvement cooks_d\n           &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n1             29    45             25           3   0.522\n\n# Visualization with Cook's distance plot\ncognitive_cooks %&gt;%\n  ggplot(aes(x = participant_id, y = cooks_d, fill = influential)) +\n  geom_col() +\n  geom_hline(yintercept = threshold_cooks, linetype = \"dashed\", \n             color = \"red\") +\n  scale_fill_manual(values = c(\"gray70\", \"coral\")) +\n  labs(title = \"Cook's Distance: Influential Cases\",\n       subtitle = sprintf(\"Threshold = 4/n = %.3f\", threshold_cooks),\n       x = \"Participant ID\", y = \"Cook's Distance\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Influence plot\ninfluencePlot(model, main = \"Influence Plot\", \n              id = list(n = 3, labels = cognitive_data$participant_id))\n\n\n\n\n\n\n\n\n   StudRes     Hat    CookD\n4    0.348 0.18230 0.009302\n16  -1.900 0.05275 0.061117\n20   1.958 0.09471 0.121031\n24   2.303 0.07912 0.130972\n29  -1.639 0.38247 0.522164\n30  -0.128 0.44496 0.004542\n\n\nCheckpoint: Some outliers (especially participant 29) should show high Cook’s D, indicating strong influence on regression coefficients.\n\n\n\nStep 6: Assess Normality Assumptions\nCheck if data meet normality assumptions:\nCheckpoint: Age likely violates normality (due to outliers). Other variables may be approximately normal.\nDiscussion: When do normality violations matter? When can you proceed despite non-normality?\n\n\n\nStep 7: Compare Analyses With/Without Outliers\nAssess impact of outliers on results:\n\n# Original model (with outliers)\nmodel_full &lt;- lm(improvement ~ age + baseline_score, data = cognitive_data)\n\n# Model without identified outliers\n# Remove participants flagged by multiple methods\noutliers_to_remove &lt;- cognitive_mahal %&gt;%\n  filter(mahal_outlier) %&gt;%\n  pull(participant_id)\n\ncognitive_clean &lt;- cognitive_data %&gt;%\n  filter(!participant_id %in% outliers_to_remove)\n\nmodel_clean &lt;- lm(improvement ~ age + baseline_score, data = cognitive_clean)\n\n# Comparison table\ncomparison &lt;- tibble(\n  Model = c(\"With Outliers\", \"Without Outliers\"),\n  N = c(nrow(cognitive_data), nrow(cognitive_clean)),\n  `Intercept` = c(coef(model_full)[1], coef(model_clean)[1]),\n  `β (Age)` = c(coef(model_full)[2], coef(model_clean)[2]),\n  `β (Baseline)` = c(coef(model_full)[3], coef(model_clean)[3]),\n  `R²` = c(summary(model_full)$r.squared, summary(model_clean)$r.squared),\n  `RMSE` = c(sigma(model_full), sigma(model_clean))\n)\n\nprint(comparison)\n\n# A tibble: 2 × 7\n  Model                N Intercept `β (Age)` `β (Baseline)`  `R²`  RMSE\n  &lt;chr&gt;            &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 With Outliers       30      34.7    -0.102         -0.596 0.230  9.75\n2 Without Outliers    30      34.7    -0.102         -0.596 0.230  9.75\n\n# Visualization\ncomparison %&gt;%\n  select(Model, `β (Age)`, `β (Baseline)`) %&gt;%\n  pivot_longer(-Model, names_to = \"Coefficient\", values_to = \"Estimate\") %&gt;%\n  ggplot(aes(x = Coefficient, y = Estimate, fill = Model)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Regression Coefficients: With vs Without Outliers\",\n       y = \"Coefficient Estimate\") +\n  theme_minimal()\n\n\n\n\n\n\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\ncat(\"Removing outliers changed coefficients by:\\n\")\n\nRemoving outliers changed coefficients by:\n\ncat(\"- Age:\", round(abs(coef(model_full)[2] - coef(model_clean)[2]), 3), \"\\n\")\n\n- Age: 0 \n\ncat(\"- Baseline:\", round(abs(coef(model_full)[3] - coef(model_clean)[3]), 3), \"\\n\")\n\n- Baseline: 0 \n\ncat(\"- R² increased by:\", \n    round((summary(model_clean)$r.squared - summary(model_full)$r.squared), 3), \"\\n\")\n\n- R² increased by: 0 \n\n\nCheckpoint: Removing outliers should improve model fit (higher R²) and potentially change coefficient estimates.\n\n\n\nStep 8: Document Screening Decisions\nCreate transparent reporting:\nCheckpoint: Professional documentation suitable for methods section or supplementary materials.\n\n\n\nDiscussion Questions\n\nMultiple Methods: Why use multiple outlier detection methods instead of just one?\nAnswer: Different methods detect different types of outliers (univariate vs multivariate, influential vs extreme). Convergent evidence from multiple methods increases confidence. Cases flagged by only one method may be borderline and warrant closer inspection.\nTo Remove or Not: When should you remove outliers vs retain them?\nAnswer: Remove if: (1) data entry error (verify first), (2) procedural error (e.g., equipment malfunction), (3) not from target population. Retain if: (1) legitimate extreme values from population, (2) no clear reason for exclusion, (3) reflects real variability. Always report analyses both ways.\nSmall Sample Impact: Why are outliers particularly problematic in small samples?\nAnswer: With small n, a single outlier can have disproportionate influence on means, correlations, and regression coefficients. One case can change statistical significance. Effect is diluted in large samples.\nNormality Violations: The Shapiro-Wilk test rejected normality, but should you still use parametric tests?\nAnswer: Depends on: (1) severity of violation, (2) sample size (CLT helps with larger n), (3) robustness of test (t-tests are robust to moderate non-normality), (4) availability of alternatives (nonparametric tests, transformations, robust methods).\nTransparency: What should you report about outlier treatment?\nAnswer: Report: (1) detection methods used, (2) number and identification of outliers, (3) decisions made (kept/removed), (4) justification for decisions, (5) sensitivity analyses showing results with/without outliers. Pre-register outlier criteria when possible.\n\n\n\n\nKey Takeaways\n✓ Screen data systematically using multiple detection methods\n✓ Univariate outliers (z-scores, Tukey) detect extreme values on single variables\n✓ Multivariate outliers (Mahalanobis) detect unusual combinations of values\n✓ Influential cases (Cook’s D) have high leverage on regression results\n✓ Always compare results with and without outliers (sensitivity analysis)\n✓ Document decisions transparently – report criteria, cases removed, justification\n✓ In small samples, outliers have disproportionate impact on results\n\n\n\nHomework Exercise\nDataset: A pilot training study (n=28) measuring reaction time, accuracy, and learning rate.\n\nset.seed(321)\ntraining_data &lt;- tibble(\n  participant_id = 1:28,\n  reaction_time_ms = c(round(rnorm(26, 300, 40)), 150, 520),  # 2 outliers\n  accuracy_pct = c(round(rnorm(26, 85, 8)), 55, 98),\n  learning_rate = round(rnorm(28, 12, 4), 1)\n) %&gt;%\n  mutate(\n    # Add multivariate outlier\n    accuracy_pct = if_else(participant_id == 27, 55, accuracy_pct),\n    learning_rate = if_else(participant_id == 27, 3, learning_rate)\n  )\n\nYour Tasks:\n\nCreate boxplots for all three variables\nCalculate z-scores and flag outliers (|z| &gt; 2.5)\nApply Tukey fences to identify IQR-based outliers\nCalculate Mahalanobis distances and flag multivariate outliers (p &lt; .001)\nFit a regression model (reaction_time ~ accuracy + learning_rate) and calculate Cook’s D\nIdentify influential cases (Cook’s D &gt; 4/n)\nCreate Q-Q plots and run Shapiro-Wilk tests for normality\nCompare regression results with vs without identified outliers\nCreate a data screening summary table\nWrite a methods paragraph documenting your screening process\n\nExpected Results:\n\nParticipants 27 and 28 flagged by multiple methods\nParticipant 27: multivariate outlier (low accuracy + low learning rate)\nParticipant 28: univariate outlier (high reaction time)\nShapiro-Wilk may reject normality for reaction_time\nRemoving outliers should improve regression R² and reduce RMSE\nAt least 2-3 cases exceed Cook’s D threshold\n\nHints: - Use scale() for z-scores - For Mahalanobis: mahalanobis(data, center, cov) with qchisq(0.999, df) - Check influencePlot(model) for visual diagnostic - Compare summary(model_full) vs summary(model_clean)\nBonus: Perform a Winsorization alternative: instead of removing outliers, replace them with the value at the 5th/95th percentile. Compare results from (1) original data, (2) outliers removed, and (3) outliers Winsorized. Which approach is most conservative? Most liberal?",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-effect-sizes",
    "href": "chapters/part-g-lab-practicals.html#lab-effect-sizes",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 11: Effect Sizes and Confidence Intervals",
    "text": "Lab 11: Effect Sizes and Confidence Intervals\nLearning Objectives:\n\nCalculate standardized effect sizes (Cohen’s d, r, odds ratios)\nCompute confidence intervals for effect sizes\nInterpret effect size magnitudes using Cohen’s benchmarks\nDistinguish between statistical and practical significance\nReport effect sizes following APA guidelines\nVisualize effect sizes with forest plots\n\nScenario: You conducted a small pilot study (n=24, 12 per group) comparing two teaching methods on test scores. The results are statistically significant, but you need to quantify and interpret the effect size to assess practical importance.\n\nStep 1: Set Up and Load Data\nCreate two-group comparison data:\n\nlibrary(tidyverse)\nlibrary(effsize)    # For effect size calculations\nlibrary(MBESS)      # For CIs on effect sizes\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(2025)\n\n# Two-group study: Method A vs Method B\nteaching_data &lt;- tibble(\n  student_id = 1:24,\n  method = rep(c(\"Method A\", \"Method B\"), each = 12),\n  test_score = c(\n    round(rnorm(12, 75, 10)),  # Method A: M=75, SD=10\n    round(rnorm(12, 85, 12))   # Method B: M=85, SD=12\n  )\n)\n\n# Descriptive statistics\nteaching_summary &lt;- teaching_data %&gt;%\n  group_by(method) %&gt;%\n  summarize(\n    n = n(),\n    M = mean(test_score),\n    SD = sd(test_score),\n    SE = SD / sqrt(n),\n    .groups = \"drop\"\n  )\n\nprint(teaching_summary)\n\n# A tibble: 2 × 5\n  method       n     M    SD    SE\n  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Method A    12  76.2  7.91  2.28\n2 Method B    12  91.7 14.1   4.06\n\n# Visualize raw data\nggplot(teaching_data, aes(x = method, y = test_score, fill = method)) +\n  geom_boxplot(alpha = 0.5) +\n  geom_jitter(width = 0.2, size = 2, alpha = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, \n               size = 4, fill = \"red\") +\n  labs(title = \"Test Scores by Teaching Method\",\n       x = \"Teaching Method\", y = \"Test Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nCheckpoint: You should see n=12 per group with Method B showing higher mean scores.\n\n\n\nStep 2: Calculate Cohen’s d\nCompute standardized mean difference:\n\n# Extract groups\nmethod_a &lt;- teaching_data %&gt;% filter(method == \"Method A\") %&gt;% pull(test_score)\nmethod_b &lt;- teaching_data %&gt;% filter(method == \"Method B\") %&gt;% pull(test_score)\n\n# Cohen's d with pooled SD\ncohen_d_result &lt;- cohen.d(method_b, method_a)\nprint(cohen_d_result)\n\n\nCohen's d\n\nd estimate: 1.358 (large)\n95 percent confidence interval:\n lower  upper \n0.4188 2.2971 \n\n# Manual calculation for verification\nm1 &lt;- mean(method_a)\nm2 &lt;- mean(method_b)\nsd1 &lt;- sd(method_a)\nsd2 &lt;- sd(method_b)\nn1 &lt;- length(method_a)\nn2 &lt;- length(method_b)\n\n# Pooled SD\npooled_sd &lt;- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))\n\n# Cohen's d\nd &lt;- (m2 - m1) / pooled_sd\n\ncat(\"\\nManual Calculation:\\n\")\n\n\nManual Calculation:\n\ncat(\"Mean difference:\", round(m2 - m1, 2), \"\\n\")\n\nMean difference: 15.5 \n\ncat(\"Pooled SD:\", round(pooled_sd, 2), \"\\n\")\n\nPooled SD: 11.41 \n\ncat(\"Cohen's d:\", round(d, 3), \"\\n\")\n\nCohen's d: 1.358 \n\n# Interpretation using Cohen's benchmarks\ninterpretation &lt;- case_when(\n  abs(d) &lt; 0.2 ~ \"Negligible\",\n  abs(d) &lt; 0.5 ~ \"Small\",\n  abs(d) &lt; 0.8 ~ \"Medium\",\n  TRUE ~ \"Large\"\n)\ncat(\"Interpretation:\", interpretation, \"effect\\n\")\n\nInterpretation: Large effect\n\n\nCheckpoint: Cohen’s d should be approximately 0.6-0.9 (medium to large effect).\n\n\n\nStep 3: Compute Confidence Interval for d\nCalculate 95% CI around the effect size:\n\n# Confidence interval for Cohen's d using MBESS\nci_d &lt;- ci.smd(\n  smd = d,\n  n.1 = n1,\n  n.2 = n2,\n  conf.level = 0.95\n)\n\ncat(\"95% Confidence Interval for Cohen's d:\\n\")\n\n95% Confidence Interval for Cohen's d:\n\ncat(\"[\", round(ci_d$Lower.Conf.Limit.smd, 3), \", \",\n    round(ci_d$Upper.Conf.Limit.smd, 3), \"]\\n\", sep = \"\")\n\n[0.452, 2.24]\n\n# Alternative: bootstrap CI\nlibrary(boot)\ncohens_d_boot &lt;- function(data, indices) {\n  d &lt;- data[indices, ]\n  a &lt;- d %&gt;% filter(method == \"Method A\") %&gt;% pull(test_score)\n  b &lt;- d %&gt;% filter(method == \"Method B\") %&gt;% pull(test_score)\n  \n  m_a &lt;- mean(a)\n  m_b &lt;- mean(b)\n  sd_pooled &lt;- sqrt(((length(a) - 1) * sd(a)^2 + \n                      (length(b) - 1) * sd(b)^2) / \n                     (length(a) + length(b) - 2))\n  \n  return((m_b - m_a) / sd_pooled)\n}\n\nboot_result &lt;- boot(teaching_data, cohens_d_boot, R = 2000)\nboot_ci &lt;- boot.ci(boot_result, type = \"bca\")\n\ncat(\"\\nBootstrap 95% CI (BCa method):\\n\")\n\n\nBootstrap 95% CI (BCa method):\n\ncat(\"[\", round(boot_ci$bca[4], 3), \", \", \n    round(boot_ci$bca[5], 3), \"]\\n\", sep = \"\")\n\n[0.421, 2.092]\n\n# Visualize effect size with CI\neffect_plot_data &lt;- tibble(\n  Effect = \"Cohen's d\",\n  Estimate = d,\n  Lower = ci_d$Lower.Conf.Limit.smd,\n  Upper = ci_d$Upper.Conf.Limit.smd\n)\n\nggplot(effect_plot_data, aes(x = Effect, y = Estimate)) +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, linewidth = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_hline(yintercept = c(0.2, 0.5, 0.8), \n             linetype = \"dotted\", color = \"blue\", alpha = 0.5) +\n  annotate(\"text\", x = 1.3, y = c(0.2, 0.5, 0.8), \n           label = c(\"Small\", \"Medium\", \"Large\"), \n           color = \"blue\", size = 3) +\n  labs(title = \"Effect Size with 95% Confidence Interval\",\n       subtitle = \"Method B vs Method A\",\n       y = \"Cohen's d\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: 95% CI should exclude zero, confirming significant difference. Width reflects small sample uncertainty.\n\n\n\nStep 4: Calculate Alternative Effect Sizes\nCompute correlation-based effect size:\n\n# Point-biserial correlation (r)\n# Create binary group variable (0 = Method A, 1 = Method B)\nteaching_data_binary &lt;- teaching_data %&gt;%\n  mutate(group_code = if_else(method == \"Method A\", 0, 1))\n\nr_pb &lt;- cor(teaching_data_binary$group_code, teaching_data_binary$test_score)\n\ncat(\"Point-Biserial Correlation: r =\", round(r_pb, 3), \"\\n\")\n\nPoint-Biserial Correlation: r = 0.578 \n\n# Convert between d and r\nr_from_d &lt;- d / sqrt(d^2 + 4)\nd_from_r &lt;- (2 * r_pb) / sqrt(1 - r_pb^2)\n\ncat(\"Conversion: d =\", round(d, 3), \"→ r =\", round(r_from_d, 3), \"\\n\")\n\nConversion: d = 1.358 → r = 0.562 \n\ncat(\"Conversion: r =\", round(r_pb, 3), \"→ d =\", round(d_from_r, 3), \"\\n\")\n\nConversion: r = 0.578 → d = 1.418 \n\n# R-squared (proportion of variance explained)\nr_squared &lt;- r_pb^2\ncat(\"\\nR² (variance explained):\", round(r_squared, 3), \n    \"(\", round(r_squared * 100, 1), \"%)\\n\")\n\n\nR² (variance explained): 0.335 ( 33.5 %)\n\n# Number Needed to Treat (NNT) approximation\n# Assumes binary outcome; here illustrative only\n# For continuous outcome, use BESD or CLES\n\n# Common Language Effect Size (CLES)\n# Probability that random Method B score &gt; Method A score\ncles &lt;- pnorm(d / sqrt(2))\ncat(\"\\nCommon Language Effect Size:\", round(cles, 3), \"\\n\")\n\n\nCommon Language Effect Size: 0.832 \n\ncat(\"Interpretation:\", round(cles * 100, 1), \n    \"% chance Method B student scores higher\\n\")\n\nInterpretation: 83.2 % chance Method B student scores higher\n\n\nCheckpoint: Point-biserial r should be around 0.4-0.5. CLES ~70-75% (Method B advantage).\n\n\n\nStep 5: Statistical vs Practical Significance\nCompare p-value and effect size interpretations:\n\n# t-test for statistical significance\nt_test_result &lt;- t.test(test_score ~ method, data = teaching_data, \n                        var.equal = TRUE)\n\ncat(\"Statistical Significance:\\n\")\n\nStatistical Significance:\n\ncat(\"t(\", t_test_result$parameter, \") = \", \n    round(t_test_result$statistic, 2), \n    \", p = \", round(t_test_result$p.value, 4), \"\\n\", sep = \"\")\n\nt(22) = -3.33, p = 0.0031\n\ncat(\"Conclusion:\", \n    if_else(t_test_result$p.value &lt; 0.05, \n            \"Statistically significant\", \n            \"Not statistically significant\"), \"\\n\\n\")\n\nConclusion: Statistically significant \n\ncat(\"Practical Significance:\\n\")\n\nPractical Significance:\n\ncat(\"Cohen's d =\", round(d, 2), \"(\", interpretation, \"effect)\\n\")\n\nCohen's d = 1.36 ( Large effect)\n\ncat(\"Mean difference:\", round(m2 - m1, 1), \"points\\n\")\n\nMean difference: 15.5 points\n\ncat(\"95% CI for difference: [\", \n    round(t_test_result$conf.int[1], 1), \", \",\n    round(t_test_result$conf.int[2], 1), \"]\\n\", sep = \"\")\n\n95% CI for difference: [-25.2, -5.8]\n\n# Scenario analysis\ncat(\"\\nPractical Interpretation:\\n\")\n\n\nPractical Interpretation:\n\ncat(\"- A\", round(m2 - m1, 1), \"point improvement on a 100-point test\\n\")\n\n- A 15.5 point improvement on a 100-point test\n\ncat(\"- Represents approximately\", round((m2 - m1) / m1 * 100, 1), \n    \"% improvement over Method A\\n\")\n\n- Represents approximately 20.4 % improvement over Method A\n\ncat(\"- CLES =\", round(cles, 2), \n    \": ~75% of Method B students outperform average Method A student\\n\")\n\n- CLES = 0.83 : ~75% of Method B students outperform average Method A student\n\ncat(\"- Question: Is this improvement worth the cost/effort of Method B?\\n\")\n\n- Question: Is this improvement worth the cost/effort of Method B?\n\n\nCheckpoint: Result may be both statistically and practically significant, but small sample yields uncertainty.\nDiscussion: Can a result be statistically significant but not practically important? Vice versa?\n\n\n\nStep 6: Effect Sizes for Categorical Outcomes\nCalculate odds ratios (illustrative with dichotomized outcome):\n\n# Dichotomize test scores (pass = ≥80, fail = &lt;80)\nteaching_dichot &lt;- teaching_data %&gt;%\n  mutate(\n    pass = if_else(test_score &gt;= 80, \"Pass\", \"Fail\"),\n    pass_binary = if_else(pass == \"Pass\", 1, 0)\n  )\n\n# Contingency table\ncont_table &lt;- table(teaching_dichot$method, teaching_dichot$pass)\nprint(cont_table)\n\n          \n           Fail Pass\n  Method A    8    4\n  Method B    1   11\n\n# Proportions\nprop_table &lt;- prop.table(cont_table, margin = 1)\nprint(round(prop_table, 3))\n\n          \n            Fail  Pass\n  Method A 0.667 0.333\n  Method B 0.083 0.917\n\n# Odds ratio\nodds_a &lt;- cont_table[1, 2] / cont_table[1, 1]  # Pass/Fail for Method A\nodds_b &lt;- cont_table[2, 2] / cont_table[2, 1]  # Pass/Fail for Method B\nodds_ratio &lt;- odds_b / odds_a\n\ncat(\"\\nOdds Ratio:\\n\")\n\n\nOdds Ratio:\n\ncat(\"Odds (Method A):\", round(odds_a, 2), \"\\n\")\n\nOdds (Method A): 0.5 \n\ncat(\"Odds (Method B):\", round(odds_b, 2), \"\\n\")\n\nOdds (Method B): 11 \n\ncat(\"OR =\", round(odds_ratio, 2), \"\\n\")\n\nOR = 22 \n\ncat(\"Interpretation: Method B students have\", round(odds_ratio, 2), \n    \"times the odds of passing\\n\")\n\nInterpretation: Method B students have 22 times the odds of passing\n\n# Risk Ratio (RR)\nrisk_a &lt;- cont_table[1, 2] / sum(cont_table[1, ])\nrisk_b &lt;- cont_table[2, 2] / sum(cont_table[2, ])\nrisk_ratio &lt;- risk_b / risk_a\n\ncat(\"\\nRisk Ratio:\\n\")\n\n\nRisk Ratio:\n\ncat(\"Risk (Method A):\", round(risk_a, 3), \"\\n\")\n\nRisk (Method A): 0.333 \n\ncat(\"Risk (Method B):\", round(risk_b, 3), \"\\n\")\n\nRisk (Method B): 0.917 \n\ncat(\"RR =\", round(risk_ratio, 2), \"\\n\")\n\nRR = 2.75 \n\n# Risk difference\nrisk_diff &lt;- risk_b - risk_a\ncat(\"\\nRisk Difference:\", round(risk_diff, 3), \n    \"(\", round(risk_diff * 100, 1), \" percentage points)\\n\")\n\n\nRisk Difference: 0.583 ( 58.3  percentage points)\n\n\nCheckpoint: OR and RR should show Method B advantage in passing rates.\n\n\n\nStep 7: Create Forest Plot\nVisualize multiple effect sizes:\n\n# Compile effect sizes\nforest_data &lt;- tibble(\n  Metric = c(\"Cohen's d\", \"Point-Biserial r\", \"CLES\", \n             \"Mean Difference\", \"Odds Ratio\", \"Risk Ratio\"),\n  Estimate = c(d, r_pb, cles, m2 - m1, odds_ratio, risk_ratio),\n  Lower = c(ci_d$Lower.Conf.Limit.smd, NA, NA, \n            t_test_result$conf.int[1], NA, NA),\n  Upper = c(ci_d$Upper.Conf.Limit.smd, NA, NA,\n            t_test_result$conf.int[2], NA, NA),\n  Category = c(\"Standardized\", \"Standardized\", \"Probability\",\n               \"Unstandardized\", \"Categorical\", \"Categorical\")\n) %&gt;%\n  filter(!is.na(Lower))  # Keep only metrics with CIs\n\n# Forest plot\nggplot(forest_data, aes(x = Estimate, y = Metric, color = Category)) +\n  geom_point(size = 4) +\n  geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2, linewidth = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Effect Sizes with 95% Confidence Intervals\",\n       subtitle = \"Method B vs Method A\",\n       x = \"Effect Size Estimate\", y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nCheckpoint: Forest plot displays effect sizes with uncertainty intervals in publication-ready format.\n\n\n\nStep 8: Report Effect Sizes (APA Style)\nCreate publication-ready summary:\n\nlibrary(gt)\n\n# Summary table\neffect_summary &lt;- tibble(\n  `Effect Size Metric` = c(\"Cohen's d\", \"95% CI\", \"Point-Biserial r\", \n                           \"R²\", \"CLES\", \"Mean Difference\"),\n  Value = c(\n    sprintf(\"%.2f\", d),\n    sprintf(\"[%.2f, %.2f]\", ci_d$Lower.Conf.Limit.smd, ci_d$Upper.Conf.Limit.smd),\n    sprintf(\"%.2f\", r_pb),\n    sprintf(\"%.3f (%.1f%%)\", r_squared, r_squared * 100),\n    sprintf(\"%.2f (%.0f%%)\", cles, cles * 100),\n    sprintf(\"%.1f points\", m2 - m1)\n  ),\n  Interpretation = c(\n    interpretation,\n    \"Excludes zero\",\n    \"Medium association\",\n    sprintf(\"%.1f%% variance explained\", r_squared * 100),\n    \"~3 in 4 advantage\",\n    sprintf(\"%.1f%% improvement\", (m2 - m1) / m1 * 100)\n  )\n)\n\neffect_summary %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Effect Size Summary\",\n    subtitle = \"Teaching Method Comparison (n=24)\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = \"Cohen's benchmarks: small (0.2), medium (0.5), large (0.8)\"\n  )\n\n\n\n\n\n\n\nEffect Size Summary\n\n\nTeaching Method Comparison (n=24)\n\n\nEffect Size Metric\nValue\nInterpretation\n\n\n\n\nCohen's d\n1.36\nLarge\n\n\n95% CI\n[0.45, 2.24]\nExcludes zero\n\n\nPoint-Biserial r\n0.58\nMedium association\n\n\nR²\n0.335 (33.5%)\n33.5% variance explained\n\n\nCLES\n0.83 (83%)\n~3 in 4 advantage\n\n\nMean Difference\n15.5 points\n20.4% improvement\n\n\n\nCohen's benchmarks: small (0.2), medium (0.5), large (0.8)\n\n\n\n\n\n\n\n# Sample Results Statement\ncat(\"\\nSample APA-Style Results Statement:\\n\")\n\n\nSample APA-Style Results Statement:\n\ncat(\"Method B (M =\", round(m2, 1), \", SD =\", round(sd2, 1), \n    \") produced significantly higher\\n\")\n\nMethod B (M = 91.7 , SD = 14.1 ) produced significantly higher\n\ncat(\"test scores than Method A (M =\", round(m1, 1), \", SD =\", round(sd1, 1), \n    \"), t(\", t_test_result$parameter, \") =\\n\", sep = \"\")\n\ntest scores than Method A (M =76.2, SD =7.9), t(22) =\n\ncat(round(t_test_result$statistic, 2), \", p =\", \n    round(t_test_result$p.value, 3), \", d =\", round(d, 2), \n    \", 95% CI [\", round(ci_d$Lower.Conf.Limit.smd, 2), \", \",\n    round(ci_d$Upper.Conf.Limit.smd, 2), \"].\\n\", sep = \"\")\n\n-3.33, p =0.003, d =1.36, 95% CI [0.45, 2.24].\n\ncat(\"This represents a\", tolower(interpretation), \n    \"effect, with approximately\", round(cles * 100, 0), \n    \"% of Method B\\n\")\n\nThis represents a large effect, with approximately 83 % of Method B\n\ncat(\"students scoring above the average Method A student (CLES).\\n\")\n\nstudents scoring above the average Method A student (CLES).\n\n\nCheckpoint: Professional reporting with multiple effect size metrics and clear interpretation.\n\n\n\nDiscussion Questions\n\nWhy Report Effect Sizes: What information does an effect size provide that a p-value doesn’t?\nAnswer: Effect sizes quantify magnitude of difference/association (how big?), are independent of sample size, allow comparison across studies, inform practical importance, and enable meta-analysis. P-values only indicate probability under null hypothesis.\nWhich Effect Size: When should you report Cohen’s d vs r vs odds ratios?\nAnswer: Cohen’s d for mean differences (t-tests, ANOVA), r for correlations, odds/risk ratios for categorical outcomes. Report the metric most appropriate for your design and most interpretable for your audience. Multiple metrics often helpful.\nConfidence Intervals: Why are CIs for effect sizes important, especially in small samples?\nAnswer: CIs quantify precision/uncertainty around effect size estimate. In small samples, CIs are wide, revealing substantial uncertainty even when result is significant. Helps distinguish “precisely small” from “imprecisely estimated.”\nPractical vs Statistical: Give an example where d=0.3 might be practically important despite being “small.”\nAnswer: In education, a d=0.3 improvement in test scores with no added cost might benefit millions of students. In medicine, small effects on mortality are critically important. “Small” is context-dependent.\nReporting Standards: What effect size information should you always report?\nAnswer: (1) Point estimate (e.g., d=0.65), (2) 95% CI, (3) sample sizes, (4) interpretation (small/medium/large or context-specific), (5) unstandardized effect when meaningful (e.g., 10-point difference).\n\n\n\n\nKey Takeaways\n✓ Effect sizes quantify magnitude, not just significance\n✓ Cohen’s d is standardized – comparable across studies\n✓ Always report confidence intervals for effect sizes\n✓ Multiple effect size metrics provide complementary perspectives\n✓ Practical significance ≠ statistical significance – consider context\n✓ Small samples yield wide CIs – interpret effect sizes cautiously\n✓ APA guidelines require effect sizes in results sections\n\n\n\nHomework Exercise\nDataset: A wellness intervention study (n=20, 10 per group) measuring stress reduction.\n\nset.seed(654)\nwellness_data &lt;- tibble(\n  participant_id = 1:20,\n  group = rep(c(\"Control\", \"Intervention\"), each = 10),\n  stress_baseline = round(rnorm(20, 65, 12)),\n  stress_post = c(\n    round(rnorm(10, 63, 11)),      # Control: minimal change\n    round(rnorm(10, 48, 10))       # Intervention: larger reduction\n  )\n) %&gt;%\n  mutate(stress_change = stress_post - stress_baseline)\n\nYour Tasks:\n\nCalculate descriptive statistics (M, SD) for each group’s stress change\nConduct independent t-test on stress_change\nCalculate Cohen’s d for the group difference\nCompute 95% CI for Cohen’s d using MBESS\nCalculate point-biserial correlation (r) and R²\nCalculate Common Language Effect Size (CLES)\nInterpret effect size using Cohen’s benchmarks\nCreate a forest plot showing d with 95% CI\nCompare statistical significance (p-value) with practical significance (effect size)\nWrite an APA-style results statement reporting all relevant statistics\n\nExpected Results:\n\nIntervention group shows greater stress reduction (larger negative change)\nCohen’s d approximately 1.0-1.5 (large effect)\nStatistically significant (p &lt; .05)\n95% CI for d should exclude zero but be fairly wide (small sample)\nCLES ~75-85% (intervention advantage)\nR² ~25-40% (substantial variance explained)\n\nHints: - Use stress_change as outcome (negative values = stress reduction) - For d: cohen.d(group2, group1) or manual calculation with pooled SD - For CI: ci.smd(smd = d, n.1 = 10, n.2 = 10) - CLES = pnorm(d / sqrt(2))\nBonus: Dichotomize the outcome: “Clinically meaningful reduction” = stress change ≤ -10 points. Calculate odds ratio and risk ratio for the intervention vs control. Which metric (d, OR, RR) is most interpretable for a clinical audience? Why?",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-g-lab-practicals.html#lab-visualization",
    "href": "chapters/part-g-lab-practicals.html#lab-visualization",
    "title": "Part G: Guided Lab Practicals",
    "section": "Lab 12: Visualization Best Practices for Small Samples",
    "text": "Lab 12: Visualization Best Practices for Small Samples\nLearning Objectives:\n\nCreate effective visualizations for small-sample data\nShow individual data points (avoid hiding observations)\nDisplay uncertainty with error bars and confidence bands\nUse appropriate plot types for different data structures\nApply principles of clear, honest data presentation\nGenerate publication-ready figures with ggplot2\n\nScenario: You have pilot data (n=18) from a within-subjects study where participants completed a task under three conditions. You need to create visualizations that clearly communicate patterns while honestly representing uncertainty.\n\nStep 1: Set Up and Load Data\nCreate within-subjects data:\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(ggbeeswarm)  # For jittered points\nlibrary(ggdist)      # For distribution visualizations\nlibrary(patchwork)   # For combining plots\n\n# Set seed for reproducibility\nset.seed(2025)\n\n# Within-subjects data: 3 conditions\npilot_data &lt;- tibble(\n  participant_id = rep(1:18, each = 3),\n  condition = rep(c(\"Baseline\", \"Treatment A\", \"Treatment B\"), times = 18),\n  reaction_time = c(\n    round(rnorm(18, 450, 60)),    # Baseline\n    round(rnorm(18, 420, 55)),    # Treatment A: slight improvement\n    round(rnorm(18, 380, 50))     # Treatment B: larger improvement\n  )\n) %&gt;%\n  mutate(condition = factor(condition, \n                           levels = c(\"Baseline\", \"Treatment A\", \"Treatment B\")))\n\n# Descriptive statistics\nsummary_stats &lt;- pilot_data %&gt;%\n  group_by(condition) %&gt;%\n  summarize(\n    n = n(),\n    M = mean(reaction_time),\n    SD = sd(reaction_time),\n    SE = SD / sqrt(n),\n    CI_lower = M - qt(0.975, n - 1) * SE,\n    CI_upper = M + qt(0.975, n - 1) * SE,\n    .groups = \"drop\"\n  )\n\nprint(summary_stats)\n\n# A tibble: 3 × 7\n  condition       n     M    SD    SE CI_lower CI_upper\n  &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Baseline       18  425.  54.8  12.9     398.     453.\n2 Treatment A    18  432.  73.3  17.3     395.     468.\n3 Treatment B    18  416.  57.0  13.4     388.     445.\n\n\nCheckpoint: You should have n=18 observations per condition with decreasing reaction times across conditions.\n\n\n\nStep 2: Bad Practices vs Good Practices\nContrast ineffective and effective visualizations:\n\n# BAD: Bar chart without individual points (hides data)\nbad_plot &lt;- ggplot(summary_stats, aes(x = condition, y = M)) +\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), width = 0.3) +\n  labs(title = \"BAD: Bars Hide Data (n=18 per condition)\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\n# GOOD: Show all individual points with summary\ngood_plot &lt;- ggplot(pilot_data, aes(x = condition, y = reaction_time)) +\n  geom_jitter(width = 0.2, alpha = 0.4, size = 2) +  # Individual points\n  stat_summary(fun = mean, geom = \"point\", \n               size = 4, color = \"red\", shape = 18) +  # Mean\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.2, linewidth = 1, color = \"red\") +  # 95% CI\n  labs(title = \"GOOD: Show Individual Data Points (n=18 per condition)\",\n       y = \"Reaction Time (ms)\",\n       caption = \"Red diamonds = means with 95% CIs\") +\n  theme_minimal()\n\n# Display side-by-side\nbad_plot + good_plot\n\n\n\n\n\n\n\n\nCheckpoint: Good plot reveals individual variability and distribution; bad plot hides critical information.\nDiscussion: Why is hiding data particularly problematic with small samples?\n\n\n\nStep 3: Effective Within-Subjects Visualization\nShow paired nature of the data:\n\n# Connect individual trajectories\ntrajectory_plot &lt;- ggplot(pilot_data, \n                          aes(x = condition, y = reaction_time, \n                              group = participant_id)) +\n  geom_line(alpha = 0.3, color = \"gray50\") +  # Individual lines\n  geom_point(alpha = 0.5, size = 2) +         # Individual points\n  stat_summary(aes(group = 1), fun = mean, geom = \"line\", \n               color = \"red\", linewidth = 1.5) +  # Mean trajectory\n  stat_summary(fun = mean, geom = \"point\", \n               color = \"red\", size = 4, shape = 18) +\n  labs(title = \"Individual Trajectories with Mean (n=18)\",\n       subtitle = \"Within-Subjects Design\",\n       y = \"Reaction Time (ms)\",\n       x = \"Condition\") +\n  theme_minimal()\n\nprint(trajectory_plot)\n\n\n\n\n\n\n\n# Alternative: Raincloud plot (distribution + points + boxplot)\nraincloud_plot &lt;- ggplot(pilot_data, aes(x = condition, y = reaction_time, \n                                          fill = condition)) +\n  # Half violin (distribution)\n  stat_halfeye(adjust = 1, width = 0.6, .width = 0, \n               justification = -0.2, point_colour = NA) +\n  # Boxplot\n  geom_boxplot(width = 0.15, outlier.shape = NA, alpha = 0.5) +\n  # Individual points\n  geom_point(size = 2, alpha = 0.5, \n             position = position_jitter(width = 0.05)) +\n  labs(title = \"Raincloud Plot: Distribution + Box + Points\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nprint(raincloud_plot)\n\n\n\n\n\n\n\n\nCheckpoint: Individual trajectories reveal within-person changes (most participants improve). Raincloud shows full distribution.\n\n\n\nStep 4: Uncertainty Visualization\nEmphasize confidence intervals and prediction intervals:\n\n# Convert to wide format for plotting\npilot_wide &lt;- pilot_data %&gt;%\n  pivot_wider(names_from = condition, values_from = reaction_time)\n\n# Scatter plot with regression line and confidence/prediction bands\nscatter_uncertainty &lt;- ggplot(pilot_wide, \n                              aes(x = Baseline, y = `Treatment B`)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", \n              color = \"gray50\", linewidth = 1) +  # Identity line\n  geom_smooth(method = \"lm\", se = TRUE, \n              level = 0.95, fill = \"blue\", alpha = 0.2) +  # 95% CI\n  geom_point(size = 3, alpha = 0.6) +\n  labs(title = \"Baseline vs Treatment B (n=18)\",\n       subtitle = \"Blue band = 95% confidence interval for regression line\",\n       x = \"Baseline Reaction Time (ms)\",\n       y = \"Treatment B Reaction Time (ms)\") +\n  coord_fixed(ratio = 1) +\n  theme_minimal()\n\nprint(scatter_uncertainty)\n\n\n\n\n\n\n\n# Error bar plot with different CI levels\nci_levels &lt;- expand_grid(\n  condition = summary_stats$condition,\n  ci_level = c(0.80, 0.90, 0.95)\n) %&gt;%\n  left_join(pilot_data, by = \"condition\") %&gt;%\n  group_by(condition, ci_level) %&gt;%\n  summarize(\n    M = mean(reaction_time),\n    SE = sd(reaction_time) / sqrt(n()),\n    CI_lower = M - qt((1 + ci_level) / 2, n() - 1) * SE,\n    CI_upper = M + qt((1 + ci_level) / 2, n() - 1) * SE,\n    .groups = \"drop\"\n  )\n\nci_comparison &lt;- ggplot(ci_levels, aes(x = condition, y = M, \n                                       color = factor(ci_level))) +\n  geom_point(size = 3, position = position_dodge(0.3)) +\n  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), \n                width = 0.2, linewidth = 1,\n                position = position_dodge(0.3)) +\n  scale_color_manual(values = c(\"green\", \"blue\", \"red\"),\n                    name = \"Confidence Level\") +\n  labs(title = \"Comparing Confidence Interval Widths\",\n       subtitle = \"Wider intervals = higher confidence\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\nprint(ci_comparison)\n\n\n\n\n\n\n\n\nCheckpoint: Visualizations honestly communicate uncertainty. Wider CIs reflect small sample imprecision.\n\n\n\nStep 5: Publication-Ready Formatting\nApply professional styling:\n\n# Publication-quality figure\npub_plot &lt;- ggplot(pilot_data, aes(x = condition, y = reaction_time)) +\n  # Individual points with beeswarm layout (no overlap)\n  geom_beeswarm(alpha = 0.5, size = 2.5, cex = 2) +\n  # Mean with 95% CI\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.2, linewidth = 1.2, color = \"red\") +\n  stat_summary(fun = mean, geom = \"point\", \n               size = 5, color = \"red\", shape = 18) +\n  # Styling\n  scale_y_continuous(breaks = seq(300, 600, 50)) +\n  labs(\n    title = \"Reaction Time by Condition\",\n    subtitle = \"Within-subjects pilot study (N=18)\",\n    x = \"Experimental Condition\",\n    y = \"Reaction Time (ms)\",\n    caption = \"Individual points shown with means (red diamonds) and 95% CIs.\\nError bars represent between-subjects variability.\"\n  ) +\n  theme_classic() +\n  theme(\n    text = element_text(size = 12, family = \"sans\"),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray30\"),\n    plot.caption = element_text(size = 9, hjust = 0, color = \"gray50\"),\n    axis.text = element_text(size = 11),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    panel.grid.major.y = element_line(color = \"gray90\"),\n    panel.grid.minor = element_blank()\n  )\n\nprint(pub_plot)\n\n\n\n\n\n\n\n# Save high-resolution version\nggsave(\"pilot_reaction_time.png\", pub_plot, \n       width = 7, height = 5, dpi = 300)\ncat(\"Figure saved as 'pilot_reaction_time.png'\\n\")\n\nFigure saved as 'pilot_reaction_time.png'\n\n\nCheckpoint: Publication-ready figure with clear labels, appropriate font sizes, and professional appearance.\n\n\n\nStep 6: Small Sample Specific Techniques\nApply strategies tailored to small n:\n\n# 1. Annotate with exact n\nannotated_plot &lt;- ggplot(summary_stats, aes(x = condition, y = M)) +\n  geom_col(fill = \"lightblue\", alpha = 0.6) +\n  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), width = 0.3) +\n  geom_text(aes(label = paste0(\"n=\", n)), \n            vjust = -0.5, size = 4, fontface = \"bold\") +\n  geom_jitter(data = pilot_data, \n              aes(x = condition, y = reaction_time),\n              width = 0.2, alpha = 0.5, size = 2) +\n  labs(title = \"Explicitly State Sample Size\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\n# 2. Show effect sizes, not just means\neffect_data &lt;- tibble(\n  comparison = c(\"Baseline vs Treatment A\", \n                 \"Baseline vs Treatment B\",\n                 \"Treatment A vs Treatment B\"),\n  cohens_d = c(-0.52, -1.32, -0.75),  # Example values\n  ci_lower = c(-1.15, -2.03, -1.40),\n  ci_upper = c(0.12, -0.60, -0.10)\n)\n\neffect_plot &lt;- ggplot(effect_data, aes(x = comparison, y = cohens_d)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_hline(yintercept = c(-0.2, -0.5, -0.8), \n             linetype = \"dotted\", color = \"blue\", alpha = 0.3) +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.2, linewidth = 1) +\n  coord_flip() +\n  labs(title = \"Effect Sizes with 95% CIs\",\n       x = \"\", y = \"Cohen's d (negative = faster reaction time)\",\n       caption = \"Blue lines indicate small (-0.2), medium (-0.5), large (-0.8) benchmarks\") +\n  theme_minimal()\n\n# 3. Emphasize individual differences\nindividual_plot &lt;- pilot_wide %&gt;%\n  mutate(improved = `Treatment B` &lt; Baseline) %&gt;%\n  ggplot(aes(x = Baseline, y = `Treatment B`, color = improved)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_point(size = 4, alpha = 0.7) +\n  geom_text(aes(label = participant_id), vjust = -0.8, size = 3) +\n  scale_color_manual(values = c(\"red\", \"darkgreen\"),\n                    labels = c(\"Worsened\", \"Improved\"),\n                    name = \"Change\") +\n  labs(title = \"Individual Response to Treatment B (n=18)\",\n       subtitle = \"Points below diagonal = improvement\",\n       x = \"Baseline RT (ms)\", y = \"Treatment B RT (ms)\") +\n  coord_fixed(ratio = 1) +\n  theme_minimal()\n\n# Display all\nannotated_plot / (effect_plot | individual_plot)\n\n\n\n\n\n\n\n\nCheckpoint: Multiple strategies communicate both group trends and individual variability.\n\n\n\nStep 7: Avoid Common Pitfalls\nDemonstrate what NOT to do:\n\n# PITFALL 1: Dynamite plots (bars + SE, no data shown)\npitfall1 &lt;- ggplot(summary_stats, aes(x = condition, y = M)) +\n  geom_col(fill = \"gray50\") +\n  geom_errorbar(aes(ymin = M - SE, ymax = M + SE), width = 0.3) +\n  labs(title = \"AVOID: Dynamite Plot (Bars + SE)\",\n       subtitle = \"Hides distribution, SE is not CI\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\n# PITFALL 2: Over-smoothing with small n\npitfall2 &lt;- ggplot(pilot_data, aes(x = as.numeric(condition), \n                                   y = reaction_time)) +\n  geom_smooth(method = \"loess\", span = 0.5, se = TRUE) +\n  labs(title = \"AVOID: Over-smoothed Curve (Implies More Data)\",\n       subtitle = \"Only 3 conditions but curve suggests continuous relationship\",\n       x = \"Condition\", y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\n# PITFALL 3: Missing sample size annotation\npitfall3 &lt;- ggplot(summary_stats, aes(x = condition, y = M)) +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), width = 0.2) +\n  labs(title = \"AVOID: No Sample Size Stated\",\n       subtitle = \"Reader cannot judge CI width appropriateness\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\n# BETTER alternatives\nbetter1 &lt;- ggplot(pilot_data, aes(x = condition, y = reaction_time)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.5) +\n  geom_jitter(width = 0.2, alpha = 0.6, size = 2) +\n  labs(title = \"BETTER: Boxplot + Points\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\nbetter2 &lt;- ggplot(summary_stats, aes(x = condition, y = M)) +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), width = 0.2) +\n  geom_jitter(data = pilot_data, \n              aes(x = condition, y = reaction_time),\n              width = 0.2, alpha = 0.4, size = 2) +\n  labs(title = \"BETTER: Points + 95% CI + Data\",\n       subtitle = \"n=18 per condition\",\n       y = \"Reaction Time (ms)\") +\n  theme_minimal()\n\n(pitfall1 | better1) / (pitfall2 | better2)\n\n\n\n\n\n\n\n\nCheckpoint: Contrast highlights importance of showing data and being transparent about sample size.\nDiscussion: Why are dynamite plots especially misleading with small samples?\n\n\n\nStep 8: Create Multi-Panel Publication Figure\nCombine multiple visualizations:\n\n# Panel A: Main result with individual trajectories\npanel_a &lt;- ggplot(pilot_data, \n                  aes(x = condition, y = reaction_time, \n                      group = participant_id)) +\n  geom_line(alpha = 0.2, color = \"gray70\") +\n  geom_point(alpha = 0.4, size = 2) +\n  stat_summary(aes(group = 1), fun = mean, geom = \"line\", \n               color = \"red\", linewidth = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", \n               color = \"red\", size = 4, shape = 18) +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.2, linewidth = 1, color = \"red\") +\n  labs(subtitle = \"A. Individual Trajectories\",\n       y = \"Reaction Time (ms)\", x = \"\") +\n  theme_classic()\n\n# Panel B: Distribution comparison\npanel_b &lt;- ggplot(pilot_data, aes(x = reaction_time, fill = condition)) +\n  geom_density(alpha = 0.5) +\n  labs(subtitle = \"B. Density Distributions\",\n       x = \"Reaction Time (ms)\", y = \"Density\") +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n# Panel C: Effect sizes\ntreat_b_vs_baseline &lt;- pilot_wide %&gt;%\n  summarize(\n    d = (mean(Baseline) - mean(`Treatment B`)) / \n        sqrt((sd(Baseline)^2 + sd(`Treatment B`)^2) / 2)\n  ) %&gt;%\n  pull(d)\n\neffect_summary &lt;- tibble(\n  Comparison = c(\"Treatment A vs Baseline\", \"Treatment B vs Baseline\"),\n  d = c(-0.5, treat_b_vs_baseline),  # Negative = improvement\n  ci_lower = c(-1.1, treat_b_vs_baseline - 0.6),\n  ci_upper = c(0.1, treat_b_vs_baseline + 0.6)\n)\n\npanel_c &lt;- ggplot(effect_summary, aes(x = Comparison, y = d)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.2, linewidth = 1) +\n  coord_flip() +\n  labs(subtitle = \"C. Standardized Effect Sizes\",\n       x = \"\", y = \"Cohen's d (95% CI)\") +\n  theme_classic()\n\n# Combine panels\nfinal_figure &lt;- (panel_a | panel_b) / panel_c +\n  plot_annotation(\n    title = \"Reaction Time Across Experimental Conditions\",\n    subtitle = \"Within-subjects pilot study (N=18)\",\n    caption = \"Red line/points = group means with 95% CIs. Effect sizes calculated as within-subjects Cohen's d.\",\n    theme = theme(\n      plot.title = element_text(size = 14, face = \"bold\"),\n      plot.subtitle = element_text(size = 11)\n    )\n  )\n\nprint(final_figure)\n\n\n\n\n\n\n\n# Save publication figure\nggsave(\"figure_multipanel.png\", final_figure, \n       width = 10, height = 8, dpi = 300)\ncat(\"Multi-panel figure saved as 'figure_multipanel.png'\\n\")\n\nMulti-panel figure saved as 'figure_multipanel.png'\n\n\nCheckpoint: Comprehensive multi-panel figure tells complete story: trajectories, distributions, and effect sizes.\n\n\n\nDiscussion Questions\n\nBars vs Points: Why are bar charts problematic for small-sample continuous data?\nAnswer: Bars (1) hide individual data points, (2) imply data start at zero (rarely meaningful for reaction time, test scores, etc.), (3) obscure distributional shape, (4) waste ink on empty space. Points + error bars are more informative.\nHow Many Points: With n=5, should you show all individual points?\nAnswer: Absolutely! With very small n, every observation is influential and readers should see them all. Use jittering to avoid overlap, and consider labeling points or using different symbols.\nError Bar Choice: SE, 95% CI, or SD? Which should you display?\nAnswer: Prefer 95% CI (matches hypothesis testing, directly interpretable). SD shows data spread (useful for understanding variability). Avoid SE (easily confused with CI, magnitude depends on n).\nColor Considerations: What color choices improve accessibility?\nAnswer: Use colorblind-safe palettes (e.g., viridis), ensure sufficient contrast, don’t rely solely on color (use shapes/patterns too), test plots in grayscale, consider cultural associations.\nTransparency: How do you communicate small sample limitations visually?\nAnswer: (1) State n prominently, (2) show wide CIs, (3) display all individual points, (4) avoid over-confident smoothing, (5) use “pilot” or “preliminary” in titles, (6) show effect size uncertainty.\n\n\n\n\nKey Takeaways\n✓ Always show individual data points in small samples (n&lt;30)\n✓ Display uncertainty with confidence intervals, not just SEs\n✓ Avoid bar charts for continuous data – prefer points or boxes\n✓ Within-subjects designs: show individual trajectories\n✓ State sample size prominently in title, subtitle, or annotations\n✓ Use professional styling – clear labels, readable fonts, appropriate colors\n✓ Multi-panel figures tell richer stories than single plots\n\n\n\nHomework Exercise\nDataset: A pilot cognitive training study (n=15) with pre- and post-test scores.\n\nset.seed(987)\ntraining_study &lt;- tibble(\n  participant_id = 1:15,\n  age = round(rnorm(15, 28, 5)),\n  pretest = round(rnorm(15, 60, 10)),\n  posttest = round(rnorm(15, 72, 12))\n) %&gt;%\n  mutate(\n    gain = posttest - pretest,\n    percent_gain = (gain / pretest) * 100\n  )\n\nYour Tasks:\n\nCreate a “bad” visualization (bar chart hiding data) and a “good” visualization (showing individual points)\nMake a paired scatter plot (pretest vs posttest) with identity line\nCreate a trajectory plot showing individual pre-to-post changes\nGenerate a raincloud plot for the gain scores\nMake a plot emphasizing uncertainty (show 80%, 90%, 95% CIs)\nCalculate Cohen’s d for pre-post difference and create a forest plot\nCreate a multi-panel figure combining: (A) trajectories, (B) gain distribution, (C) effect size\nApply publication-ready formatting (clear labels, professional theme, sample size annotation)\nSave the final figure as high-resolution PNG (300 dpi)\nWrite a figure caption describing what is shown\n\nExpected Results:\n\nSignificant pre-post improvement (gain ~10-15 points)\nWide CIs reflecting small sample (n=15)\nMost individuals improve (but show variability)\nCohen’s d approximately 0.8-1.2 (large effect)\nPublication figure should be clear, honest, and professional\n\nHints: - Use geom_segment() or geom_line() for trajectory plots - For paired scatter: geom_abline(slope=1, intercept=0) for identity line - Use patchwork to combine plots: plot_a | plot_b or plot_a / plot_b - Add n to subtitle: subtitle = \"Pilot cognitive training study (N=15)\"\nBonus: Create an animated version showing the transition from pretest to posttest using gganimate. This can effectively communicate within-subjects change even in presentations. Export as GIF or MP4.\n\nCongratulations! You’ve completed all 12 guided lab practicals covering essential statistical techniques for small-sample research. These hands-on exercises complement the theoretical content throughout the book and prepare you for real-world data analysis challenges.",
    "crumbs": [
      "Part G: Guided Lab Practicals",
      "Part G: Guided Lab Practicals"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html",
    "href": "chapters/part-h-instructors-manual.html",
    "title": "7  Part H: Instructor’s Manual",
    "section": "",
    "text": "7.1 Quantitative Analysis with Small Samples\nFor Academic Use Only\nConfidential—Not for Student Distribution",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#table-of-contents",
    "href": "chapters/part-h-instructors-manual.html#table-of-contents",
    "title": "7  Part H: Instructor’s Manual",
    "section": "7.2 Table of Contents",
    "text": "7.2 Table of Contents\n\nCourse Planning\nSample Syllabi\nAnswer Keys\nGrading Rubrics\nCommon Teaching Challenges\nAdditional Resources",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#course-planning",
    "href": "chapters/part-h-instructors-manual.html#course-planning",
    "title": "7  Part H: Instructor’s Manual",
    "section": "7.3 1. COURSE PLANNING",
    "text": "7.3 1. COURSE PLANNING\n\n7.3.1 1.1 Target Audiences\nPrimary: - Graduate students (Master’s/PhD) in social sciences, health sciences, business, education - Social Sciences: Psychology, sociology, anthropology, political science - Health Sciences: Nursing, public health, clinical research, health psychology - Business: Marketing analytics, organizational behavior, operations management - Education: Curriculum & instruction, educational psychology, special education, teacher education - Researchers in SIDS or resource-constrained settings - Quantitative methods courses emphasizing practical skills - Education practitioners conducting classroom-based research (action research, program evaluation)\nSecondary: - Advanced undergraduates with prior statistics coursework - Continuing education for practitioners (workshops, short courses) - Self-study for researchers transitioning from large-sample methods - School administrators and instructional coaches evaluating interventions\n\n\n7.3.2 1.2 Prerequisites\nRequired: - Introductory statistics (hypothesis testing, confidence intervals, regression) - Basic R programming (loading data, running functions, interpreting output) - Familiarity with RStudio interface\nRecommended: - Experience with linear and logistic regression - Prior exposure to ANOVA/t-tests - Understanding of p-values and effect sizes\nIf Prerequisites Are Missing: - Week 0: R Bootcamp (4 hours)—install R/RStudio, basic syntax, importing data - Pre-course materials: Recommend DataCamp’s “Introduction to R” or R for Data Science (Wickham & Grolemund)\n\n\n7.3.3 1.3 Learning Outcomes (Course-Level)\nBy the end of this course, students will be able to:\nConceptual: - Explain why large-sample methods fail with small n - Distinguish between exact, resampling, and nonparametric approaches - Identify when small samples are sufficient vs. insufficient\nPractical: - Conduct Fisher’s exact test, Mann-Whitney U, Wilcoxon signed-rank - Fit Firth logistic regression and Bayesian models - Compute Cronbach’s alpha and McDonald’s omega - Generate bootstrap confidence intervals\nProfessional: - Report results transparently following best practices - Interpret non-significant results appropriately - Pre-register analyses and document deviations - Critique published small-sample studies",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#sample-syllabi",
    "href": "chapters/part-h-instructors-manual.html#sample-syllabi",
    "title": "7  Part H: Instructor’s Manual",
    "section": "7.4 2. SAMPLE SYLLABI",
    "text": "7.4 2. SAMPLE SYLLABI\n\n7.4.1 2.1 Fifteen-Week Semester Course (Graduate Level)\nCourse: Advanced Quantitative Methods for Small Samples\nCredits: 3\nMeeting Time: 3 hours/week (2 hours lecture + 1 hour lab)\nLevel: Graduate (Master’s/PhD)\n\n7.4.1.1 Week-by-Week Schedule\n\n\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings\nLab/Activities\nAssessments\nNotes\n\n\n\n\n1\nCourse Introduction- Syllabus overview- Why small samples matter- Small n ≠ bad research\nCh 1\nInstall R/RStudio/packagesExplore example datasetsIntroductions: Share research contexts\n—\nSet up computational environment\n\n\n2\nResearch Design Foundations- Framing research questions- Outcome selection- Exploratory vs. confirmatory\nCh 2\nGenerate synthetic datasetsCritique study designsDiscussion: When are small samples justified?\nQuiz 1 (Ch 1-2)\nEmphasize question formulation\n\n\n3\nSampling & Power- Sampling strategies- Power analysis- Minimum detectable effects\nCh 9\nLab 1: Power AnalysisCalculate sample size requirementsSensitivity analyses\n—\nCritical for study planning\n\n\n4\nMeasurement Quality I- Scale development- Validity assessment- Pilot testing\nCh 10 (first half)\nCognitive interview exerciseItem-level diagnosticsContent validity evaluation\nAssignment 1 Due:Study Design Proposal\nInclude measurement plan\n\n\n5\nMeasurement Quality II- Reliability theory- Cronbach’s α, McDonald’s ω- Short scale challenges\nCh 10 (second half)\nLab 6: Reliability AnalysisCompute internal consistencyInterpret with small n\n—\nStress limitations with small n\n\n\n6\nData Screening- Outlier detection- Normality assessment- Diagnostic checks\nCh 11\nLab 8: Data ScreeningUnivariate & multivariate outliersQ-Q plots, Shapiro-Wilk tests\nQuiz 2 (Ch 9-11)\nDocument all decisions\n\n\n7\nMissing Data- MCAR, MAR, MNAR- Multiple imputation- MICE implementation\nCh 12\nLab 9: Multiple ImputationConvergence diagnosticsPooling results (Rubin’s rules)\n—\nEmphasize assumptions\n\n\n8\nMIDTERM BREAK / REVIEW- Synthesis of Weeks 1-7- Q&A session- Worked Project 1\nPart E (Project 1)\nWork through employee engagement projectOpen lab time for Assignment 2\n—\nConsolidation week\n\n\n9\nExact Tests & Resampling- Fisher’s exact test- Permutation tests- Bootstrap CIs\nCh 3\nLab 2: Exact TestsCompare Fisher vs. chi-squareGenerate permutation distributions\nAssignment 2 Due:Data Analysis Report\nHands-on resampling\n\n\n10\nNonparametric Methods- Mann-Whitney U- Wilcoxon signed-rank- Rank-based effect sizes\nCh 4\nLab 3: Nonparametric TestsCompute rank-biserial correlationWhen to use vs. t-tests\n—\nEmphasize robustness\n\n\n11\nRegression for Small Samples- Separation in logistic regression- Firth’s penalized method- Bayesian regression intro\nCh 5\nLab 4: Penalized RegressionLab 5: Bayesian Estimation (brms basics)Prior sensitivity analysis\nQuiz 3 (Ch 3-5)\nMay skip Bayesian if time-limited\n\n\n12\nSpecialized Methods- Reliability for short scales- MCDM techniques (optional)- Sparse counts\nCh 6, 8 (Ch 7 optional)\nLab 7: Bootstrap MethodsApplied reliability analysisZero-inflated Poisson models\n—\nAdapt to student interests\n\n\n13\nReporting & Interpretation I- Effect sizes over p-values- Confidence intervals- Transparent reporting\nCh 13-14\nLab 11: Effect Sizes & CIsCritique published papersAPA-style reporting workshop\n—\nCritical reporting skills\n\n\n14\nReporting & Interpretation II- Non-significant results- Equivalence testing- Visualization best practices\nCh 15-16\nLab 12: VisualizationCreate raincloud plotsPublication-ready figures\nAssignment 3 Draft Due:For peer review\nEmphasize uncertainty visualization\n\n\n15\nSynthesis & Presentations- Worked Projects 2-3- Student presentations- Course wrap-up\nPart E (Projects 2-3)\nStudent presentations (15 min each)Peer feedbackReflection on learning\nFinal PresentationAssignment 3 Final Due\nCelebrate learning!\n\n\n\n\n\n7.4.1.2 Grading Breakdown\n\n\n\n\n\n\n\n\nComponent\nWeight\nDetails\n\n\n\n\nQuizzes (3)\n15%\n5% each, open-book, conceptual focus (Ch 1-2, 9-11, 3-5)\n\n\nLab Participation\n15%\nAttendance + completion of 12 labs (1.25% each)\n\n\nAssignment 1: Study Design Proposal\n15%\nDesign small-sample study with power analysis, measurement plan (due Week 4)\n\n\nAssignment 2: Data Analysis Report\n25%\nAnalyze provided dataset, full write-up with screening, analysis, interpretation (due Week 9)\n\n\nAssignment 3: Final Report\n20%\nIndependent analysis + report (APA format); draft due Week 14, final due Week 15\n\n\nFinal Presentation\n10%\n15-min presentation of Assignment 3 (Week 15)\n\n\n\nTotal: 100%\n\n\n\n\n7.4.2 2.2 Fifteen-Week Semester Course (Undergraduate Adaptation)\nCourse: Quantitative Methods for Small Samples\nCredits: 3\nMeeting Time: 3 hours/week (2 hours lecture + 1 hour lab)\nLevel: Advanced Undergraduate (junior/senior with stats prerequisite)\n\n7.4.2.1 Key Adaptations for Undergraduates\n1. Simplified Content: - Omit: Bayesian methods (Ch 5, Lab 5), MCDM (Ch 7), advanced topics in Ch 6, 8 - Emphasize: Exact tests, nonparametric methods, effect sizes, visualization - Focus: Practical application over theoretical depth\n2. Reduced Technical Complexity: - Use pre-written R scripts (students modify, not write from scratch) - Provide more scaffolding in labs (fill-in-the-blank code) - Focus on interpretation over programming\n3. Modified Assessments: - Replace “Study Design Proposal” with “Mini Literature Review” (critique 3 small-sample studies) - Simplify Assignment 2 (guided analysis with step-by-step instructions) - Allow group projects for Assignment 3 (pairs)\n4. Additional Support: - Weekly R help sessions (30 min before/after class) - Video tutorials for all labs - More worked examples in lectures\n\n\n7.4.2.2 Undergraduate Week-by-Week Schedule\n\n\n\n\n\n\n\n\n\n\n\nWeek\nTopics\nReadings\nLab/Activities\nAssessments\nUG Modifications\n\n\n\n\n1\nIntroduction- What are small samples?- When & why they occur\nCh 1\nR/RStudio setupExplore datasets\n—\nExtra R tutorial time\n\n\n2\nResearch Questions- Formulating testable questions- Choosing outcomes\nCh 2\nCritique example studiesDiscussion activity\nQuiz 1 (Ch 1-2)\nUse real-world examples\n\n\n3\nPower & Sample Size- What is statistical power?- Calculating minimum n\nCh 9 (simplified)\nLab 1: Power Analysis (guided)Use online calculators\n—\nFocus on interpretation, not derivation\n\n\n4\nMeasurement Basics- Reliability & validity- Scale quality\nCh 10 (core concepts only)\nItem analysis exerciseCompute Cronbach’s α\nAssignment 1 Due:Mini Literature Review\nSkip advanced psychometrics\n\n\n5\nData Screening- Checking for errors- Outlier detection- Visualizing data\nCh 11\nLab 8: Data Screening (guided)Create boxplots, histograms\n—\nEmphasize visual methods\n\n\n6\nMissing Data Basics- Why data go missing- Simple imputation- Complete-case analysis\nCh 12 (intro only)\nLab 9: Multiple Imputation (simplified)Use mice package with defaults\nQuiz 2 (Ch 9-12)\nSkip MCAR/MAR/MNAR theory\n\n\n7\nReview & Midterm- Consolidation- Practice problems\n—\nWork through Project 1 togetherOpen lab time\n—\nMore guided support\n\n\n8\nExact Tests- Fisher’s exact test- When to use vs. chi-square\nCh 3 (Fisher’s exact only)\nLab 2: Exact Tests (Fisher focus)Compare with chi-square\n—\nSkip permutation tests\n\n\n9\nNonparametric Tests I- Mann-Whitney U- When data aren’t normal\nCh 4 (Mann-Whitney)\nLab 3: Nonparametric Tests (Part 1)Compare with t-test\nAssignment 2 Due:Guided Data Analysis\nFocus on one test at a time\n\n\n10\nNonparametric Tests II- Wilcoxon signed-rank- Paired comparisons\nCh 4 (Wilcoxon)\nLab 3: Nonparametric Tests (Part 2)Paired data examples\n—\nUse familiar contexts (pre/post)\n\n\n11\nEffect Sizes- Cohen’s d- Why p-values aren’t enough\nCh 13\nLab 11: Effect Sizes & CIs (Part 1)Calculate by hand, then in R\nQuiz 3 (Ch 3-4, 13)\nEmphasize practical significance\n\n\n12\nConfidence Intervals- What CIs tell us- Interpreting uncertainty\nCh 13\nLab 11: Effect Sizes & CIs (Part 2)Bootstrap CIs (simplified)\n—\nFocus on interpretation\n\n\n13\nInterpreting Results- Non-significant results- Avoiding over-interpretation\nCh 15\nCase studies discussionRewrite bad conclusions\n—\nUse published examples\n\n\n14\nVisualization- Creating clear plots- Showing uncertainty- Common mistakes\nCh 16\nLab 12: Visualizationggplot2 basicsError bars, dot plots\nAssignment 3 Draft Due\nProvide ggplot2 templates\n\n\n15\nPresentations & Wrap-Up- Student presentations- Course reflection\nPart E (Project 1)\nGroup presentations (10 min)Peer feedback\nFinal PresentationAssignment 3 Final Due\nGroup projects allowed\n\n\n\n\n\n7.4.2.3 Undergraduate Grading Breakdown\n\n\n\n\n\n\n\n\nComponent\nWeight\nDetails\n\n\n\n\nQuizzes (3)\n15%\n5% each, shorter (10 questions), focus on key concepts\n\n\nLab Participation\n20%\nAttendance + completion (more weight due to guided nature)\n\n\nAssignment 1: Literature Review\n15%\nCritique 3 small-sample studies (scaffolded template provided)\n\n\nAssignment 2: Guided Analysis\n20%\nStep-by-step analysis with provided data and instructions\n\n\nAssignment 3: Group Project\n20%\nPairs allowed; analyze dataset + 5-page report\n\n\nFinal Presentation\n10%\n10-min group presentation\n\n\n\nTotal: 100%\n\n\n7.4.2.4 Recommended Chapters for Undergraduate Course\nCore (Must Cover): - ✅ Chapter 1: When and Why Small Samples - ✅ Chapter 2: Framing Research Questions - ✅ Chapter 9: Sampling Strategies (simplified) - ✅ Chapter 11: Data Screening - ✅ Chapter 3: Exact Tests (Fisher’s exact only) - ✅ Chapter 4: Nonparametric Methods (Mann-Whitney, Wilcoxon) - ✅ Chapter 13: Effect Sizes and Confidence Intervals - ✅ Chapter 15: Interpreting Non-Significant Results - ✅ Chapter 16: Visualization\nOptional (If Time Permits): - ⭕ Chapter 10: Measurement Quality (basics only) - ⭕ Chapter 12: Missing Data (complete-case analysis + simple imputation) - ⭕ Part E: Worked Project 1 (walk through together)\nOmit for Undergraduates: - ❌ Chapter 5: Penalized and Bayesian Regression (too advanced) - ❌ Chapter 6: Reliability (advanced psychometrics) - ❌ Chapter 7: MCDM (specialized application) - ❌ Chapter 8: Sparse Counts and Time Series (advanced) - ❌ Chapter 14: Transparent Reporting (briefly mention, don’t deep-dive) - ❌ Part F: Technical Appendices - ❌ Labs 4, 5, 7 (Bayesian, MCDM, advanced bootstrap)\n\n\n\n\n7.4.3 2.3 Graduate vs. Undergraduate Comparison Table\n\n\n\n\n\n\n\n\nAspect\nGraduate Course\nUndergraduate Course\n\n\n\n\nDepth\nTheoretical foundations + application\nApplication-focused, minimal theory\n\n\nR Programming\nWrite scripts from scratch\nModify provided templates\n\n\nChapters Covered\nAll 16 chapters\n9 core chapters\n\n\nLabs\nAll 12 labs\n6-7 labs (simplified versions)\n\n\nBayesian Methods\nFull coverage (Ch 5, Lab 5)\nOmitted\n\n\nAssignments\nIndependent design + analysis\nGuided, scaffolded tasks\n\n\nProjects\nIndividual work\nGroups allowed (pairs)\n\n\nPrerequisites\nIntermediate stats + R\nIntro stats (t-test, ANOVA, regression)\n\n\nReading Load\n~60 pages/week\n~30 pages/week\n\n\nAssessment Rigor\nAPA-style reports, preregistration\nSimplified reporting, templates provided\n\n\n\n\n\n\n7.4.4 2.4 Twelve-Week Semester Course (Condensed Graduate)\nFor quarters or accelerated programs\nStrategy: Combine weeks, prioritize core methods\n\n\n\n\n\n\n\n\n\n\nWeek\nCombined Topics\nReadings\nLabs\nAssessments\n\n\n\n\n1\nFoundations + Research Design\nCh 1-2\nSetup + Discussion\nQuiz 1\n\n\n2\nSampling + Power\nCh 9\nLab 1\nAssignment 1 Due\n\n\n3\nMeasurement Quality\nCh 10\nLab 6\n—\n\n\n4\nData Screening + Missing Data\nCh 11-12\nLabs 8-9\nQuiz 2\n\n\n5\nExact Tests + Nonparametric\nCh 3-4\nLabs 2-3\n—\n\n\n6\nRegression Methods\nCh 5\nLab 4 or 5 (choose one)\nAssignment 2 Due\n\n\n7\nSpecialized Methods\nCh 6, 8\nLab 7\nQuiz 3\n\n\n8\nReporting I\nCh 13-14\nLab 11\n—\n\n\n9\nReporting II\nCh 15-16\nLab 12\n—\n\n\n10\nWorked Projects\nPart E\nProjects 1-2\n—\n\n\n11\nSynthesis\nPart E\nProject 3\nAssignment 3 Due\n\n\n12\nPresentations\n—\nStudent presentations\nFinal Presentation\n\n\n\n\n\n\n7.4.5 2.5 Five-Day Intensive Workshop\nTarget: Practicing researchers, continuing education\nDuration: 5 days × 6 hours = 30 contact hours\n\n7.4.5.1 Daily Schedule\nDay 1: Foundations & Design - 09:00–10:30: Lectures (Ch 1-2) - 10:45–12:00: Power analysis lab - 13:00–14:30: Sampling strategies (Ch 9) - 14:45–16:00: Study design exercise (groups)\nDay 2: Exact & Nonparametric Methods - 09:00–10:30: Exact tests lecture (Ch 3) - 10:45–12:00: Lab 2: Fisher’s exact, permutation tests - 13:00–14:30: Nonparametric tests lecture (Ch 4) - 14:45–16:00: Lab 3: Mann-Whitney, Wilcoxon\nDay 3: Regression & Reliability - 09:00–10:30: Firth logistic regression (Ch 5) - 10:45–12:00: Lab 4: Penalized regression - 13:00–14:30: Reliability for short scales (Ch 6) - 14:45–16:00: Lab 6: Cronbach’s alpha, omega\nDay 4: Data Quality & Missing Data - 09:00–10:30: Data screening (Ch 11) - 10:45–12:00: Lab 8: Outlier detection - 13:00–14:30: Missing data (Ch 12) - 14:45–16:00: Lab 9: Multiple imputation with MICE\nDay 5: Reporting & Visualization - 09:00–10:30: Effect sizes, non-significant results (Ch 13, 15) - 10:45–12:00: Lab 11: Effect size calculation - 13:00–14:30: Visualization (Ch 16) - 14:45–16:00: Lab 12: Publication-ready figures, wrap-up\nAssessment: - Daily quizzes (10 questions each, 5% × 5 = 25%) - Lab completion certificates (50%) - Take-home capstone project (25%)\n\n\n\n\n7.4.6 2.6 Instructor’s Guide: Adapting for Undergraduates\n\n7.4.6.1 Overview of Adaptations\nWhen teaching this material to undergraduates (typically juniors/seniors with introductory statistics background), several strategic adaptations ensure accessibility while maintaining rigor:\nPhilosophy: Focus on doing over deriving. Undergraduates should master practical application and interpretation, not mathematical proofs.\n\n\n7.4.6.2 1. Content Prioritization\nMUST COVER (Core UG Content):\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nWhy Essential\nAdaptation\n\n\n\n\nCh 1\nWhen & Why Small Samples\nMotivation, real-world relevance\nUse more familiar examples (sports, psychology studies)\n\n\nCh 2\nResearch Questions\nFoundation for study design\nProvide question-writing templates\n\n\nCh 9\nSampling & Power\nCritical for understanding limitations\nUse online calculators, skip derivations\n\n\nCh 11\nData Screening\nPractical necessity\nEmphasize visual methods (boxplots, histograms)\n\n\nCh 3\nFisher’s Exact Test\nKey alternative to chi-square\nFocus only on 2×2 tables; skip larger tables\n\n\nCh 4\nMann-Whitney & Wilcoxon\nWidely applicable nonparametric tests\nOne test per week (Week 9, Week 10)\n\n\nCh 13\nEffect Sizes & CIs\nCounteract p-value obsession\nCalculate by hand first, then use R\n\n\nCh 15\nNon-Significant Results\nCommon misinterpretation\nUse real published examples to critique\n\n\nCh 16\nVisualization\nCommunication skills\nProvide ggplot2 templates to modify\n\n\n\nMAY COVER (If Time/Background Permits):\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nCondition\nAdaptation\n\n\n\n\nCh 10\nMeasurement Quality\nIf psychology/health students\nBasics only: Cronbach’s α, skip omega\n\n\nCh 12\nMissing Data\nIf students have research experience\nComplete-case analysis + mice package (default settings)\n\n\nPart E\nWorked Projects\nGood for consolidation\nWalk through Project 1 together in class\n\n\n\nOMIT (Too Advanced):\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nWhy Omit\nGraduate Alternative\n\n\n\n\nCh 5\nBayesian Regression\nRequires prior distributions, MCMC understanding\nGraduate: Full brms treatment\n\n\nCh 6\nAdvanced Reliability\nFactor analysis, IRT beyond UG scope\nGraduate: Polychoric correlations, omega hierarchical\n\n\nCh 7\nMCDM\nSpecialized application\nGraduate: AHP, TOPSIS, VIKOR\n\n\nCh 8\nTime Series & Counts\nRequires time-series background\nGraduate: State-space models, zero-inflation\n\n\nCh 14\nTransparent Reporting\nBetter as guest lecture\nGraduate: Preregistration, deviation documentation\n\n\n\n\n\n7.4.6.3 2. Pedagogical Modifications\nA. R Programming Support\n\n\n\n\n\n\n\n\nChallenge\nGraduate Approach\nUndergraduate Approach\n\n\n\n\nPackage Installation\nStudents install independently\nProvide RStudio Cloud workspace (pre-installed)\n\n\nScript Writing\nWrite from scratch\nFill-in-the-blank templates with TODOs\n\n\nError Debugging\nGoogle errors, consult Stack Overflow\nDebugging cheat sheet with common errors\n\n\nData Import\nUse read_csv(), troubleshoot paths\nProvide data() pre-loaded datasets\n\n\nFunction Documentation\nRead ?function help files\nAnnotated code with plain-English comments\n\n\n\nExample: UG Template for Mann-Whitney Test\nB. Assessment Scaffolding\n\n\n\n\n\n\n\n\nAssessment Type\nGraduate Version\nUndergraduate Version\n\n\n\n\nStudy Design Proposal\nOpen-ended design (any topic)\nChoose from 3 pre-approved scenarios\n\n\nData Analysis Report\nIndependent variable selection\nGuided: “Analyze X using method Y”\n\n\nQuizzes\nConceptual + calculation\nMore multiple choice, less open-ended\n\n\nLab Write-Ups\nInterpret all output\nAnswer specific questions about key output\n\n\nFinal Project\nIndividual, novel analysis\nPairs allowed, provided dataset\n\n\n\nExample UG Assignment 2 Prompt:\n\nAssignment 2: Guided Data Analysis (Undergraduate)\nDataset: anxiety_study.csv (pre-loaded in RStudio Cloud)\nResearch Question: Does a 6-week mindfulness intervention reduce anxiety scores?\nPart 1: Data Screening (20 points) 1. Create a histogram of pre-test anxiety scores. Are there any outliers? (Use boxplot too) 2. Check normality using a Q-Q plot. Does the distribution look roughly normal? 3. Are there any missing values? Report the percentage missing.\nPart 2: Analysis (40 points) 4. Conduct a paired t-test comparing pre and post scores. Report t, df, p-value. 5. Calculate Cohen’s d for the difference. Is this a small, medium, or large effect? 6. Create a plot showing pre and post scores for each participant (use geom_line).\nPart 3: Interpretation (40 points) 7. Write a Results paragraph (150-200 words) in APA style. 8. Write a Discussion paragraph addressing: Did the intervention work? What are the limitations?\nDeliverable: RMarkdown file (.Rmd) + knitted PDF. Use the provided template.\n\nC. Lecture Structure Adjustments\n\n\n\n\n\n\n\n\nComponent\nGraduate (90 min)\nUndergraduate (90 min)\n\n\n\n\nLecture\n60 min (theory + proofs)\n40 min (concepts + examples)\n\n\nLive Coding\n15 min (students follow along)\n30 min (instructor codes, students observe, then try)\n\n\nDiscussion\n15 min (open-ended)\n20 min (structured prompts with think-pair-share)\n\n\n\nExample UG Lecture Outline (Ch 4: Mann-Whitney U)\n\n\n\n\n\n\n\n\nTime\nActivity\nContent\n\n\n\n\n0:00–0:10\nWarm-Up\nPoll: “Have you ever used a t-test? What assumptions does it make?”\n\n\n0:10–0:25\nConcept\nWhy ranks? Show skewed data where t-test fails. Introduce Mann-Whitney logic.\n\n\n0:25–0:35\nDemo\nInstructor runs wilcox.test() on example data, interprets output.\n\n\n0:35–0:50\nPractice\nStudents run on practice dataset (provided), answer worksheet questions.\n\n\n0:50–1:05\nCompare\n“When would you use Mann-Whitney vs. t-test?” Create comparison table.\n\n\n1:05–1:20\nLab Start\nBegin Lab 3 (Part 1: Mann-Whitney). Students work, instructor circulates.\n\n\n1:20–1:30\nDebrief\nCommon errors, preview next week (Wilcoxon).\n\n\n\n\n\n7.4.6.4 3. Lab Modifications\nUndergraduate Lab Philosophy: - Shorter: 60 min instead of 90 min - More Guided: Step-by-step instructions with expected output shown - Checkpoints: Instructor pauses class every 15 min to ensure everyone is on track - Grading: Completion-based (not correctness-based) to reduce anxiety\nExample: Lab 2 (Fisher’s Exact Test) - UG vs. Graduate\n\n\n\n\n\n\n\n\nSection\nGraduate Version\nUndergraduate Version\n\n\n\n\nIntroduction\n“Read about Fisher’s exact test and when to use it.”\n“Watch 5-min video on Fisher’s exact test. Take notes on 3 key points.”\n\n\nStep 1\n“Create a 2×2 contingency table from raw data.”\n“The table is already created for you: table_data &lt;- ... Run this code.”\n\n\nStep 2\n“Apply Fisher’s exact test.”\n“Run: fisher.test(table_data). What is the p-value? Write it here: ___”\n\n\nStep 3\n“Interpret the odds ratio and confidence interval.”\n“The odds ratio is printed. Is it greater than 1? Yes/No. What does this mean?”\n\n\nCheckpoint\nSelf-paced\n“STOP: Raise hand when you reach this point. Instructor will check.”\n\n\nExtension\n“Compare with chi-square test. Why do p-values differ?”\n(Optional for fast finishers)\n\n\nSubmission\nFull write-up with interpretation\n“Answer the 5 questions in the lab worksheet. Submit via LMS.”\n\n\n\n\n\n7.4.6.5 4. Recommended Weekly Pacing (UG 15-Week Course)\nPacing Philosophy: One new concept per week. Slow is smooth, smooth is fast.\n\n\n\nWeek\nNew Concept\nConsolidation\nLab Time\n\n\n\n\n1\nSmall samples introduction\nNone (course setup)\n60 min\n\n\n2\nResearch questions\nCh 1 recap\n60 min\n\n\n3\nPower analysis\nCh 2 worksheet\n60 min\n\n\n4\nReliability basics\nQuiz 1 review\n60 min\n\n\n5\nData screening\nCh 10 exercises\n60 min\n\n\n6\nMissing data\nCh 11 recap\n60 min\n\n\n7\nMidterm Review\nWeeks 1-6 synthesis\n90 min open lab\n\n\n8\nFisher’s exact\nQuiz 2 review\n60 min\n\n\n9\nMann-Whitney U\nCh 3 recap\n60 min\n\n\n10\nWilcoxon signed-rank\nCh 4 Part 1 recap\n60 min\n\n\n11\nEffect sizes\nQuiz 3 review\n60 min\n\n\n12\nConfidence intervals\nCh 13 Part 1 recap\n60 min\n\n\n13\nNon-significant results\nCh 15 case studies\n60 min\n\n\n14\nVisualization\nDraft feedback\n60 min\n\n\n15\nPresentations\nPeer review\n90 min presentations\n\n\n\n\n\n7.4.6.6 5. Student Support Structures\nFor Undergraduates, Provide:\n\nPre-Course Checklist:\n\n✓ Install R and RStudio (video tutorial provided)\n✓ Complete “Intro to R” module (DataCamp, 2 hours)\n✓ Read syllabus and sign learning contract\n✓ Join course Slack/Discord for help\n\nWeekly Office Hours:\n\nR Help Sessions: Mon/Wed 4-5pm (drop-in, no appointment)\nConcept Review: Tue 3-4pm (by appointment)\nAssignment Help: Thu 2-4pm (open door)\n\nPeer Support:\n\nStudy Groups: Form groups of 3-4 (sign-up Week 1)\nPeer Tutors: Upper-level students (if available)\nSlack Channels: #r-help, #lab-questions, #general\n\nFormative Feedback:\n\nLow-Stakes Quizzes: Can retake once (take higher score)\nLab Checkpoints: Graded on completion, not correctness\nDraft Feedback: Assignment 3 draft reviewed before final submission\n\nAccommodations:\n\nExtended Time: Extra 50% time on quizzes (documented need)\nAlternative Formats: Oral exams instead of written (case-by-case)\nFlexible Deadlines: 2-day grace period on assignments (email notification required)\n\n\n\n\n7.4.6.7 6. Common UG Challenges & Solutions\n\n\n\n\n\n\n\n\nChallenge\nSymptoms\nSolutions\n\n\n\n\nR Anxiety\n“I’m not a programmer!”\nRStudio Cloud, fill-in-blanks, peer coding\n\n\nStats Prerequisite Gap\nWeak t-test/ANOVA foundation\nWeek 0 review session (optional), provide cheat sheet\n\n\nOverwhelm\n“Too much new info!”\nOne concept/week, frequent recaps, visual aids\n\n\nImposter Syndrome\n“Everyone else gets it…”\nNormalize struggle, celebrate small wins, growth mindset messages\n\n\nMotivation\n“When will I use this?”\nReal examples from student interests, guest speakers\n\n\n\nExample Solutions in Action:\nR Anxiety → Pair Programming - Assign “Driver” (types code) and “Navigator” (reads instructions, checks output) - Rotate roles every 15 min - Both submit same lab (collaboration encouraged)\nStats Gap → Visual Cheat Sheet - Create one-page handout: “T-Test vs. Mann-Whitney: When to Use What” - Flowchart format with decision tree - Laminate and allow on quizzes\nOverwhelm → Daily Learning Objectives - Start each class: “Today you will learn X, practice Y, and be able to Z” - End each class: “Today you learned X. Next time: Y.” - Weekly email: “This week we covered… Next week we’ll cover…”\n\n\n7.4.6.8 7. Sample UG Quiz (15 minutes, 10 questions)\nQuiz 2: Data Screening & Missing Data (Chapters 9, 11, 12)\nOpen-book, open-notes. You may use R/RStudio.\nPart A: Multiple Choice (5 questions, 2 pts each)\n\nWhich is NOT a reason to check for outliers in small samples?\n\n\nOutliers can dominate the mean\n\n\nOutliers can violate normality assumptions\n\n\nOutliers always indicate data entry errors\n\n\nOutliers can distort correlations\n\n\nIf data are MCAR (Missing Completely at Random), which is true?\n\n\nMissingness is related to the outcome\n\n\nMissingness is random and unrelated to any variable\n\n\nWe should always delete cases with missing data\n\n\nWe cannot use multiple imputation\n\n\nA Q-Q plot shows points far from the diagonal line. This suggests:\n\n\nPerfect normality\n\n\nDeparture from normality\n\n\nNo outliers\n\n\nHomoscedasticity\n\n\nWhat does a Shapiro-Wilk test p-value of 0.02 mean?\n\n\nData are definitely normal\n\n\nData are likely not normal (reject normality)\n\n\nWe need more data\n\n\nThe mean is 0.02\n\n\nMultiple imputation creates:\n\n\nOne perfect dataset with no missing values\n\n\nMany plausible datasets, then pools results\n\n\nZeros in place of missing values\n\n\nCopies of the original dataset\n\n\n\nPart B: Short Answer (5 questions, 2 pts each)\n\nYou have n = 20 participants. Three have outlier scores (z &gt; 3). Should you delete them? Justify your answer in 1-2 sentences.\nWhat is the difference between a boxplot and a histogram for detecting outliers?\nIf 30% of your data are missing on one variable (n = 25), would you recommend multiple imputation or complete-case analysis? Why?\nA researcher conducts a t-test without checking normality (n = 12 per group). The data are heavily skewed. What should they have done instead?\nWrite one sentence interpreting this result: “After multiple imputation (m = 20), the pooled estimate for the intervention effect was β = 0.45 (SE = 0.12, p = 0.001).”\n\nAnswer Key Provided to Instructor\n\n\n\n\n7.4.7 2.7 Assignment 3: Complete Undergraduate Research Report Guide\n\n7.4.7.1 Overview\nConcern: Can undergraduate students complete a full research project with only 9 core chapters?\nAnswer: Yes, with proper scaffolding and clear deliverables.\nThis section provides a complete assignment template that undergraduate students can successfully complete using only the simplified content from Chapters 1, 2, 3, 4, 9, 11, 13, 15, 16.\n\n\n\n7.4.7.2 Assignment 3: Independent Research Report (UG Version)\nDue Dates: - Draft: End of Week 14 (peer review) - Final: End of Week 15 (with presentation)\nFormat: - 5–7 pages (double-spaced, 12pt font, 1-inch margins) - APA 7th edition style - RMarkdown (.Rmd) + knitted PDF - Groups of 2 allowed (or individual)\nWeight: 20% of final grade\n\n\n\n7.4.7.3 Part 1: Assignment Instructions for Students\nASSIGNMENT 3: Small-Sample Research Report\nObjective: Conduct a complete quantitative analysis of a small-sample dataset (n &lt; 30) and report findings following best practices learned in this course.\nYour Task:\nYou will analyze one of the provided datasets (or your own data with instructor approval) and write a research report addressing a specific research question. Your report must demonstrate:\n\nProper data screening\nAppropriate test selection for small samples\nEffect size calculation and interpretation\nAppropriate visualization\nTransparent reporting of limitations\n\n\nSTEP 1: Choose Your Dataset (Week 11)\nSelect ONE dataset from these options:\nOption A: Sleep Study (n = 24, paired) - Research Question: Does a sleep hygiene intervention increase sleep duration? - Data: sleep_intervention.csv (pre/post design) - Variables: participant_id, pre_hours, post_hours, age, gender - Recommended Test: Wilcoxon signed-rank test (if non-normal) or paired t-test\nOption B: Teaching Methods (n₁ = 14, n₂ = 12) - Research Question: Do students taught with active learning score higher than lecture-only students? - Data: teaching_methods.csv (two independent groups) - Variables: student_id, method (active vs. lecture), exam_score, prior_GPA - Recommended Test: Mann-Whitney U test (if skewed) or independent t-test\nOption C: Mindfulness & Anxiety (n = 18, paired) - Research Question: Does a 6-week mindfulness program reduce anxiety scores? - Data: anxiety_study.csv (pre/post design) - Variables: participant_id, pre_anxiety, post_anxiety, age, compliance (%) - Recommended Test: Paired t-test (if normal) or Wilcoxon signed-rank\nOption D: Your Own Data (with approval) - Must have n &lt; 30 - Must have clear research question - Must be approved by instructor by Week 11\n\nSTEP 2: Data Screening (Week 12-13)\nBefore analyzing, you MUST screen your data and document decisions:\nRequired Checks:\n\nOutliers: Create boxplots. Calculate z-scores. Any |z| &gt; 3?\nNormality: Create Q-Q plots. Run Shapiro-Wilk test.\nMissing Data: Report % missing. Use complete-case or simple imputation.\nData Entry Errors: Check for impossible values (e.g., negative scores, out-of-range values).\n\nDeliverable for Draft: 1 paragraph + 2-3 plots documenting screening decisions.\n\nSTEP 3: Analysis (Week 13-14)\nConduct your primary analysis using methods from this course:\nRequired Components:\n\nDescriptive Statistics: Report mean, SD, median, IQR for each group/time.\nPrimary Test: Conduct the appropriate test (t-test, Mann-Whitney, Wilcoxon, or Fisher’s exact).\nEffect Size: Calculate Cohen’s d (for t-tests) or rank-biserial correlation (for nonparametric tests).\nConfidence Interval: Report 95% CI for the effect size.\nVisualization: Create ONE publication-quality plot showing your key finding.\n\nDeliverable for Draft: Results section (2 paragraphs) with all components above.\n\nSTEP 4: Write Your Report (Week 14)\nRequired Sections:\n1. Introduction (1 page, ~300 words) - Background: Why is this question important? - Brief literature review: Cite 2-3 relevant studies. - Research question and hypothesis: State clearly.\n2. Methods (1–1.5 pages, ~400 words) - Participants: Describe sample (n, demographics if available). - Measures: Describe outcome variable (how measured, scale/range). - Procedure: Describe how data were collected (even if simulated). - Data Screening: Summarize outlier/normality checks. - Analysis Plan: State your test, alpha level (0.05), software (R version).\n3. Results (1.5 pages, ~450 words) - Descriptive Statistics: Table or text reporting M, SD, etc. - Assumption Checks: Report normality test results. - Primary Analysis: Report test statistic, df, p-value. - Example: “A paired t-test revealed a significant difference, t(17) = 3.45, p = .003.” - Effect Size: Report Cohen’s d or r with 95% CI. - Example: “The effect size was large, d = 0.81, 95% CI [0.28, 1.34].” - Figure: Include your visualization with caption.\n4. Discussion (1.5 pages, ~450 words) - Summary: What did you find? - Interpretation: What does it mean? Is the effect practically significant? - Limitations: Acknowledge small sample size, lack of random assignment (if applicable), generalizability concerns. - Implications: What should future research do? What are practical applications?\n5. References (APA format) - At least 3 sources (can include textbook)\n6. Appendix (R Code) - Include all R code used (cleaning, analysis, visualization) - Must be reproducible\n\nSTEP 5: Create Your Presentation (Week 15)\n10-Minute Group Presentation\nRequired Slides (7-8 slides):\n\nTitle Slide: Research question, your names\nBackground: Why does this matter? (1 slide)\nMethods: Sample, measures, analysis (1-2 slides)\nResults - Descriptives: Table or simple plot (1 slide)\nResults - Main Finding: Your key figure + test results (1 slide)\nDiscussion: Interpretation, limitations (1 slide)\nConclusions: Take-home message (1 slide)\nQ&A: Be ready for questions\n\nTips: - Rehearse to stay within 10 minutes - Use large fonts (≥24pt for body text) - Avoid reading slides word-for-word - Explain statistics in plain language\n\n\n\n7.4.7.4 Part 2: Detailed Grading Rubric (100 points)\n\n\n\n\n\n\n\n\n\n\n\nSection\nExcellent (A)\nProficient (B)\nDeveloping (C)\nNeeds Improvement (D/F)\nPoints\n\n\n\n\nIntroduction (15 pts)\nClear research question, strong rationale, 3+ citations, testable hypothesis\nResearch question clear, adequate rationale, 2 citations\nResearch question vague, weak rationale, 1 citation\nNo clear question or rationale\n/15\n\n\nMethods - Participants (5 pts)\nComplete description: n, demographics, recruitment\nn and basic demographics\nOnly n reported\nMissing or incorrect\n/5\n\n\nMethods - Measures (5 pts)\nClear description of all variables, scales, reliability if applicable\nVariables described, some details missing\nVague variable description\nMissing or confusing\n/5\n\n\nMethods - Analysis Plan (10 pts)\nJustifies test choice for small n, states alpha, software, screening approach\nStates test and alpha, minimal justification\nTest stated, no justification\nInappropriate test or missing\n/10\n\n\nResults - Descriptives (10 pts)\nTable/text with M, SD, median, IQR for all groups/conditions\nM and SD reported, some missing\nOnly means reported\nIncomplete or incorrect\n/10\n\n\nResults - Assumption Checks (5 pts)\nReports normality tests, outlier checks, decisions documented\nNormality test reported, decisions made\nMentions checks, no detail\nMissing or ignored\n/5\n\n\nResults - Primary Test (15 pts)\nCorrect test, accurate reporting (test stat, df, p), proper APA format\nCorrect test, minor formatting errors\nTest reported, major errors in format\nWrong test or missing info\n/15\n\n\nResults - Effect Size (10 pts)\nCorrect ES for test, 95% CI reported, interpreted (small/med/large)\nES calculated, CI reported, no interpretation\nES reported, no CI\nMissing or incorrect\n/10\n\n\nResults - Figure (10 pts)\nPublication-quality: clear labels, error bars/uncertainty, follows best practices\nGood plot, minor issues (axis labels, legend)\nBasic plot, multiple issues\nPoor quality or missing\n/10\n\n\nDiscussion - Interpretation (5 pts)\nContextualizes findings, distinguishes statistical vs. practical significance\nInterprets findings, limited depth\nRestates results, minimal interpretation\nMisinterprets or missing\n/5\n\n\nDiscussion - Limitations (10 pts)\nIdentifies small n, threats to validity, generalizability, missing data, etc.\nMentions 2-3 limitations with depth\nLists limitations, no elaboration\nGeneric or missing\n/10\n\n\nWriting Quality (5 pts)\nClear, concise, professional, no grammatical errors\nMinor grammatical errors, clear communication\nSeveral errors, somewhat unclear\nPoor writing quality\n/5\n\n\nAPA Format (5 pts)\nPerfect APA 7th: citations, references, headings, numbers\n1-2 APA errors\n3-5 APA errors\nMany APA errors or non-compliance\n/5\n\n\n\nTOTAL: /100 points (× 0.20 = 20% of final grade)\n\n\n\n7.4.7.5 Part 3: Provided Support Materials\nTo ensure UG students succeed, provide these scaffolding materials:\n1. RMarkdown Template (Starter File)\nDecision: [State whether you will use parametric or nonparametric test based on normality.]",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#analysis-plan",
    "href": "chapters/part-h-instructors-manual.html#analysis-plan",
    "title": "7  Part H: Instructor’s Manual",
    "section": "7.5 Analysis Plan",
    "text": "7.5 Analysis Plan\nWe conducted a [TEST NAME] to compare [GROUP 1] vs. [GROUP 2]. We set alpha at .05 and used R version [X.X.X]. Effect sizes were calculated using [Cohen’s d / rank-biserial correlation].",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#descriptive-statistics",
    "href": "chapters/part-h-instructors-manual.html#descriptive-statistics",
    "title": "7  Part H: Instructor’s Manual",
    "section": "8.1 Descriptive Statistics",
    "text": "8.1 Descriptive Statistics\n[Report means, SDs, medians for each group/condition]",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#assumption-checks",
    "href": "chapters/part-h-instructors-manual.html#assumption-checks",
    "title": "7  Part H: Instructor’s Manual",
    "section": "8.2 Assumption Checks",
    "text": "8.2 Assumption Checks\nThe Shapiro-Wilk test indicated [normality was met / violated], W = [value], p = [p-value]. [No outliers were detected / X outliers were found and retained/removed because…]",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#primary-analysis",
    "href": "chapters/part-h-instructors-manual.html#primary-analysis",
    "title": "7  Part H: Instructor’s Manual",
    "section": "8.3 Primary Analysis",
    "text": "8.3 Primary Analysis\nA [TEST NAME] revealed [a significant difference / no significant difference], t([df]) = [test statistic], p = [p-value]. The effect size was [small/medium/large], d = [value], 95% CI [[lower, upper]].",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#visualization",
    "href": "chapters/part-h-instructors-manual.html#visualization",
    "title": "7  Part H: Instructor’s Manual",
    "section": "8.4 Visualization",
    "text": "8.4 Visualization",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#answer-keys",
    "href": "chapters/part-h-instructors-manual.html#answer-keys",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.1 3. ANSWER KEYS",
    "text": "11.1 3. ANSWER KEYS\n\n11.1.1 3.1 Chapter 1 Homework Solutions\nQuestion 1: A study with n = 15 finds d = 0.75 and p = 0.08. The authors conclude “no significant effect.” Critique this conclusion.\nAnswer: The conclusion is misleading. With n = 15 and d = 0.75 (a large effect by Cohen’s benchmarks), the study likely had low power (approximately 40–50% to detect d = 0.75 at α = 0.05). A p-value of 0.08 indicates the data are marginally inconsistent with the null hypothesis, but insufficient to reject it at α = 0.05. However, p &gt; 0.05 does not mean “no effect”—it means “no statistically significant evidence of effect given the sample size.” The confidence interval for d would be wide (e.g., 0.0 to 1.5), including both trivial and very large effects. The authors should report the effect size, confidence interval, and acknowledge low power. A better conclusion: “We observed a large effect (d = 0.75), but the small sample (n = 15) yields wide uncertainty (95% CI: X to Y). The result is consistent with both a moderate-to-large effect and sampling variability.”\nGrading Rubric (5 points): - Identifies low power (1 pt) - Explains p &gt; 0.05 ≠ “no effect” (2 pts) - Mentions effect size and CI (1 pt) - Proposes better conclusion (1 pt)\n\nQuestion 2: Calculate post-hoc power for a t-test with n₁ = n₂ = 12, d = 0.60, α = 0.05.\nAnswer:\nOutput:\n     Two-sample t test power calculation \n\n              n = 12\n              d = 0.6\n      sig.level = 0.05\n          power = 0.34\n    alternative = two.sided\nInterpretation: The study had only 34% power to detect d = 0.60 at α = 0.05. This is severely underpowered (conventional target: 80%). The likelihood of a Type II error (failing to detect a true effect) is 66%.\nGrading (3 points): - Correct R code (1 pt) - Correct power value (1 pt) - Interpretation (1 pt)\n\n\n\n11.1.2 3.2 Chapter 3 Homework Solutions\nQuestion 1: A 2×2 table shows Treatment A (10 successes, 2 failures) vs. Treatment B (4 successes, 8 failures). Conduct Fisher’s exact test.\nAnswer:\n# Create contingency table\ntable_data &lt;- matrix(c(10, 2, 4, 8), nrow = 2, byrow = TRUE)\nrownames(table_data) &lt;- c(\"Treatment A\", \"Treatment B\")\ncolnames(table_data) &lt;- c(\"Success\", \"Failure\")\n\n# Fisher's exact test\nfisher.test(table_data)\nOutput:\nFisher's Exact Test for Count Data\n\ndata:  table_data\np-value = 0.0265\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.15 70.89\nsample estimates:\nodds ratio \n      6.77\nInterpretation: The two-sided p-value is 0.0265, indicating significant evidence (α = 0.05) that success rates differ between treatments. The estimated odds ratio is 6.77 (95% CI: 1.15 to 70.89), suggesting Treatment A has higher odds of success, though the CI is wide due to small sample size.\nGrading (4 points): - Correct table construction (1 pt) - Correct function call (1 pt) - Correct p-value and OR (1 pt) - Interpretation (1 pt)\n\nQuestion 2: Generate a permutation distribution for the difference in means between two groups (n₁ = 8, n₂ = 7).\nAnswer:\n# Simulated data\nset.seed(123)\ngroup1 &lt;- c(12, 15, 18, 14, 16, 13, 17, 15)\ngroup2 &lt;- c(10, 11, 9, 12, 10, 8, 11)\n\n# Observed difference\nobs_diff &lt;- mean(group1) - mean(group2)\n\n# Permutation test\nlibrary(coin)\nperm_test &lt;- oneway_test(c(group1, group2) ~ factor(rep(c(\"A\", \"B\"), c(8, 7))),\n                         distribution = approximate(nresample = 9999))\nperm_test\nExpected Output:\nApproximative Two-Sample Fisher-Pitman Permutation Test\n\ndata:  ... by factor(...)\nZ = 3.12, p-value = 0.002\nInterpretation: The permutation test yields p = 0.002, providing strong evidence that the groups differ. The permutation approach is valid even with small, potentially non-normal data.\nGrading (5 points): - Correct data setup (1 pt) - Observed difference calculated (1 pt) - Correct permutation function (2 pts) - Interpretation (1 pt)\n\n\n\n11.1.3 3.3 Chapter 5 Homework Solutions\nQuestion 1: Fit a Firth-penalized logistic regression to predict binary outcome (y) from predictor (x) with separation.\nAnswer:\nlibrary(logistf)\n\n# Data with separation\ndf &lt;- data.frame(\n  x = c(1, 2, 3, 4, 5, 6, 7, 8),\n  y = c(0, 0, 0, 0, 1, 1, 1, 1)\n)\n\n# Standard logistic regression (will warn about separation)\nglm_standard &lt;- glm(y ~ x, data = df, family = binomial)\nsummary(glm_standard)\n\n# Firth's penalized logistic regression\nglm_firth &lt;- logistf(y ~ x, data = df)\nsummary(glm_firth)\nExpected Results: - Standard GLM: Coefficient estimates inflate (e.g., β₁ = 15+), standard errors huge, warning about “fitted probabilities numerically 0 or 1.” - Firth GLM: Finite coefficient (e.g., β₁ ≈ 2.5), reasonable SE (e.g., 1.8), p-value valid.\nInterpretation: Firth’s method applies a penalty that shrinks coefficients away from infinity, yielding stable estimates even with perfect separation.\nGrading (5 points): - Correct data with separation (1 pt) - Standard GLM fitted (1 pt) - Firth GLM fitted (2 pts) - Comparison and interpretation (1 pt)\n\n\n\n11.1.4 3.4 Chapter 9 Homework Solutions\nQuestion 1: Calculate the minimum detectable effect (MDE) for a two-sample t-test with n₁ = n₂ = 10, α = 0.05, power = 0.80.\nAnswer:\nlibrary(pwr)\nresult &lt;- pwr.t.test(n = 10, sig.level = 0.05, power = 0.80, type = \"two.sample\")\nresult$d\nOutput:\n[1] 1.28\nInterpretation: With n = 10 per group, 80% power, and α = 0.05, the MDE is d = 1.28—a very large effect by Cohen’s benchmarks (small = 0.2, medium = 0.5, large = 0.8). This study can only reliably detect extremely large effects. Smaller effects (e.g., d = 0.5) would require n ≈ 64 per group.\nGrading (3 points): - Correct function call (1 pt) - Correct MDE (1 pt) - Interpretation with context (1 pt)\n\n\n\n11.1.5 3.5 Chapter 12 Homework Solutions\nQuestion 1: Implement multiple imputation with MICE for a dataset with 20% missingness.\nAnswer:\nlibrary(mice)\nlibrary(VIM)\n\n# Load or simulate data with missingness\ndata(\"nhanes2\")  # Built-in dataset with missing values\n\n# Visualize missingness pattern\nmd.pattern(nhanes2)\n\n# Multiple imputation (m = 20 datasets)\nimp &lt;- mice(nhanes2, m = 20, method = \"pmm\", seed = 123, printFlag = FALSE)\n\n# Check convergence\nplot(imp)\n\n# Fit model on imputed data\nfit &lt;- with(imp, lm(bmi ~ age + hyp + chl))\n\n# Pool results\npooled &lt;- pool(fit)\nsummary(pooled)\nExpected Output:\n         term estimate std.error statistic    df p.value\n1 (Intercept)   20.324     4.123     4.93  12.5   0.000\n2        age    1.234     0.456     2.71   8.9   0.024\n3        hyp    2.145     1.023     2.10  10.2   0.061\n4        chl    0.034     0.012     2.83   9.8   0.018\nInterpretation: The pooled estimates account for both within-imputation and between-imputation variance (Rubin’s rules). Coefficients, SEs, and p-values are valid under MAR assumption.\nGrading (6 points): - Missingness visualization (1 pt) - Correct mice() call (2 pts) - Convergence check (1 pt) - Model fitting and pooling (2 pts)",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#grading-rubrics",
    "href": "chapters/part-h-instructors-manual.html#grading-rubrics",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.2 4. GRADING RUBRICS",
    "text": "11.2 4. GRADING RUBRICS\n\n11.2.1 4.1 Assignment 1: Study Design Proposal (100 points)\nComponents:\n\n\n\n\n\n\n\n\nSection\nPoints\nCriteria\n\n\n\n\nResearch Question\n15\nClear, testable hypothesis; appropriate for small sample; practical relevance\n\n\nSampling Plan\n20\nJustified sample size; realistic recruitment; addresses feasibility\n\n\nPower Analysis\n25\nCorrect calculation; sensitivity analysis; interprets MDE; discusses trade-offs\n\n\nMeasurement\n15\nIdentifies valid, reliable measures; plans pilot testing; acknowledges brevity constraints\n\n\nAnalytic Plan\n15\nSpecifies primary/secondary analyses; chooses appropriate methods (exact, nonparametric, Bayesian); pre-registers decisions\n\n\nLimitations\n10\nAcknowledges small-sample limitations; discusses generalizability; transparent about power\n\n\n\nGrading Notes: - Exemplary (90–100): Demonstrates deep understanding; creative solutions; pre-registration plan - Proficient (80–89): Solid grasp of concepts; minor gaps in justification - Developing (70–79): Basic understanding; missing key elements (e.g., no power analysis) - Needs Improvement (&lt;70): Major conceptual errors; unrealistic design\n\n\n\n11.2.2 4.2 Assignment 2: Data Analysis (100 points)\nStudents analyze a provided dataset (e.g., anxiety_study.csv) and write a 5–7 page report.\n\n\n\n\n\n\n\n\nSection\nPoints\nCriteria\n\n\n\n\nIntroduction\n10\nStates research question; reviews relevant literature briefly\n\n\nMethods\n20\nDescribes sample, measures, data screening, analytic approach\n\n\nData Screening\n15\nChecks outliers, normality, missingness; documents decisions\n\n\nPrimary Analysis\n25\nCorrect test selection (e.g., Wilcoxon if non-normal); appropriate inference; interprets p-value and effect size\n\n\nEffect Sizes & CIs\n15\nReports Cohen’s d or rank-biserial r; computes CI; interprets precision\n\n\nVisualization\n10\nCreates informative plot (e.g., raincloud, dot plot with error bars); follows best practices\n\n\nDiscussion\n5\nAcknowledges limitations; avoids over-interpretation; discusses practical significance\n\n\n\nCommon Errors to Watch For: - Using t-test on severely skewed data without transformation or nonparametric alternative - Reporting p-values without effect sizes - Over-interpreting non-significant results (claiming “no effect”) - Deleting outliers without justification\n\n\n\n11.2.3 4.3 Lab Participation Rubric (Per Lab, 10 points)\n\n\n\n\n\n\n\n\nCriterion\nPoints\nDescription\n\n\n\n\nCompletion\n5\nAll code chunks run without errors; answers all checkpoint questions\n\n\nInterpretation\n3\nCorrectly interprets output; explains what results mean\n\n\nEngagement\n2\nAsks questions; helps peers; attempts discussion questions\n\n\n\nLate Submission: -20% per day up to 3 days; 0 points after 3 days.\n\n\n\n11.2.4 4.4 Final Presentation Rubric (50 points)\n15-minute presentation of final project (Assignment 3).\n\n\n\n\n\n\n\n\nCriterion\nPoints\nDescription\n\n\n\n\nContent Clarity\n15\nClear research question; logical flow; summarizes methods and results concisely\n\n\nStatistical Rigor\n15\nAppropriate methods; correct interpretation; acknowledges limitations\n\n\nVisualization\n10\nEffective slides; informative figures; no chart junk\n\n\nQ&A Handling\n5\nAnswers questions thoughtfully; defends choices; acknowledges uncertainties\n\n\nTime Management\n5\nStays within 15 minutes; covers all required sections",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#common-teaching-challenges",
    "href": "chapters/part-h-instructors-manual.html#common-teaching-challenges",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.3 5. COMMON TEACHING CHALLENGES",
    "text": "11.3 5. COMMON TEACHING CHALLENGES\n\n11.3.1 5.1 Challenge: Students Resist Bayesian Methods\nSymptoms: - “Priors are just making up data” - “I don’t know how to choose priors” - Confusion about interpreting credible intervals vs. confidence intervals\nSolutions: 1. Start with weakly informative priors: Use brms defaults (student-t priors) and show sensitivity analyses. 2. Compare frequentist and Bayesian results: Run both lm() and brm() on same data—show results are similar with weak priors. 3. Use prior predictive checks: Have students simulate data from priors to see what they imply. 4. Frame as regularization: Explain priors as “hedges against overfitting” (like penalized regression).\nActivity: - Lab 5: Students run same model with three priors (very weak, weakly informative, informative) and compare posteriors. - Discussion: When would you use informative priors? (e.g., clinical trials with historical data)\n\n\n\n11.3.2 5.2 Challenge: Over-Reliance on P-Values\nSymptoms: - Students report only p-values, ignore effect sizes - Statements like “p = 0.06 means it almost worked” - Belief that p &lt; 0.05 = “proof”\nSolutions: 1. Ban p-values for one week: Require reporting only effect sizes and CIs. 2. Show p-value variability: Simulate 100 studies with same population effect; plot histogram of p-values (many &gt; 0.05 even when effect is real). 3. Use “new statistics” framework: Emphasize estimation over testing (Cumming, 2014).\nActivity: - Students replicate a published small-sample study (simulated data) 50 times and plot distribution of p-values and effect sizes. Observe that p-values vary wildly; effect size estimates center on truth.\n\n\n\n11.3.3 5.3 Challenge: Confusion About “No Evidence of Effect” vs. “Evidence of No Effect”\nSymptoms: - p &gt; 0.05 interpreted as “no effect exists” - Failure to consider power - No discussion of equivalence testing\nSolutions: 1. Introduce TOST early: Show equivalence testing in Chapter 15. 2. Require power analysis in all assignments: Students must state “we had X% power to detect d = Y.” 3. Use visualization: Plot CI overlapping zero—ask “does this rule out meaningful effects?”\nActivity: - Give students underpowered study (n = 12, d = 0.5, p = 0.15). Ask them to: - Calculate 95% CI for d - Calculate post-hoc power - Rewrite conclusion appropriately\n\n\n\n11.3.4 5.4 Challenge: R Programming Struggles\nSymptoms: - Errors with file paths, working directories - Confusion about piping (%&gt;% vs. |&gt;) - Cannot install packages\nSolutions: 1. Provide RStudio Cloud workspace: Pre-installed packages, datasets loaded. 2. Live coding in lectures: Share screen, type slowly, explain each line. 3. “Code-along” labs: Instructor codes alongside students; pause for questions. 4. Debugging guide: Create handout with common errors (e.g., “object not found”, “non-numeric argument”).\nResources: - Provide RMarkdown templates for assignments - Create video tutorials for common tasks (e.g., “How to install mice package”) - Office hours dedicated to R troubleshooting\n\n\n\n11.3.5 5.5 Challenge: Diverse Student Backgrounds\nSymptoms: - Some students breeze through labs; others struggle with basic R - Different fields have different norms (education vs. health sciences) - Uneven prerequisite preparation\nSolutions: 1. Pre-course survey: Assess R experience, stats background, research area. 2. Differentiated labs: Provide “basic” and “advanced” versions of exercises. 3. Pair programming: Group strong R users with novices. 4. Flexible pacing: Asynchronous modules allow students to work at own pace.\nExample Differentiation (Lab 3): - Basic: Run wilcox.test() on provided data, interpret output. - Advanced: Write function to compute rank-biserial correlation manually; compare with package output.",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#additional-resources",
    "href": "chapters/part-h-instructors-manual.html#additional-resources",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.4 6. ADDITIONAL RESOURCES",
    "text": "11.4 6. ADDITIONAL RESOURCES\n\n11.4.1 6.1 Recommended Textbooks\nPrimary Supplementary Texts: 1. Maxwell, S. E., Delaney, H. D., & Kelley, K. (2017). Designing Experiments and Analyzing Data: A Model Comparison Perspective (3rd ed.). Routledge. - Excellent for understanding small-sample ANOVA, power analysis, effect sizes.\n\nHox, J. J., Moerbeek, M., & van de Schoot, R. (2017). Multilevel Analysis: Techniques and Applications (3rd ed.). Routledge.\n\nUseful for small-cluster designs.\n\nKruschke, J. K. (2015). Doing Bayesian Data Analysis (2nd ed.). Academic Press.\n\nAccessible intro to Bayesian methods with R/JAGS.\n\n\nApplied Examples: 4. Field, A., Miles, J., & Field, Z. (2012). Discovering Statistics Using R. Sage. - Great for students new to R; conversational tone.\n\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.\n\nAdvanced treatment of small-sample regression issues.\n\n\n\n\n\n11.4.2 6.2 R Packages to Highlight\n\n\n\n\n\n\n\n\nPackage\nPurpose\nKey Functions\n\n\n\n\npwr\nPower analysis\npwr.t.test(), pwr.anova.test()\n\n\neffectsize\nEffect size calculation\ncohens_d(), eta_squared()\n\n\ncoin\nPermutation tests\noneway_test(), independence_test()\n\n\nlogistf\nFirth logistic regression\nlogistf()\n\n\nbrms\nBayesian regression\nbrm(), prior(), pp_check()\n\n\nmice\nMultiple imputation\nmice(), pool()\n\n\npsych\nReliability analysis\nalpha(), omega()\n\n\nboot\nBootstrap methods\nboot(), boot.ci()\n\n\nggplot2\nVisualization\ngeom_point(), geom_errorbar()\n\n\nggdist\nUncertainty visualization\nstat_halfeye(), stat_dots()\n\n\n\nInstallation Script (Provide to Students):\n# Install all required packages for the course\ninstall.packages(c(\n  \"pwr\", \"effectsize\", \"coin\", \"logistf\", \"brms\",\n  \"mice\", \"psych\", \"boot\", \"ggplot2\", \"ggdist\",\n  \"tidyverse\", \"rmarkdown\", \"knitr\"\n))\n\n\n\n11.4.3 6.3 Datasets for Teaching\nIncluded in Book: - anxiety_study.csv — Pre/post anxiety scores (n = 18) - employee_engagement.csv — Employee survey (n = 25) - mini_marketing.csv — A/B test (n₁ = 12, n₂ = 14) - process_change.csv — Before/after process improvement (n = 10) - hospital_readmissions.csv — Readmission rates (n = 15 hospitals)\nExternal Datasets: - palmerpenguins::penguins — Penguin measurements (subset for small-sample demos) - datasets::PlantGrowth — Classic small ANOVA dataset (n = 30) - MASS::cats — Body/heart weight (n = 144; subsample to n = 20)\nGenerating Synthetic Data:\n# Generate small dataset with known effect\nset.seed(42)\nn &lt;- 15\ntreatment &lt;- rnorm(n, mean = 100, sd = 15)\ncontrol &lt;- rnorm(n, mean = 85, sd = 15)\n\ndf &lt;- data.frame(\n  score = c(treatment, control),\n  group = factor(rep(c(\"Treatment\", \"Control\"), each = n))\n)\n\n# Save for student use\nwrite.csv(df, \"treatment_study.csv\", row.names = FALSE)\n\n\n\n11.4.4 6.4 Online Resources\nInteractive Tutorials: - Seeing Theory — Visualizations of probability and statistics - rpsychologist — Interactive CI and effect size demos - ShinyApps for Stats — Dashboards for power analysis, NHST\nVideos: - StatQuest (Josh Starmer) — YouTube channel with clear explanations of bootstrap, permutation tests - Richard McElreath’s Lectures — Statistical Rethinking course (Bayesian methods)\nPreprint Repositories: - PsyArXiv, OSF Preprints — Examples of preregistered small-sample studies\n\n\n\n11.4.5 6.5 Assessment Item Bank\nSample Quiz Questions (Multiple Choice):\n\nWhich method is most appropriate for a 2×2 table with expected cell counts &lt; 5?\n\n\nChi-square test\n\n\nFisher’s exact test ✓\n\n\nLogistic regression\n\n\nPearson correlation\n\n\nA study with n = 20 finds p = 0.12. The authors conclude “no effect exists.” What is wrong?\n\n\nNothing, p &gt; 0.05 means no effect\n\n\nThey should use α = 0.10\n\n\np &gt; 0.05 only means insufficient evidence, not “no effect” ✓\n\n\nThey should have used a one-tailed test\n\n\nWhich of the following increases statistical power?\n\n\nLarger sample size ✓\n\n\nSmaller effect size\n\n\nLower alpha level (e.g., 0.01 instead of 0.05)\n\n\nHigher measurement error\n\n\n\nSample Short-Answer Questions:\n\nExplain why Cronbach’s alpha is attenuated (lower) for short scales (3–5 items) compared to long scales (20+ items).\nA researcher plans a study with n = 10 per group and expects d = 0.5. Calculate power and interpret the result.\nWhat is the difference between MCAR, MAR, and MNAR? Give an example of each.",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#lecture-notes-tips",
    "href": "chapters/part-h-instructors-manual.html#lecture-notes-tips",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.5 7. LECTURE NOTES & TIPS",
    "text": "11.5 7. LECTURE NOTES & TIPS\n\n11.5.1 7.1 Chapter 1: When and Why Small Samples\nLearning Objectives: - Students should leave knowing: (1) why large-sample methods fail, (2) when small samples are acceptable, (3) how to justify sample size transparently.\nLecture Outline (90 minutes):\n\nMotivation (15 min):\n\nPoll: “What’s the smallest sample size in your field?”\nShow examples: clinical case studies (n = 1), pilot studies (n = 12), SIDS research (n &lt; 50)\nDiscuss: When are small samples unavoidable? (rare diseases, endangered species, expensive interventions)\n\nWhy Large-Sample Methods Fail (30 min):\n\nAsymptotic approximations: Chi-square test assumes np &gt; 5; violates with small n\nNormality assumptions: t-test robust with n &gt; 30; questionable with n &lt; 15\nPower: Demonstrate with simulation (50 studies, n = 10 vs. n = 100)\nOverfitting: Show regression with p predictors, n = p + 5 (huge standard errors)\n\nWhen Small Samples Are Sufficient (20 min):\n\nLarge effects: d = 1.5+ detectable with n = 10 per group\nHighly controlled settings: Lab experiments with low noise\nProof-of-concept: Early-stage intervention development\nMechanism testing: Not aiming for generalization\n\nWhat Changes with Small Samples (15 min):\n\nMethods: Exact tests, permutation, Bayesian priors, robust estimators\nReporting: Emphasize effect sizes and CIs over p-values\nInterpretation: More cautious; acknowledge uncertainty\n\nActivity (10 min):\n\nStudents pair up and discuss: “Find a published small-sample study (n &lt; 30) in your field. Was the sample size justified? What methods were used?”\n\n\nCommon Student Questions: - “Can I just collect more data?” — Sometimes yes, but often no (cost, time, access). Focus on optimal analysis given constraints. - “Isn’t n = 30 the magic number?” — No. It’s a rough heuristic for CLT, not a universal threshold.\n\n\n\n11.5.2 7.2 Chapter 3: Exact Tests and Resampling\nLearning Objectives: - Conduct Fisher’s exact test in R - Understand when exact tests are conservative - Generate permutation distributions\nLecture Outline (90 minutes):\n\nExact vs. Asymptotic Tests (20 min):\n\nChi-square test: Relies on approximation; poor with small expected counts\nFisher’s exact: Computes exact p-value from hypergeometric distribution\nExample: 2×2 table with n = 12; compare chi-square (p = 0.08) vs. Fisher (p = 0.05)\n\nFisher’s Exact Test in R (25 min):\n\nLive coding: fisher.test()\nInterpret odds ratio and CI\nShow how to extract p-value, OR from output object\n\nPermutation Tests (30 min):\n\nConcept: Null hypothesis = “group labels are exchangeable”\nProcedure: Randomly shuffle labels, recompute test statistic, repeat 10,000 times\nExample: Two-sample t-test via permutation (compare with t.test())\nAdvantage: No distributional assumptions\n\nBootstrap Confidence Intervals (15 min):\n\nConcept: Resample with replacement; estimate sampling distribution\nExample: Bootstrap CI for Cohen’s d\nCaution: Small samples → few unique resamples; wide CIs\n\n\nActivity: - Lab 2: Students run Fisher’s exact test on mini_marketing.csv; compare with chi-square; generate permutation distribution for difference in means.\nCommon Pitfalls: - Forgetting to set set.seed() for reproducibility - Confusing bootstrap (resample with replacement) with permutation (resample without replacement)\n\n\n\n11.5.3 7.3 Chapter 12: Missing Data\nLearning Objectives: - Distinguish MCAR, MAR, MNAR - Implement multiple imputation with mice - Interpret pooled results\nLecture Outline (90 minutes):\n\nMissing Data Mechanisms (20 min):\n\nMCAR (Missing Completely at Random): Missingness unrelated to any variable (e.g., data lost in computer crash)\nMAR (Missing at Random): Missingness related to observed data, not unobserved (e.g., older participants skip exercise questions)\nMNAR (Missing Not at Random): Missingness related to unobserved value (e.g., depressed individuals don’t report mood scores)\nTest: Little’s MCAR test (mice::mcar())\n\nWhy Complete-Case Analysis Fails (15 min):\n\nLoses power (deletes data)\nBiased if MAR or MNAR\nExample: Study with 20% missingness → effective n drops from 50 to 40\n\nMultiple Imputation Theory (20 min):\n\nIdea: Generate m plausible datasets; analyze each; pool results\nPooling (Rubin’s rules): Combine estimates and SEs accounting for within- and between-imputation variance\nAssumptions: MAR; imputation model correctly specified\n\nMICE in R (25 min):\n\nLive coding: mice() with method = \"pmm\" (predictive mean matching)\nCheck convergence: plot(imp)\nFit model: with(imp, lm(...))\nPool: pool(); interpret summary(pooled)\n\nActivity (10 min):\n\nStudents load anxiety_study.csv (with missing data), visualize missingness with md.pattern(), run MICE, pool a t-test.\n\n\nCommon Student Questions: - “How many imputations (m) should I use?” — Default m = 5; modern recommendation m = 20–40 for more stable SEs. - “What if I have MNAR?” — Sensitivity analysis; report results with/without imputation; acknowledge limitation.",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#extending-the-course",
    "href": "chapters/part-h-instructors-manual.html#extending-the-course",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.6 8. EXTENDING THE COURSE",
    "text": "11.6 8. EXTENDING THE COURSE\n\n11.6.1 8.1 Advanced Topics (Optional Modules)\nFor students who complete core material early or want deeper treatment:\nModule A: Sequential Analysis - Content: Group-sequential designs, alpha spending functions, O’Brien-Fleming boundaries - Reading: Jennison & Turnbull (1999), Group Sequential Methods with Applications to Clinical Trials - Activity: Design a two-stage trial; compute stopping boundaries; simulate operating characteristics\nModule B: Bayesian Hierarchical Models - Content: Shrinkage, partial pooling, random effects in brms - Reading: Gelman & Hill (2006), Chapter 12 - Activity: Fit hierarchical model to multi-site small-sample data (e.g., hospital readmissions)\nModule C: Network Meta-Analysis with Few Studies - Content: Mixed treatment comparisons, indirect evidence - Reading: Dias et al. (2013), Evidence Synthesis for Decision Making - Activity: Pool evidence from 3–5 small trials using netmeta package\nModule D: Small-Sample Survival Analysis - Content: Kaplan-Meier with small n, log-rank test, Cox regression with penalization - Reading: Collett (2014), Modelling Survival Data in Medical Research - Activity: Analyze time-to-event data (n = 20) with survival package\n\n\n\n11.6.2 8.2 Capstone Project Ideas\nStudents design and execute a complete small-sample study (real or simulated).\nOption 1: Secondary Data Analysis - Obtain small dataset from repository (ICPSR, OSF, Dataverse) - Conduct thorough analysis following best practices - Write APA-style research report\nOption 2: Pilot Study - Design and conduct pilot (n = 15–25) for student’s thesis/dissertation - Collect data, analyze, report - Discuss implications for full-scale study\nOption 3: Reanalysis of Published Study - Find published small-sample study with data available - Replicate original analysis - Conduct alternative analyses (e.g., Bayesian instead of frequentist) - Compare conclusions\nOption 4: Simulation Study - Investigate performance of method under small-sample conditions - Vary n, effect size, distributional assumptions - Summarize findings in mini-research paper\nGrading Rubric (200 points): - Proposal (20 pts) - Data collection/acquisition (30 pts) - Analysis rigor (60 pts) - Reporting transparency (40 pts) - Presentation (30 pts) - Peer review participation (20 pts)\n\n\n\n11.6.3 8.3 Flipped Classroom Implementation\nStructure: - Before Class: Students watch 20–30 min lecture video; complete reading quiz (5 questions) - In Class: Brief recap (10 min) → Lab/activity (60 min) → Discussion (20 min)\nAdvantages: - More hands-on time - Instructor circulates, provides individual help - Students learn by doing, not just listening\nChallenges: - Requires pre-recorded videos (time-intensive to create) - Students may not watch videos beforehand - Need accountability mechanism (quizzes, participation points)\nTechnology: - Use Panopto or Kaltura for video hosting - Embed quizzes in videos (e.g., via EdPuzzle) - Track completion via LMS analytics\n\n\n\n11.6.4 8.4 Collaborative Learning Activities\nThink-Pair-Share: - Pose question (e.g., “When would you use Fisher’s exact vs. chi-square?”) - Students think individually (2 min) - Pair up and discuss (3 min) - Share with class (5 min)\nJigsaw Groups: - Divide class into “home groups” (4 students each) - Assign each student a topic (e.g., MCAR, MAR, MNAR, MICE) - Students become “experts” on their topic (read, research) - Return to home group; teach each other\nPeer Review: - Students complete Assignment 2 (data analysis) - Exchange papers anonymously - Provide feedback using rubric - Revise based on feedback before final submission\nGallery Walk: - Students create posters of final projects - Display around room - Class circulates, leaves sticky-note comments - Author responds to questions",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#instructor-reflection-prompts",
    "href": "chapters/part-h-instructors-manual.html#instructor-reflection-prompts",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.7 9. INSTRUCTOR REFLECTION PROMPTS",
    "text": "11.7 9. INSTRUCTOR REFLECTION PROMPTS\nAfter teaching the course, consider these questions to improve future iterations:\n\nWhat concepts did students struggle with most? (Power analysis? Bayesian priors? MICE convergence?)\nWhich labs were most/least effective? (Did students finish on time? Were instructions clear?)\nDid students have adequate R skills? (Should prerequisite be strengthened? More R tutorials needed?)\nWere assessments well-calibrated? (Too easy? Too hard? Good spread of grades?)\nWhat would you change next time? (More examples? Different datasets? Reorder chapters?)\nDid students achieve learning outcomes? (Administer pre/post concept inventory)\nWhat feedback did students provide? (End-of-course evaluations)",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#ethical-considerations-in-teaching",
    "href": "chapters/part-h-instructors-manual.html#ethical-considerations-in-teaching",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.8 10. ETHICAL CONSIDERATIONS IN TEACHING",
    "text": "11.8 10. ETHICAL CONSIDERATIONS IN TEACHING\n\n11.8.1 10.1 Responsible Use of Small Samples\nTeaching Points: - Small samples are sometimes necessary, but researchers must be transparent about limitations - Avoid “p-hacking” (running multiple tests until one is significant) - Preregister analyses when possible (OSF, AsPredicted) - Report all analyses conducted, not just significant ones\nCase Study for Discussion: - Show example of published study with n = 12, p = 0.048, no effect size reported, no preregistration - Ask: What are the red flags? How could this study be improved?\n\n\n\n11.8.2 10.2 Diversity and Inclusion\nSmall-Sample Research in Underrepresented Populations: - Emphasize that small samples are common when studying marginalized groups (rare diseases, Indigenous communities, LGBTQ+ youth) - Discuss community-based participatory research (CBPR) and ethical data collection - Highlight researchers from diverse backgrounds\nInclusive Pedagogy: - Use diverse examples (not just biomedical; include education, social work, ecology) - Provide multiple modes of assessment (written, presentation, coding) - Offer flexible deadlines for students with caregiving responsibilities\n\n\n\n11.8.3 10.3 Open Science Practices\nEncourage: - Data sharing: Deposit anonymized data in repositories (OSF, Dataverse, Zenodo) - Code sharing: Provide reproducible scripts (RMarkdown, Jupyter Notebooks) - Preregistration: Template available at AsPredicted.org or OSF - Transparent reporting: Use CONSORT, STROBE checklists\nActivity: - Students create OSF project for final assignment; upload data, code, preregistration",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#conclusion",
    "href": "chapters/part-h-instructors-manual.html#conclusion",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.9 11. CONCLUSION",
    "text": "11.9 11. CONCLUSION\nTeaching small-sample methods requires balancing statistical rigor with practical constraints. Students must learn that:\n\nSmall samples are not inherently bad, but require appropriate methods and transparent reporting.\nP-values are insufficient; emphasize effect sizes, confidence intervals, and uncertainty quantification.\nContext matters: A well-designed study with n = 15 can be more valuable than a poorly designed study with n = 1,500.\n\nBy equipping students with exact tests, resampling methods, Bayesian tools, and robust reporting practices, we prepare them to conduct credible, ethical research even when large samples are infeasible.\nGood luck with your teaching! For questions or suggestions, contact the authors or visit the companion website.",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#appendix-a-solutions-to-worked-projects",
    "href": "chapters/part-h-instructors-manual.html#appendix-a-solutions-to-worked-projects",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.10 APPENDIX A: SOLUTIONS TO WORKED PROJECTS",
    "text": "11.10 APPENDIX A: SOLUTIONS TO WORKED PROJECTS\n\n11.10.1 Project 1: Employee Engagement\nResearch Question: Does a mindfulness intervention improve employee engagement scores?\nData: employee_engagement.csv (n = 25; pre/post design)\nFull Solution:\nlibrary(tidyverse)\nlibrary(effsize)\nlibrary(ggplot2)\n\n# Load data\ndf &lt;- read_csv(\"data/employee_engagement.csv\")\n\n# Descriptive statistics\nsummary(df)\n\n# Check normality\nshapiro.test(df$pre_score)   # p = 0.23 (normal)\nshapiro.test(df$post_score)  # p = 0.18 (normal)\n\n# Paired t-test\nt.test(df$post_score, df$pre_score, paired = TRUE)\n\n# Effect size (Cohen's d for paired samples)\ncohen.d(df$post_score, df$pre_score, paired = TRUE)\n\n# Visualization\ndf_long &lt;- df %&gt;%\n  pivot_longer(cols = c(pre_score, post_score), \n               names_to = \"time\", values_to = \"score\")\n\nggplot(df_long, aes(x = time, y = score)) +\n  geom_boxplot() +\n  geom_line(aes(group = employee_id), alpha = 0.3) +\n  labs(title = \"Employee Engagement: Pre vs. Post\",\n       y = \"Engagement Score\", x = \"\") +\n  theme_minimal()\nExpected Results: - t(24) = 3.45, p = 0.002 - d = 0.69 (95% CI: 0.25 to 1.13) - Conclusion: Significant moderate-to-large improvement in engagement.\nDiscussion Points: - Sample size adequate for d ≈ 0.70 (post-hoc power ≈ 75%) - No control group—cannot rule out placebo effects, time trends - Generalizability limited to this organization\n\n\n\n11.10.2 Project 2: A/B Test\nResearch Question: Does Email Version B increase click-through rate vs. Version A?\nData: mini_marketing.csv (n₁ = 12, n₂ = 14)\nFull Solution:\nlibrary(tidyverse)\n\n# Load data\ndf &lt;- read_csv(\"data/mini_marketing.csv\")\n\n# Create contingency table\ntable_data &lt;- table(df$version, df$clicked)\n\n# Fisher's exact test (small expected counts)\nfisher.test(table_data)\n\n# Effect size: Odds ratio (from Fisher output)\n# OR = 4.2 (95% CI: 0.9 to 25.3)\n\n# Visualization\ndf %&gt;%\n  group_by(version) %&gt;%\n  summarise(click_rate = mean(clicked)) %&gt;%\n  ggplot(aes(x = version, y = click_rate)) +\n  geom_col(fill = \"steelblue\") +\n  ylim(0, 1) +\n  labs(title = \"Click-Through Rate by Email Version\",\n       y = \"Proportion Clicked\", x = \"Version\") +\n  theme_minimal()\nExpected Results: - p = 0.08 (two-sided) - OR = 4.2 (very wide CI due to small n) - Conclusion: Suggestive evidence favoring Version B, but underpowered.\nDiscussion Points: - Recommend larger follow-up study (power analysis: need n ≈ 100 per group for 80% power) - Report CI to show uncertainty - Consider Bayesian approach with informative prior from past A/B tests\n\n\n\n11.10.3 Project 3: Hospital Readmissions\nResearch Question: Do hospitals differ in 30-day readmission rates?\nData: hospital_readmissions.csv (k = 15 hospitals, n = 10–30 per hospital)\nFull Solution:\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(broom.mixed)\n\n# Load data\ndf &lt;- read_csv(\"data/hospital_readmissions.csv\")\n\n# Calculate readmission rates by hospital\nrates &lt;- df %&gt;%\n  group_by(hospital_id) %&gt;%\n  summarise(\n    n_patients = n(),\n    n_readmit = sum(readmitted),\n    rate = mean(readmitted)\n  )\n\n# Visualization\nggplot(rates, aes(x = reorder(hospital_id, rate), y = rate)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = rate - 1.96*sqrt(rate*(1-rate)/n_patients),\n                    ymax = rate + 1.96*sqrt(rate*(1-rate)/n_patients)),\n                width = 0.2) +\n  coord_flip() +\n  labs(title = \"30-Day Readmission Rates by Hospital\",\n       y = \"Readmission Rate\", x = \"Hospital\") +\n  theme_minimal()\n\n# Hierarchical logistic regression\nmodel &lt;- glmer(readmitted ~ 1 + (1 | hospital_id), \n               data = df, family = binomial)\nsummary(model)\nExpected Results: - Readmission rates vary from 8% to 25% - Random intercept variance = 0.42 (significant heterogeneity) - Conclusion: Hospitals differ in readmission risk; further investigation needed.\nDiscussion Points: - Small sample per hospital → wide CIs - Hierarchical model pools information across hospitals (shrinkage) - Could add covariates (patient age, comorbidities) to explain variation",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-h-instructors-manual.html#appendix-b-sample-final-exam-take-home-48-hours",
    "href": "chapters/part-h-instructors-manual.html#appendix-b-sample-final-exam-take-home-48-hours",
    "title": "7  Part H: Instructor’s Manual",
    "section": "11.11 APPENDIX B: SAMPLE FINAL EXAM (Take-Home, 48 Hours)",
    "text": "11.11 APPENDIX B: SAMPLE FINAL EXAM (Take-Home, 48 Hours)\nInstructions: Answer all questions. Show all R code. Submit RMarkdown file + PDF.\n\n11.11.1 Question 1 (20 points): Conceptual Understanding\nA researcher plans a study with n = 20 participants. They want to detect a “medium” effect (d = 0.5) with 80% power at α = 0.05.\n\nCalculate the required sample size per group for a two-sample t-test. (5 pts)\n\nInterpret the result. Is n = 20 adequate? (5 pts)\n\nIf they proceed with n = 20, what is the achieved power? (5 pts)\n\nSuggest two alternative strategies if they cannot increase sample size. (5 pts)\n\nAnswer Key:\n# a) Required sample size\npwr::pwr.t.test(d = 0.5, power = 0.80, sig.level = 0.05, type = \"two.sample\")\n# n = 64 per group\n\n# b) No, n = 20 total (10 per group) is severely inadequate.\n\n# c) Achieved power with n = 10 per group\npwr::pwr.t.test(n = 10, d = 0.5, sig.level = 0.05, type = \"two.sample\")\n# Power = 0.18 (only 18% chance of detecting d = 0.5)\n\n# d) Alternatives:\n# 1. Target larger effect (focus on contexts where d &gt; 0.8)\n# 2. Use Bayesian methods with informative priors\n# 3. Accept lower power; report as pilot study\n\n\n\n11.11.2 Question 2 (30 points): Data Analysis\nAnalyze the provided dataset (quiz_final.csv) containing exam scores for two teaching methods (n₁ = 14, n₂ = 12).\n\nCheck assumptions (normality, homogeneity of variance). (10 pts)\n\nConduct an appropriate test. (10 pts)\n\nCompute effect size with CI. (5 pts)\n\nCreate a visualization. (5 pts)\n\nAnswer Key:\nlibrary(tidyverse)\nlibrary(car)\nlibrary(effectsize)\n\n# Load data\ndf &lt;- read_csv(\"quiz_final.csv\")\n\n# a) Check assumptions\n# Normality\ntapply(df$score, df$method, shapiro.test)\n# Method A: p = 0.08; Method B: p = 0.12 (both roughly normal)\n\n# Homogeneity of variance\nleveneTest(score ~ method, data = df)\n# p = 0.45 (variances equal)\n\n# b) Independent-samples t-test\nt.test(score ~ method, data = df, var.equal = TRUE)\n\n# c) Effect size\ncohens_d(score ~ method, data = df)\n\n# d) Visualization\nggplot(df, aes(x = method, y = score)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1, alpha = 0.5) +\n  labs(title = \"Exam Scores by Teaching Method\",\n       y = \"Score\", x = \"Method\") +\n  theme_minimal()\n\n\n\n11.11.3 Question 3 (25 points): Missing Data\nThe dataset quiz_missing.csv has 30% missing values on variable outcome.\n\nDiagnose missingness mechanism. (10 pts)\n\nImplement multiple imputation (m = 20). (10 pts)\n\nCompare complete-case vs. imputed results. (5 pts)\n\nAnswer Key:\nlibrary(mice)\nlibrary(naniar)\n\n# Load data\ndf &lt;- read_csv(\"quiz_missing.csv\")\n\n# a) Diagnose missingness\n# Visualize pattern\nvis_miss(df)\n\n# Test MCAR\nmcar_test(df)\n# p = 0.03 (reject MCAR; likely MAR)\n\n# b) Multiple imputation\nimp &lt;- mice(df, m = 20, method = \"pmm\", seed = 123, printFlag = FALSE)\nplot(imp)  # Check convergence\n\n# Fit model\nfit_imp &lt;- with(imp, lm(outcome ~ predictor))\npooled &lt;- pool(fit_imp)\nsummary(pooled)\n\n# c) Compare\n# Complete-case\nfit_cc &lt;- lm(outcome ~ predictor, data = df)\nsummary(fit_cc)\n\n# Comparison: Imputed SE smaller, coefficient similar\n\n\n\n11.11.4 Question 4 (25 points): Reporting\nWrite a Results section (300 words) reporting the analysis from Question 2. Include: - Descriptive statistics - Test results - Effect size with CI - Interpretation (avoid over-interpreting non-significant results if applicable)\nGrading Rubric: - APA format (5 pts) - Descriptive stats reported (5 pts) - Test results with df, t, p (5 pts) - Effect size and CI (5 pts) - Appropriate interpretation (5 pts)\n\nEND OF INSTRUCTOR’S MANUAL",
    "crumbs": [
      "Part H: Instructor's Manual",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part H: Instructor's Manual</span>"
    ]
  },
  {
    "objectID": "chapters/part-a-foundations.html",
    "href": "chapters/part-a-foundations.html",
    "title": "1  Part A: Foundations",
    "section": "",
    "text": "1.1 Chapter 1. Why Small-Sample Research Matters\nThis part establishes why small-sample research is important and how to frame research questions that align with limited data availability.",
    "crumbs": [
      "Part A: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part A: Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/part-a-foundations.html#chapter-1.-why-small-sample-research-matters",
    "href": "chapters/part-a-foundations.html#chapter-1.-why-small-sample-research-matters",
    "title": "1  Part A: Foundations",
    "section": "",
    "text": "1.1.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain why small samples are common in applied research settings - ✓ Identify practical, ethical, and logistical constraints that limit sample size - ✓ Understand how large-sample approximations fail with modest datasets - ✓ Recognize contexts where small-sample methods are necessary and appropriate\nPractical Skills - ✓ Calculate statistical power for different sample sizes - ✓ Create power curves to visualize sample size trade-offs - ✓ Distinguish between situations requiring small-sample vs large-sample methods\nCritical Evaluation - ✓ Assess when conventional parametric tests become unreliable - ✓ Evaluate the impact of outliers and assumption violations in small samples - ✓ Critique the “apologetic” framing of small-sample research\nApplication - ✓ Justify research decisions when large samples are infeasible - ✓ Select appropriate statistical methods given sample size constraints - ✓ Communicate small-sample research findings with appropriate caveats\n\n\n1.1.2 Why Small Samples Are Often Unavoidable\nMany textbooks assume that researchers can collect hundreds or thousands of observations. In practice, however, numerous research contexts yield small samples. Clinical studies of rare diseases, evaluations of pilot programmes, classroom-based educational interventions, community-based participatory research, and studies in Small Island Developing States (SIDS) often involve fewer than 100 participants. Resource constraints, logistical barriers, and ethical considerations (such as minimising burden on vulnerable populations) make small samples the norm rather than the exception.\nExamples of small-sample contexts:\n\nHealth sciences: Clinical trials for rare diseases (n &lt; 30), pilot studies testing feasibility before large RCTs, single-site hospital studies.\nEducation: Evaluating a new teaching method in a single classroom (n = 15-25), assessing specialized programs for gifted/special education students, teacher professional development studies.\nBusiness: A/B tests in niche markets, customer satisfaction surveys for small businesses, startup product testing with limited users.\nSocial sciences: Studies in remote communities, indigenous populations, or specialized occupational groups where the total accessible population is small.\n\nDespite their ubiquity, small samples are often treated as deficient or temporary. Researchers may apologise for limited data, or reviewers may demand larger samples without considering feasibility. This mindset overlooks the fact that many important questions can only be addressed with small datasets. Rather than apologising, researchers should select methods that are appropriate for the sample size at hand.\n\n\n1.1.3 When Large-Sample Approximations Fail\nClassical parametric tests (t-tests, ANOVA, standard logistic regression) rely on asymptotic theory. They assume that sampling distributions approximate normality as sample size increases. With small samples, these approximations can be inaccurate. P-values may be misleading, confidence intervals may have poor coverage, and maximum likelihood estimates may be unstable or even undefined (for example, in logistic regression with separation).\nSmall samples also amplify the impact of outliers and violations of distributional assumptions. A single extreme value can dominate a mean or distort a regression slope. Skewed or heavy-tailed distributions, which cause few problems in large samples, become serious concerns when n is small.\n\n\n1.1.4 Visualising Power Trade-offs\nEven modest reductions in sample size can have a dramatic impact on statistical power. The figure below uses the exact power.t.test() function to illustrate how power declines for medium-sized effects as per-group sample size drops from 60 to 10. Curves are shown for three standardised effect sizes (Cohen’s d = 0.3, 0.5, and 0.8).\n\nlibrary(tidyverse)\n\neffect_sizes &lt;- c(0.3, 0.5, 0.8)\nn_values &lt;- seq(10, 60, by = 5)\nalpha &lt;- 0.05\n\npower_grid &lt;- crossing(n = n_values, d = effect_sizes) %&gt;%\n  mutate(power = map2_dbl(\n    n,\n    d,\n    ~ power.t.test(\n      n = .x,\n      delta = .y,\n      sd = 1,\n      sig.level = alpha,\n      type = \"two.sample\",\n      alternative = \"two.sided\"\n    )$power\n  ))\n\nggplot(power_grid, aes(x = n, y = power, colour = factor(d))) +\n  geom_line(linewidth = 1) +\n  geom_point() +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", colour = \"grey40\") +\n  scale_colour_brewer(palette = \"Set1\", name = \"Effect size (d)\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    x = \"Sample size per group\",\n    y = \"Power\",\n    title = \"Power drops steeply as per-group sample size decreases\",\n    subtitle = \"Dashed line marks the conventional 80% power threshold\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\nPower curves illustrating sensitivity to sample size.\n\n\n\n\nInterpretation: With medium effects (d ≈ 0.5), power slips below 50% once per-group sample size falls beneath about 20 participants. Detecting smaller effects (d ≈ 0.3) would require many more observations than are typically feasible in small-sample settings. This visual reinforces the need to report minimum detectable effects and to focus on estimation rather than binary significance testing when n is limited.\n\n\n1.1.5 Appropriate Methods for Small Samples\nFortunately, a suite of exact, resampling-based, and robust methods can provide valid inferences with limited data. Exact tests (such as Fisher’s exact test, exact binomial tests, and exact Poisson tests) compute p-values directly from the combinatorial distribution of the data, without relying on asymptotic approximations. Resampling methods (bootstrap and permutation tests) use the observed data to approximate the sampling distribution, often yielding more accurate inferences than large-sample formulas.\nNonparametric rank-based tests (Mann–Whitney U, Wilcoxon signed-rank, Kruskal–Wallis) make fewer distributional assumptions and are less sensitive to outliers. Penalised regression (Firth logistic regression, ridge, LASSO) can stabilise coefficient estimates when events are sparse. Bayesian methods incorporate prior information and quantify uncertainty through posterior distributions, which remain well-defined even when data are limited.\n\n\n1.1.6 Example: Comparing Two Small Groups\nSuppose we wish to compare customer satisfaction scores (on a 1–10 scale) between two service branches, each with only 12 observations. The scores are ordinal and may not be normally distributed.\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\nset.seed(2025)\nbranch_a &lt;- c(7, 8, 6, 7, 9, 8, 7, 6, 8, 7, 9, 8)\nbranch_b &lt;- c(5, 6, 7, 5, 6, 5, 7, 6, 5, 6, 7, 6)\n\ndata_satisfaction &lt;- tibble(\n  score = c(branch_a, branch_b),\n  branch = rep(c(\"A\", \"B\"), each = 12)\n)\n\n# Mann–Whitney U test (Wilcoxon rank-sum)\nresult &lt;- wilcox_test(data_satisfaction, score ~ branch, detailed = TRUE)\nprint(result)\n\n# A tibble: 1 × 12\n  estimate .y.   group1 group2    n1    n2 statistic       p conf.low conf.high\n*    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     2.00 score A      B         12    12       127 0.00119     1.00      2.00\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nThe Mann–Whitney U test compares the distributions of the two groups without assuming normality. The p-value indicates whether the observed difference in ranks is unlikely under the null hypothesis of identical distributions. Because the test is based on ranks, it is robust to skewness and outliers.\nInterpretation: If the p-value is small (typically p &lt; 0.05), we have evidence that customer satisfaction differs between the two branches. The effect size (such as rank-biserial correlation) quantifies the magnitude of the difference.\n\n\n1.1.7 Key Takeaways\n\nSmall samples are common and legitimate in many research contexts, particularly in SIDS, clinical studies, and community-based research.\nLarge-sample approximations can fail when n is small, leading to inaccurate p-values and confidence intervals.\nExact tests, resampling methods, and rank-based procedures provide valid inferences without requiring large samples.\nThe choice of method should match the research question, the type of outcome, and the sample size actually available.\n\n\n\n\n1.1.8 Self-Assessment Quiz\nTest your understanding of the key concepts from Chapter 1. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. A study with n=12 per group has 25% power to detect d=0.5. What does this mean?\nA. There is a 25% chance the treatment is effective\nB. If the true effect is d=0.5, there is a 25% probability of detecting it (p&lt;0.05)\nC. The Type I error rate is 25%\nD. 25% of participants will show the effect\n\nQ2. Why might large-sample approximations fail with n=15?\nA. Computers cannot process small datasets\nB. Sampling distributions may not be approximately normal\nC. Effect sizes cannot be calculated\nD. P-values are always incorrect\n\nQ3. Which research question is MOST appropriate for n=20?\nA. “What are all factors that predict customer loyalty?” (testing 15 predictors)\nB. “Is there a difference in satisfaction between two service approaches?”\nC. “How do age, gender, income, education, and occupation interact to predict outcomes?”\nD. “Can we build a machine learning model to predict customer behavior?”\n\nQ4. A pilot study with n=8 finds a mean difference of 5 points (95% CI: [-2, 12], p=0.14). The correct interpretation is:\nA. There is no effect\nB. The effect is exactly 5 points\nC. The study is underpowered; effects from -2 to 12 points are plausible\nD. The null hypothesis is proven true\n\nQ5. Which outcome measure provides the MOST statistical information per observation?\nA. Binary (pass/fail)\nB. Ordinal (grade A-F)\nC. Continuous (test score 0-100)\nD. All provide equal information\n\nQ6. With n=10 per group, which statement about power is TRUE?\nA. Power is always 50% regardless of effect size\nB. Power increases as the true effect size increases\nC. Power is unrelated to sample size\nD. Power cannot be calculated for small samples\n\nQ7. A study finds p=0.048 with n=8 per group. Which concern is MOST valid?\nA. The result is definitely a false positive\nB. With small n, results near the significance threshold should be interpreted cautiously\nC. Small samples always produce spurious results\nD. The p-value is meaningless with n&lt;30\n\nQ8. When is a small sample (n&lt;30) potentially SUFFICIENT?\nA. Never—all research requires n≥100\nB. When the effect is very large and variance is low\nC. Only for qualitative research\nD. When using machine learning methods\n\nQ9. Which is a legitimate reason for small sample size?\nA. The researcher is lazy\nB. The population is rare (e.g., a genetic disorder affecting 1 in 100,000)\nC. The researcher wants to save time\nD. Small samples are always preferable\n\nQ10. A researcher states: “My study has n=15, so I’ll just use nonparametric tests.” What is the problem with this reasoning?\nA. Nonparametric tests require n≥30\nB. The choice of test should depend on the data characteristics and research question, not just sample size\nC. Nonparametric tests are always inferior\nD. Parametric tests always work regardless of assumptions\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: Statistical power is the probability of correctly rejecting a false null hypothesis when a specific effect size exists. With 25% power, there is a 75% chance of a Type II error (failing to detect a real effect of d=0.5). This concept is directly illustrated in the power curve figure in this chapter, which shows how power declines as sample size decreases.\nQ2. Answer: B\nExplanation: The Central Limit Theorem requires sufficient sample size for sampling distributions to approximate normality. With n=15, especially if data are skewed or have outliers, parametric test assumptions may be violated. This is why the chapter emphasizes that “large-sample approximations can fail when n is small, leading to inaccurate p-values and confidence intervals.”\nQ3. Answer: B\nExplanation: Focused, binary comparisons are feasible with small samples. Complex multivariate questions (A, C, D) require much larger samples to avoid overfitting and ensure stable estimates. The chapter states: “focused questions about a single outcome or a few key comparisons can often be addressed with modest samples.”\nQ4. Answer: C\nExplanation: The wide confidence interval reflects substantial uncertainty. The study cannot rule out small negative effects (-2) or large positive effects (12). Non-significance with small n indicates insufficient evidence, not absence of effect. This aligns with the chapter’s emphasis on focusing “on estimation rather than binary significance testing when n is limited.”\nQ5. Answer: C\nExplanation: Continuous measures preserve all variation in the data. Dichotomizing or coarsening into categories discards information, reduces statistical power, and limits the ability to detect effects. This principle underlies the recommendation to select outcome measures carefully when working with small samples.\nQ6. Answer: B\nExplanation: Statistical power increases with larger effect sizes, larger sample sizes, and lower variance. Even with n=10, a very large effect (d=1.5) might have adequate power, while a small effect (d=0.2) would not. This is demonstrated in the power curve plot showing different effect sizes (d=0.3, 0.5, 0.8).\nQ7. Answer: B\nExplanation: P-values near cutoffs (0.05) are highly variable with small samples. A slight change in data or analysis could flip the result. Emphasis should be on effect size magnitude and confidence intervals, not borderline p-values. The chapter warns that “small samples amplify the impact of outliers and violations of distributional assumptions.”\nQ8. Answer: B\nExplanation: If the true effect is very large (e.g., d=2.0) and within-group variability is small, even modest samples can provide clear evidence. Examples: a new drug that doubles survival rates, or a teaching method that improves scores by 30 points. The chapter acknowledges that some questions “can only be addressed with small datasets.”\nQ9. Answer: B\nExplanation: Rare populations, pilot studies, ethical constraints (minimizing burden on vulnerable groups), and resource limitations in SIDS contexts are all legitimate reasons for small samples. The chapter explicitly mentions “clinical studies of rare diseases” as one context where “small samples are the norm rather than the exception.”\nQ10. Answer: B\nExplanation: Test selection should consider: outcome type (continuous, ordinal, binary), distributional properties (normality, skewness), and research question. Small n is ONE consideration, but not the sole criterion. The chapter states: “The choice of method should match the research question, the type of outcome, and the sample size actually available.”\n\n\n\n\n\n\n1.1.9 Smoke Test\n\n# Re-run a simple Mann–Whitney test to verify code\nlibrary(rstatix)\nset.seed(2025)\nx &lt;- c(7, 8, 6, 7, 9, 8)\ny &lt;- c(5, 6, 7, 5, 6, 5)\ntest_data &lt;- tibble(value = c(x, y), group = rep(c(\"X\", \"Y\"), each = 6))\nwilcox_test(test_data, value ~ group)\n\n# A tibble: 1 × 7\n  .y.   group1 group2    n1    n2 statistic      p\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 value X      Y          6     6        33 0.0175",
    "crumbs": [
      "Part A: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part A: Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/part-a-foundations.html#chapter-2.-questions-and-outcomes-that-fit-small-n",
    "href": "chapters/part-a-foundations.html#chapter-2.-questions-and-outcomes-that-fit-small-n",
    "title": "1  Part A: Foundations",
    "section": "1.2 Chapter 2. Questions and Outcomes that Fit Small n",
    "text": "1.2 Chapter 2. Questions and Outcomes that Fit Small n\n\n1.2.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Distinguish between exploratory and confirmatory research aims - ✓ Understand when to prioritize effect size estimation over hypothesis testing - ✓ Recognize which research questions are realistic for small samples - ✓ Explain the relationship between outcome measurement and sample size requirements\nPractical Skills - ✓ Formulate focused research questions appropriate for limited data - ✓ Select outcome measures that yield interpretable results with small n - ✓ Design studies that maximize information from modest samples - ✓ Calculate minimum detectable effects for planned sample sizes\nCritical Evaluation - ✓ Assess whether complex multivariate questions are feasible with available data - ✓ Evaluate trade-offs between breadth (many variables) and depth (focused questions) - ✓ Critique overly ambitious research designs given sample constraints\nApplication - ✓ Design pilot studies with clear, answerable questions - ✓ Choose between continuous, ordinal, and binary outcome measures - ✓ Justify research scope and question framing in proposals and manuscripts\n\n\n1.2.2 Framing Realistic Research Questions\nNot all research questions are equally suited to small samples. Broad, multivariate questions (such as identifying dozens of predictors or testing complex mediation models) typically require large datasets. In contrast, focused questions about a single outcome or a few key comparisons can often be addressed with modest samples.\nWhen planning a small-sample study, prioritise clarity and specificity. Instead of asking “What are all the factors that influence patient adherence?” ask “Does a brief reminder intervention improve adherence compared to standard care?” The latter question is binary, focused, and testable with a small randomised trial.\nSimilarly, consider whether the study is exploratory or confirmatory. Exploratory studies generate hypotheses, describe patterns, and refine measurement instruments. They do not require large samples, but results should be interpreted cautiously and replicated before drawing firm conclusions. Confirmatory studies test prespecified hypotheses and require sufficient statistical power. With small samples, power is limited, so confirmatory aims should be modest and well-justified.\n\n\n1.2.3 Choosing Appropriate Outcomes\nThe type of outcome variable influences which methods are feasible and how much information can be extracted from limited data. Binary outcomes (yes/no, success/failure) are common but carry less information per observation than continuous or ordinal measures. If your sample is small, consider whether a continuous or ordinal outcome might capture more variation and yield more precise inferences.\nFor example, rather than dichotomising patient improvement into “improved” versus “not improved”, use a continuous measure of symptom severity or an ordinal scale with several levels. This preserves information and increases statistical efficiency. However, if the outcome is inherently binary (such as survival within 30 days), do not force it into a continuous form.\nCount outcomes (number of adverse events, number of customer complaints) are also informative but may be sparse when samples are small. Exact Poisson tests and negative binomial models can handle low counts, but very sparse data (many zeros, few events) may require careful interpretation or resampling methods.\n\n1.2.3.1 Outcome Selection Decision Guide\n\nStart with the construct of interest. Can it be measured on a genuine numeric scale (e.g., duration, dosage, test score)?\n\n\nYes → Prefer a continuous measure. Report means/medians with confidence intervals; consider transformations if the scale is skewed.\nNo/uncertain → Proceed to step 2.\n\n\nCan respondents make ordered distinctions beyond “yes/no”?\n\n\nYes → Use an ordinal scale with 4–7 categories. Analyse with rank-based or ordinal models; report medians or cumulative odds ratios.\nNo → Retain a binary outcome. Use exact or penalised methods and emphasise risk differences or odds ratios with wide intervals.\n\n\nIs the event a count that can exceed one per subject?\n\n\nYes → Model counts directly (Poisson, negative binomial, or exact tests). Report event rates and their uncertainty.\n\n\nDocument the rationale. Explain why the chosen outcome scale is the most informative and feasible given participant burden, measurement error, and sample size.\n\n\n\n\n1.2.4 Effect Sizes and Estimation\nIn small-sample research, point estimates of effect sizes (differences in means, odds ratios, correlation coefficients) are often more useful than p-values alone. A small sample may lack power to detect a meaningful effect, but the estimated effect size and its confidence interval indicate the likely magnitude and precision of the effect.\nWhen reporting results, emphasise effect sizes and uncertainty intervals. For example, “The median difference in satisfaction scores was 1.5 points (95% CI: 0.5 to 2.5)” is more informative than “The difference was statistically significant (p = 0.03)”. Effect size estimates help readers judge practical importance and facilitate meta-analysis or future sample size planning.\n\n\n1.2.5 Example: Outcome Selection in a Pilot Study\nSuppose you are evaluating a pilot training programme with 18 participants. You have two outcome options: (1) binary pass/fail on a final assessment, or (2) a continuous score (0–100) on the same assessment.\n\nlibrary(tidyverse)\n\nset.seed(2025)\nn &lt;- 18\n\n# Simulate continuous scores\nscores &lt;- round(rnorm(n, mean = 68, sd = 12))\nscores &lt;- pmax(0, pmin(100, scores))  # clamp to [0, 100]\n\n# Create binary outcome: pass if score &gt;= 60\npass_fail &lt;- ifelse(scores &gt;= 60, \"Pass\", \"Fail\")\n\ndata_pilot &lt;- tibble(\n  participant = 1:n,\n  score = scores,\n  outcome = pass_fail\n)\n\n# Summary statistics for continuous outcome\nsummary(data_pilot$score)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   47.0    66.0    70.0    69.9    75.8    83.0 \n\n# Frequency table for binary outcome\ntable(data_pilot$outcome)\n\n\nFail Pass \n   1   17 \n\n# The continuous outcome carries more information\n# We can estimate a mean and standard error:\nmean_score &lt;- mean(data_pilot$score)\nse_score &lt;- sd(data_pilot$score) / sqrt(n)\nci_lower &lt;- mean_score - 1.96 * se_score\nci_upper &lt;- mean_score + 1.96 * se_score\n\ncat(\"Mean score:\", round(mean_score, 1), \"\\n\")\n\nMean score: 69.9 \n\ncat(\"95% CI: [\", round(ci_lower, 1), \",\", round(ci_upper, 1), \"]\\n\", sep = \"\")\n\n95% CI: [66,73.8]\n\n\nBy retaining the continuous score, we obtain a precise estimate of average performance with a confidence interval. Had we dichotomised into pass/fail, we would only know that 14 out of 18 passed, which provides less information about the central tendency and spread of performance.\nInterpretation: The continuous outcome allows us to estimate the mean score with reasonable precision. The confidence interval indicates the range of plausible population means. If the goal is to understand typical performance (not just pass rates), the continuous measure is more informative.\n\n\n1.2.6 Research Design Considerations\nSmall-sample studies benefit from tight experimental control. Paired or matched designs (before–after, crossover, matched-pair comparisons) reduce variability by comparing each unit to itself or a closely matched control. This within-unit comparison can yield precise inferences even when the number of units is small.\nStratification and blocking can also improve efficiency by accounting for known sources of variation. For example, if you are comparing two teaching methods in a small class, stratify by prior achievement level to reduce heterogeneity within each comparison.\nFinally, consider sequential or adaptive designs if feasible. Rather than committing to a fixed sample size in advance, you might plan an interim analysis and decide whether to stop early (if results are clear) or continue (if uncertainty remains). Bayesian methods are well-suited to adaptive designs, as they naturally update beliefs as data accumulate.\n\n\n1.2.7 Designing Pilot Studies\nPilot studies serve specific purposes: assessing feasibility (recruitment rates, attrition, protocol adherence), refining measurement instruments, and estimating variability to inform future sample size calculations. With very small n (often 10–30 participants), focus on collecting process metrics and precision estimates rather than hypothesis testing. Report:\n\nPrimary feasibility outcomes (e.g., proportion screened who consent, time to complete assessments).\nPreliminary effect estimates with wide confidence intervals, making clear that they are exploratory.\nAdaptations for the main study, especially where procedures proved onerous or data quality issues emerged.\n\nGuidance: choose a pilot sample large enough to detect major logistical problems (often 12–20 per arm is sufficient), prespecify success criteria (such as acceptable recruitment rate), and plan in advance how you will decide whether to proceed to a full trial.\n\n\n1.2.8 Key Takeaways\n\nFrame research questions narrowly and realistically given the sample size constraints.\nDistinguish between exploratory (hypothesis-generating) and confirmatory (hypothesis-testing) aims.\nPrefer continuous or ordinal outcomes over binary outcomes when possible, to maximise information per observation.\nReport effect sizes and confidence intervals, not just p-values, to convey magnitude and precision.\nUse paired, matched, or stratified designs to reduce variability and improve efficiency.\nConsider adaptive or sequential designs if ethically and practically feasible.\n\n\n\n\n1.2.9 Self-Assessment Quiz\nTest your understanding of the key concepts from Chapter 2. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. Which research question is better suited to small samples?\nA. “What is the relationship between 20 personality traits and job performance?”\nB. “Does a brief mindfulness intervention reduce test anxiety compared to control?”\nC. “Can we predict customer churn using all available behavioral data?”\nD. “How do socioeconomic factors interact to predict health outcomes?”\n\nQ2. An exploratory study (n=20) finds that meditation reduces anxiety (p=0.04, d=0.7). How should this be framed?\nA. “Meditation is proven effective”\nB. “Preliminary evidence suggests meditation may reduce anxiety; replication needed”\nC. “No conclusions can be drawn from n=20”\nD. “The effect is definitely due to chance”\n\nQ3. A researcher dichotomizes a continuous outcome (0-100 scale) into “high” (≥70) vs “low” (&lt;70). With n=25, what is the consequence?\nA. Power increases because binary outcomes are simpler\nB. Power decreases because information is discarded\nC. No effect on statistical power\nD. Analysis becomes impossible\n\nQ4. A study aims to detect a “small” effect (d=0.2) with 80% power. Approximately how many participants per group are needed?\nA. n=20 per group\nB. n=50 per group\nC. n=200 per group\nD. n=500 per group\n\nQ5. Which statement about pilot studies is CORRECT?\nA. Pilot studies should always test hypotheses\nB. Pilot studies assess feasibility and refine procedures\nC. Pilot studies require the same sample size as main studies\nD. Pilot studies never provide useful effect size estimates\n\nQ6. A researcher plans a study with n=15 per group but calculates they need n=50 per group for 80% power. What should they do?\nA. Proceed with n=15 and interpret p-values cautiously\nB. Reframe the study as exploratory/pilot\nC. Report minimum detectable effect (MDE) given n=15\nD. All of the above\n\nQ7. Which outcome is LEAST appropriate for n=20?\nA. Binary outcome (success/failure)\nB. Ordinal outcome (1-7 Likert scale)\nC. Continuous outcome (0-100 scale)\nD. 50-item questionnaire with subscale factor analysis\n\nQ8. A study comparing two teaching methods (n=12 per class) finds no significant difference (p=0.18, d=0.45). The conclusion should be:\nA. “The two methods are equally effective”\nB. “The study found no evidence of a difference, but was underpowered to detect medium effects”\nC. “Teaching method has no effect on learning”\nD. “The null hypothesis is confirmed”\n\nQ9. When choosing between a paired and independent-groups design with small samples, which is generally preferable?\nA. Always use independent groups—pairing is only for large samples\nB. Paired designs reduce within-subject variability and increase power\nC. The choice makes no difference statistically\nD. Paired designs require larger samples than independent designs\n\nQ10. A pilot study with n=18 yields a mean difference of 5 points (95% CI: [0.2, 9.8]). What is the appropriate next step?\nA. Conclude the intervention is effective and implement widely\nB. Use this estimate to plan a fully-powered confirmatory study\nC. Abandon the research because the sample was too small\nD. Report only the p-value and ignore the confidence interval\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: Focused, binary comparisons with a single primary outcome are feasible with small samples. Multivariate questions (A, C, D) require large samples to estimate many parameters reliably. The chapter emphasizes: “focused questions about a single outcome or a few key comparisons can often be addressed with modest samples.”\nQ2. Answer: B\nExplanation: Exploratory studies with small samples generate hypotheses but require replication. Results should be framed as preliminary, with acknowledgment of limited power and potential for Type I error. As stated in the chapter: “Exploratory studies generate hypotheses, describe patterns… They do not require large samples, but results should be interpreted cautiously and replicated before drawing firm conclusions.”\nQ3. Answer: B\nExplanation: Dichotomizing continuous variables discards information about the magnitude of differences, reduces statistical power, and can create spurious findings at arbitrary cut-points. The chapter clearly states: “rather than dichotomising patient improvement into ‘improved’ versus ‘not improved’, use a continuous measure… This preserves information and increases statistical efficiency.”\nQ4. Answer: C\nExplanation: Detecting small effects requires large samples. For d=0.2 with 80% power and α=0.05 (two-tailed), approximately n=393 per group is needed. With small samples, only large effects (d≥0.8) can be reliably detected. This aligns with the power curve from Chapter 1 showing that detecting d=0.3 effects is beyond reach of small samples.\nQ5. Answer: B\nExplanation: Pilot studies (typically n=10-30) assess feasibility (recruitment rates, protocol adherence, measurement properties), refine procedures, and provide preliminary effect size estimates for sample size planning—but are not powered for definitive hypothesis testing. The chapter’s “Designing Pilot Studies” section explicitly states: “focus on collecting process metrics and precision estimates rather than hypothesis testing.”\nQ6. Answer: D\nExplanation: When desired sample size is infeasible: (1) proceed but interpret with appropriate caution, (2) frame as exploratory, (3) report what effect sizes CAN be detected (MDE), (4) emphasize effect size estimation over hypothesis testing. This integrates guidance from both the “Framing Realistic Research Questions” and “Effect Sizes and Estimation” sections.\nQ7. Answer: D\nExplanation: Factor analysis requires n≥100-200 (ideally 5-10 observations per item). With n=20 and 50 items, factor analysis would be completely unreliable. This reflects general principles about matching analysis complexity to sample size discussed throughout the chapter.\nQ8. Answer: B\nExplanation: Non-significance with small samples indicates insufficient evidence, not proof of no effect. A medium effect (d=0.45) is plausible given the CI, but the study lacked power to detect it definitively. The chapter emphasizes: “A small sample may lack power to detect a meaningful effect, but the estimated effect size and its confidence interval indicate the likely magnitude and precision of the effect.”\nQ9. Answer: B\nExplanation: Paired designs reduce within-subject variability and increase power. The chapter’s “Research Design Considerations” section states: “Paired or matched designs (before–after, crossover, matched-pair comparisons) reduce variability by comparing each unit to itself or a closely matched control. This within-unit comparison can yield precise inferences even when the number of units is small.”\nQ10. Answer: B\nExplanation: Use this estimate to plan a fully-powered confirmatory study. Pilot studies provide preliminary effect estimates and variability information needed for sample size planning. The “Designing Pilot Studies” section recommends reporting “Preliminary effect estimates with wide confidence intervals, making clear that they are exploratory” and using pilots to “estimating variability to inform future sample size calculations.”\n\n\n\n\n\n\n1.2.10 Smoke Test\n\n# Re-run outcome comparison example\nset.seed(2025)\nscores_test &lt;- round(rnorm(10, mean = 70, sd = 10))\nmean(scores_test)\n\n[1] 73.6\n\nsd(scores_test)\n\n[1] 5.103",
    "crumbs": [
      "Part A: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part A: Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/part-a-foundations.html#summary-of-part-a",
    "href": "chapters/part-a-foundations.html#summary-of-part-a",
    "title": "1  Part A: Foundations",
    "section": "1.3 Summary of Part A",
    "text": "1.3 Summary of Part A\nIn Part A, we established that small-sample research is both common and legitimate. We reviewed why large-sample approximations can fail with limited data and introduced exact, resampling, and robust methods as alternatives. We also discussed how to formulate research questions and select outcomes that are realistic and informative given small sample sizes. The next part will address data collection and preparation strategies tailored to small studies.",
    "crumbs": [
      "Part A: Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part A: Foundations</span>"
    ]
  },
  {
    "objectID": "chapters/part-b-data-collection.html",
    "href": "chapters/part-b-data-collection.html",
    "title": "2  Part B: Data Collection and Preparation",
    "section": "",
    "text": "2.1 Chapter 9. Sampling Strategies for Small Studies\nThis part addresses practical challenges in collecting and preparing data for small-sample studies. We cover sampling strategies that maximise information with limited resources, measurement quality and scale development, data screening and diagnostic checks, and handling missing data transparently.",
    "crumbs": [
      "Part B: Data Collection and Preparation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part B: Data Collection and Preparation</span>"
    ]
  },
  {
    "objectID": "chapters/part-b-data-collection.html#chapter-9.-sampling-strategies-for-small-studies",
    "href": "chapters/part-b-data-collection.html#chapter-9.-sampling-strategies-for-small-studies",
    "title": "2  Part B: Data Collection and Preparation",
    "section": "",
    "text": "2.1.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the trade-offs between probability and purposive sampling - ✓ Understand the relationship between sample size, power, and detectable effects - ✓ Recognize when small samples are sufficient vs. inadequate - ✓ Distinguish between sampling for generalizability vs. mechanism testing\nPractical Skills - ✓ Select appropriate sampling methods given resource and population constraints - ✓ Calculate minimum detectable effects for planned sample sizes - ✓ Implement stratified sampling to improve precision with small n - ✓ Design sequential and adaptive sampling strategies\nCritical Evaluation - ✓ Assess the tension between ideal and feasible sample sizes - ✓ Evaluate whether purposive sampling is appropriate for research aims - ✓ Critique sampling justifications in published small-sample studies\nApplication - ✓ Justify sample sizes transparently in research proposals - ✓ Design sampling plans that maximize information with limited resources - ✓ Report sampling procedures and achieved samples with appropriate caveats\n\n\n2.1.2 The Tension Between Ideal and Feasible Sample Sizes\nMost power analysis guides assume that researchers can achieve conventionally adequate sample sizes (n ≥ 30 per group for t-tests, 10–15 events per predictor for regression). In practice, resource constraints, rare populations, and ethical considerations often make these targets unattainable. Rather than abandoning research in such contexts, we should adopt methods suited to smaller samples and report findings with appropriate caveats.\nTransparent reporting of sampling rationale, achieved sample size, and power or precision estimates helps readers judge the strength of evidence. Researchers should distinguish between studies designed to test specific hypotheses (which require adequate power) and exploratory studies that generate hypotheses or provide preliminary effect estimates (which can proceed with modest samples).\n\n\n2.1.3 Probability Sampling with Small Samples\nProbability sampling (simple random sampling, stratified sampling, cluster sampling) ensures that every unit has a known, non-zero probability of selection. This supports generalisation to the target population and enables design-based inference. However, probability sampling requires a sampling frame and may be logistically complex or expensive.\nWith small samples, probability sampling can still be valuable, but estimates will have wide confidence intervals. Stratified sampling (dividing the population into strata and sampling proportionally or disproportionally from each) can improve precision by ensuring representation of key subgroups.\nWhen to use: Accessible sampling frame, desire for generalisability, resources permit random selection, even if total sample size is modest.\n\n\n2.1.4 Sequential and Adaptive Sampling\nWhen recruitment is costly or uncertain, sequential designs allow researchers to review interim results and decide whether to continue sampling. For example, you might pre-specify that recruitment will proceed in waves of five participants, stopping early if credible intervals for the primary outcome are sufficiently narrow or if feasibility metrics (e.g., consent rates) fall below thresholds. Adaptive sampling can also target underrepresented strata after an initial wave, improving balance without committing to a large upfront sample. Key principles:\n\nSet decision rules in advance. Define stopping boundaries for efficacy, futility, or feasibility to avoid ad hoc choices.\nMaintain error control. Use exact tests, Bayesian posterior probabilities, or alpha-spending functions appropriate for small n.\nDocument adaptations transparently. Report how the sampling plan evolved, including any changes to recruitment targets or strata weights.\n\nSequential or response-adaptive sampling is especially valuable in rare populations, where pausing after each wave prevents over-committing resources if early data already provide actionable evidence.\n\n\n2.1.5 Example: Stratified Sampling Calculation\nSuppose we are surveying employees in a small organisation with 120 total staff: 60 in Department A, 40 in Department B, 20 in Department C. We can afford to survey 30 employees. Proportional stratified sampling ensures each department is represented in proportion to its size.\n\nlibrary(tidyverse)\n\n# Population strata\nstrata &lt;- tibble(\n  Department = c(\"A\", \"B\", \"C\"),\n  Population_N = c(60, 40, 20),\n  Proportion = Population_N / sum(Population_N)\n)\n\n# Total sample size\ntotal_sample &lt;- 30\n\n# Allocate sample proportionally\nstrata &lt;- strata %&gt;%\n  mutate(\n    Sample_n = round(Proportion * total_sample),\n    Sampling_Fraction = Sample_n / Population_N\n  )\n\nprint(strata)\n\n# A tibble: 3 × 5\n  Department Population_N Proportion Sample_n Sampling_Fraction\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n1 A                    60      0.5         15              0.25\n2 B                    40      0.333       10              0.25\n3 C                    20      0.167        5              0.25\n\ncat(\"\\nTotal sample allocated:\", sum(strata$Sample_n), \"\\n\")\n\n\nTotal sample allocated: 30 \n\n\nInterpretation: Proportional allocation ensures that each department contributes to the sample in proportion to its population size. Department A, being the largest, provides 15 respondents; Department C, the smallest, provides 5. This approach yields unbiased estimates for the overall population. If precision for small strata is a concern, disproportionate allocation (oversampling small strata) can be used, though this requires weighting in analysis.\n\n\n2.1.6 Purposive and Convenience Sampling\nPurposive (judgmental) sampling selects units based on researcher judgement of their informativeness or representativeness. Convenience sampling selects units that are easily accessible. Neither method supports probabilistic generalisation, but both are common in small-sample research where probability sampling is infeasible.\nFindings from purposive or convenience samples should be interpreted cautiously and presented as preliminary or context-specific. Replication in independent samples strengthens confidence.\nWhen to use: No sampling frame available, exploratory research, pilot studies, rare or hard-to-reach populations, tight resource constraints.\n\n\n2.1.7 Quota Sampling\nQuota sampling (a form of purposive sampling) selects units to match known population characteristics (such as age, gender, or occupation distribution). It mimics stratified sampling but without random selection within strata. Quota sampling can improve representativeness compared to convenience sampling, though it remains non-probabilistic.\nWhen to use: Known population characteristics to match, desire for balanced sample composition, probability sampling infeasible.\n\n\n2.1.8 Power and Precision with Small Samples\nStatistical power is the probability of detecting a true effect of a given size. With small samples, power is limited, meaning that even if a meaningful effect exists, the study may fail to detect it (high Type II error rate). Researchers should conduct power analyses before data collection to understand what effects are detectable given sample size constraints.\nIf the achieved sample size is smaller than desired, report the minimum detectable effect (MDE): the smallest effect the study can detect with specified power (typically 80%) and alpha (typically 0.05). This helps readers judge whether the study could have detected effects of practical importance.\n\n\n2.1.9 Finite Population Correction\nWhen sampling without replacement from a small, known population, the variance of estimates decreases because each sampled unit reduces remaining uncertainty. The finite population correction (FPC) adjusts the required sample size accordingly. If a power analysis suggests 30 participants are needed assuming an infinite population, but the accessible population is only 120 people, the adjusted sample size is smaller:\n\n# Finite population correction example\nn_required_infinite &lt;- 30  # From power analysis\nN_population &lt;- 120        # Size of accessible population\n\nn_adjusted &lt;- n_required_infinite /\n  (1 + (n_required_infinite - 1) / N_population)\n\nn_adjusted\n\n[1] 24.16\n\n\nInterpretation: Sampling without replacement from 120 individuals means that a sample of roughly 24 (instead of 30) achieves the same precision. Always report whether you applied the FPC so readers can replicate the calculation.\n\n\n2.1.10 Example: Power Calculation for a Small Study\nWe plan a study comparing two groups with n = 12 per group. We compute power to detect a medium effect size (Cohen’s d = 0.5) using a two-sample t-test.\n\n# Power calculation using pwr package (if available)\n# If not installed, use approximations or manual calculation\nif (requireNamespace(\"pwr\", quietly = TRUE)) {\n  library(pwr)\n  \n  power_result &lt;- pwr.t.test(n = 12, d = 0.5, sig.level = 0.05, \n                              type = \"two.sample\", alternative = \"two.sided\")\n  print(power_result)\n  \n  cat(\"\\nWith n = 12 per group, power to detect d = 0.5 is:\", \n      round(power_result$power, 2), \"\\n\")\n  \n  # What effect size is detectable with 80% power?\n  mde_result &lt;- pwr.t.test(n = 12, power = 0.80, sig.level = 0.05,\n                           type = \"two.sample\", alternative = \"two.sided\")\n  cat(\"Minimum detectable effect (80% power):\", round(mde_result$d, 2), \"\\n\")\n} else {\n  cat(\"Install 'pwr' package to run power calculations.\\n\")\n}\n\n\n     Two-sample t test power calculation \n\n              n = 12\n              d = 0.5\n      sig.level = 0.05\n          power = 0.2161\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nWith n = 12 per group, power to detect d = 0.5 is: 0.22 \nMinimum detectable effect (80% power): 1.2 \n\n\nInterpretation: With 12 participants per group, power to detect a medium effect (d = 0.5) is modest (approximately 30–40%). To achieve 80% power, we would need to detect a larger effect (d ≈ 1.2, a very large effect). This illustrates the limitation of small samples for hypothesis testing. If the true effect is small or medium, the study is underpowered. Researchers should acknowledge this limitation and interpret non-significant results cautiously (absence of evidence is not evidence of absence).\n\n\n2.1.11 Sample Size Planning Workflow\nIntegrating power analysis into a broader planning conversation prevents unrealistic promises and surfaces design trade-offs early. Use the following workflow whenever you scope a small-sample study:\n\nClarify the question and estimand. What parameter (difference in means, odds ratio, correlation) must the study estimate?\nSpecify tolerable uncertainty. Define the minimum detectable effect or target confidence-interval width that would make the study actionable.\nMap constraints. Document recruitment limits, budget, timeline, and ethical restrictions (e.g., maximum patient burden).\nSelect design and analysis. Choose the test/model, decide on one- vs two-sided inference, and note planned covariates or repeated measures.\nCompute required n. Use analytical power formulas, simulation, or resampling as appropriate; apply finite-population corrections if sampling without replacement.\nAssess feasibility. Compare required n to constraints. If infeasible, adjust expectations (e.g., shift to estimation focus, reduce assurance level, adopt sequential design).\nDocument decisions. Record assumptions, software/code used, and any compromises for transparency.\n\n\n\n\n\n\nflowchart TD\n  Q[Define research question & estimand] --&gt; U[Specify target effect or CI width]\n  U --&gt; C[Document recruitment, budget, ethical constraints]\n  C --&gt; D[Choose design & analysis plan]\n  D --&gt; N[Compute required sample size / MDE]\n  N --&gt; F{Feasible within constraints?}\n  F -- Yes --&gt; T[Lock plan & preregister]\n  F -- No --&gt; A[Adjust goals: revise estimand, adopt sequential design, or reframe as exploratory]\n  A --&gt; C\n  T --&gt; R[Record assumptions & share in protocol]\n\n\n\n\n\n\nThis loop makes trade-offs explicit: if the required sample size exceeds what is feasible, researchers can justify an exploratory framing, add interim analyses, or negotiate for additional resources before data collection begins.\n\n\n2.1.12 Justifying Small Sample Sizes\nWhen sample sizes are constrained, justify them transparently:\n\nState the target population and accessible population.\nDescribe sampling method and rationale.\nReport planned and achieved sample sizes.\nProvide power or precision estimates (confidence interval widths).\nAcknowledge limitations and interpret findings accordingly.\nFrame the study as exploratory or preliminary if appropriate.\n\n\n\n2.1.13 Sample Size Planning Flowchart\n\n\n\n\n\nflowchart TD\n  A[Define research question] --&gt; B[Specify primary outcome and test]\n  B --&gt; C[Determine minimally important effect]\n  C --&gt; D{Effect size from...}\n  D --&gt;|Pilot data| E[Use observed effect]\n  D --&gt;|Literature| F[Use meta-analytic estimate]\n  D --&gt;|Stakeholder| G[Use practical threshold]\n  E --&gt; H[Conduct power analysis]\n  F --&gt; H\n  G --&gt; H\n  H --&gt; I{Achieve n for 80% power?}\n  I --&gt;|Yes| J[Proceed with confirmatory study]\n  I --&gt;|No| K[Consider alternatives]\n  K --&gt; L[Paired/within design?]\n  K --&gt; M[More sensitive outcome?]\n  K --&gt; N[Continuous vs. binary?]\n  K --&gt; O[Bayesian with priors?]\n  K --&gt; P[Reframe as exploratory pilot]\n  L --&gt; Q[Recalculate power]\n  M --&gt; Q\n  N --&gt; Q\n  O --&gt; Q\n  P --&gt; R[Document limitations]\n  J --&gt; S[Pre-register plan]\n  Q --&gt; I\n\n\n\n\n\n\nInterpretation: This flowchart guides researchers through sample size planning, showing decision points and alternatives when the target sample size is not feasible.\n\n\n\n2.1.14 Self-Assessment Quiz\nTest your understanding of sampling strategies from Chapter 9. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. A researcher uses a “rule of thumb” of n=30 per group for all studies. What is the primary problem with this approach?\nA. n=30 is always too small\nB. Sample size should depend on effect size, power, and research question—not arbitrary rules\nC. n=30 is always too large\nD. Rules of thumb are always correct\n\nQ2. Stratified sampling is most useful when:\nA. The population is homogeneous\nB. You want to ensure representation of key subgroups that differ on the outcome\nC. Random selection is impossible\nD. Sample size exceeds 1,000\n\nQ3. Power analysis reveals you need n=50 per group, but only n=20 is feasible. What should you do?\nA. Abandon the study\nB. Proceed, but report the study as exploratory/pilot and calculate minimum detectable effect (MDE)\nC. Proceed and claim the same statistical power\nD. Ignore power entirely\n\nQ4. Which sampling method allows probabilistic generalization to a target population?\nA. Convenience sampling\nB. Purposive sampling\nC. Simple random sampling\nD. Snowball sampling\n\nQ5. Quota sampling differs from stratified sampling in that:\nA. It uses random selection within strata\nB. It matches population proportions but does not use random selection\nC. It requires a sampling frame\nD. It is always more accurate\n\nQ6. A study with n=15 per group has 30% power to detect d=0.5. The researcher should report:\nA. “The study was adequately powered”\nB. “The study was underpowered to detect medium effects; only large effects (d≥1.0) could be reliably detected”\nC. “Power is irrelevant with small samples”\nD. “Non-significant results prove no effect exists”\n\nQ7. The finite population correction (FPC) is relevant when:\nA. Sampling with replacement from an infinite population\nB. Sampling without replacement from a small, known population (e.g., N=100)\nC. Sample size exceeds population size\nD. Using convenience sampling\n\nQ8. Sequential sampling allows researchers to:\nA. Collect all data simultaneously\nB. Stop early if interim results show sufficient precision or evidence\nC. Ignore power analysis\nD. Change hypotheses after seeing data\n\nQ9. A convenience sample from one university is used to test a new teaching method. Which statement is TRUE?\nA. Results generalize to all universities\nB. Results are context-specific and require replication\nC. Convenience sampling is never acceptable\nD. Results are as valid as random sampling\n\nQ10. Minimum Detectable Effect (MDE) refers to:\nA. The smallest effect that exists in the population\nB. The smallest effect the study can detect with specified power (e.g., 80%)\nC. The p-value threshold\nD. The confidence interval width\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: Sample size should depend on effect size, power, and research question—not arbitrary rules. A small effect requires larger n; a large effect can be detected with smaller n. The chapter emphasizes: “Rather than abandoning research in such contexts, we should adopt methods suited to smaller samples and report findings with appropriate caveats.”\nQ2. Answer: B\nExplanation: Stratified sampling divides the population into strata and ensures each stratum is represented. This improves precision when strata differ on the outcome. The chapter states: “Stratified sampling (dividing the population into strata and sampling proportionally or disproportionally from each) can improve precision by ensuring representation of key subgroups.”\nQ3. Answer: B\nExplanation: Proceed, but report the study as exploratory/pilot and calculate minimum detectable effect (MDE). Transparency about power limitations is essential. The chapter recommends: “If the achieved sample size is smaller than desired, report the minimum detectable effect (MDE).”\nQ4. Answer: C\nExplanation: Simple random sampling (and other probability sampling methods) ensures every unit has a known, non-zero probability of selection, supporting generalization. The chapter states: “Probability sampling…ensures that every unit has a known, non-zero probability of selection. This supports generalisation to the target population.”\nQ5. Answer: B\nExplanation: Quota sampling matches population proportions but does not use random selection within strata. It “mimics stratified sampling but without random selection within strata.” This makes it non-probabilistic.\nQ6. Answer: B\nExplanation: With 30% power for d=0.5, the study can only reliably detect large effects (d≥1.0, which has ~80% power with n=15). The chapter emphasizes transparent reporting: “Researchers should conduct power analyses before data collection to understand what effects are detectable.”\nQ7. Answer: B\nExplanation: FPC adjusts required sample size when sampling without replacement from a small, finite population. The chapter explains: “When sampling without replacement from a small, known population, the variance of estimates decreases…The finite population correction (FPC) adjusts the required sample size accordingly.”\nQ8. Answer: B\nExplanation: Sequential sampling allows stopping early based on pre-specified decision rules if interim results show sufficient precision or evidence. The chapter describes: “sequential designs allow researchers to review interim results and decide whether to continue sampling.”\nQ9. Answer: B\nExplanation: Convenience samples are context-specific and require replication. The chapter states: “Findings from purposive or convenience samples should be interpreted cautiously and presented as preliminary or context-specific. Replication in independent samples strengthens confidence.”\nQ10. Answer: B\nExplanation: MDE is the smallest effect the study can detect with specified power (typically 80%) and alpha (typically 0.05). The chapter defines it: “the minimum detectable effect (MDE): the smallest effect the study can detect with specified power.”\n\n\n\n\n\n\n2.1.15 Key Takeaways\n\nProbability sampling supports generalisation but may be infeasible with small samples or rare populations.\nStratified sampling can improve precision by ensuring representation of key subgroups.\nPurposive and convenience sampling are common in small-sample research but limit generalisability.\nQuota sampling balances sample composition without requiring random selection.\nPower analyses reveal what effects are detectable given sample size; small samples have limited power for detecting small or medium effects.\nTransparent reporting of sampling methods, achieved sample sizes, and power or precision estimates is essential for interpreting small-sample findings.\n\n\n\n2.1.16 Smoke Test\n\n# Re-run stratified allocation\ndepartments &lt;- c(60, 40, 20)\ntotal_n &lt;- 30\nallocation &lt;- round((departments / sum(departments)) * total_n)\nprint(allocation)\n\n[1] 15 10  5",
    "crumbs": [
      "Part B: Data Collection and Preparation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part B: Data Collection and Preparation</span>"
    ]
  },
  {
    "objectID": "chapters/part-b-data-collection.html#chapter-10.-measurement-quality-and-scale-development",
    "href": "chapters/part-b-data-collection.html#chapter-10.-measurement-quality-and-scale-development",
    "title": "2  Part B: Data Collection and Preparation",
    "section": "2.2 Chapter 10. Measurement Quality and Scale Development",
    "text": "2.2 Chapter 10. Measurement Quality and Scale Development\n\n2.2.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the distinctions between content, construct, and criterion validity - ✓ Understand reliability theory and sources of measurement error - ✓ Recognize the limitations of psychometric analyses with small samples - ✓ Distinguish quantitative vs. qualitative approaches to scale validation\nPractical Skills - ✓ Pilot test scales and collect qualitative feedback with small samples - ✓ Compute item-level statistics (means, SDs, correlations) in R - ✓ Calculate internal consistency (Cronbach’s α) and interpret appropriately - ✓ Identify problematic items using discrimination and ceiling/floor effects\nCritical Evaluation - ✓ Assess when sample sizes are adequate for factor analysis vs. when to defer - ✓ Evaluate the trade-off between brief scales (fewer items) and reliability - ✓ Critique whether items have content validity for the intended construct\nApplication - ✓ Design iterative scale refinement processes with limited samples - ✓ Report scale properties transparently (including small-sample limitations) - ✓ Prioritize qualitative feedback over unstable quantitative indices when appropriate\n\n\n2.2.2 The Challenge of Measurement in Small Studies\nMany small-sample studies rely on brief, custom-developed measurement instruments. Standard scale development protocols (large pilot studies, factor analysis, item response theory) require hundreds of observations. With small samples, researchers must balance the need for reliable, valid measurement with practical constraints.\nShort scales (3–5 items) can be internally consistent and valid if items are carefully chosen. Pilot testing with qualitative feedback (cognitive interviews, think-aloud protocols) can identify ambiguous wording, response biases, and cultural appropriateness. Quantitative pilot data (even with n ≈ 20–30) can reveal extreme floor or ceiling effects, items with no variance, and obvious inconsistencies.\n\n\n2.2.3 Content and Face Validity\nContent validity refers to whether items comprehensively and appropriately represent the construct being measured. Face validity refers to whether items appear relevant and appropriate to respondents. Both are assessed through expert review and respondent feedback, not statistical tests.\nWhen to assess: During scale development, before quantitative pilot testing. Involves domain experts and representatives of the target population.\n\n2.2.3.1 Content Validity Ratio (CVR)\nLawshe’s Content Validity Ratio provides a simple index of expert agreement on whether an item is “essential” to a construct. With N experts and Ne rating an item as essential, the CVR is:\n[ = ]\nCVR ranges from −1 to +1. Positive values indicate majority agreement that the item is essential; thresholds depend on the number of experts (e.g., ≥0.75 when N = 8). Use CVR alongside qualitative feedback to decide which items to retain.\n\n# CVR example: 8 experts, 6 judge the item essential\nn_experts &lt;- 8\nn_essential &lt;- 6\ncvr &lt;- (n_essential - n_experts / 2) / (n_experts / 2)\ncvr\n\n[1] 0.5\n\n\nInterpretation: A CVR of 0.50 indicates that 75% of experts deemed the item essential. Consult published critical values to confirm whether the item meets the desired level of agreement for your expert panel size.\n\n\n\n2.2.4 Steps for Scale Development with Small Samples\n\nDefine the construct clearly: What are you measuring? What are its dimensions or facets?\nGenerate candidate items: Write more items than needed (e.g., 10–15 items for a final 5-item scale).\nExpert review: Ask domain experts to rate item relevance, clarity, and representativeness.\nCognitive interviews: Ask a few respondents (n = 5–10) to complete the scale and think aloud, explaining their interpretations and any confusion.\nQuantitative pilot: Administer the scale to a small sample (n = 20–40) and compute item statistics.\nItem analysis: Identify problematic items (low variance, weak correlations with total score, ceiling/floor effects).\nRefine and re-test: Remove or revise problematic items and retest if resources permit.\n\n\n\n2.2.5 Example: Item Analysis for a Pilot Scale\nWe pilot a 5-item job satisfaction scale with n = 25 employees. Each item uses a 1–7 Likert response.\n\nlibrary(tidyverse)\nlibrary(psych)\n\nset.seed(2025)\n\n# Simulated pilot data: 25 respondents, 5 items\npilot_data &lt;- tibble(\n  respondent = 1:25,\n  item1 = sample(3:7, 25, replace = TRUE, prob = c(0.1, 0.2, 0.3, 0.25, 0.15)),\n  item2 = sample(2:7, 25, replace = TRUE, prob = c(0.1, 0.15, 0.25, 0.25, 0.15, 0.1)),\n  item3 = sample(4:7, 25, replace = TRUE, prob = c(0.2, 0.3, 0.3, 0.2)),  # restricted range\n  item4 = sample(1:7, 25, replace = TRUE),\n  item5 = sample(2:7, 25, replace = TRUE, prob = c(0.15, 0.2, 0.25, 0.2, 0.15, 0.05))\n)\n\n# Item descriptive statistics\nitem_stats &lt;- pilot_data %&gt;%\n  select(starts_with(\"item\")) %&gt;%\n  summarise(across(everything(), list(\n    mean = mean,\n    sd = sd,\n    min = min,\n    max = max\n  ))) %&gt;%\n  pivot_longer(everything(), names_to = c(\"item\", \".value\"), names_sep = \"_\")\n\nprint(item_stats)\n\n# A tibble: 5 × 5\n  item   mean    sd   min   max\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 item1  5.52 1.16      3     7\n2 item2  4.44 1.66      2     7\n3 item3  5.72 0.980     4     7\n4 item4  4.2  1.96      1     7\n5 item5  3.8  1.55      2     6\n\n# Inter-item correlations\nitems_only &lt;- select(pilot_data, starts_with(\"item\"))\ncor_matrix &lt;- cor(items_only)\nprint(round(cor_matrix, 2))\n\n      item1 item2 item3 item4 item5\nitem1  1.00  0.24 -0.20 -0.14  0.15\nitem2  0.24  1.00 -0.05 -0.08  0.12\nitem3 -0.20 -0.05  1.00  0.07  0.04\nitem4 -0.14 -0.08  0.07  1.00 -0.19\nitem5  0.15  0.12  0.04 -0.19  1.00\n\n# Item-total correlations (corrected for item overlap)\nitem_total &lt;- psych::alpha(items_only)$item.stats\n\nSome items ( item3 item4 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\nprint(item_total)\n\n       n  raw.r  std.r   r.cor   r.drop mean     sd\nitem1 25 0.4048 0.4768  0.3271  0.05850 5.52 1.1590\nitem2 25 0.5818 0.5538  0.5443  0.09640 4.44 1.6603\nitem3 25 0.2675 0.3919 -0.1461 -0.03047 5.72 0.9798\nitem4 25 0.4359 0.2981 -0.5152 -0.17220 4.20 1.9579\nitem5 25 0.4824 0.5039  0.3391  0.01298 3.80 1.5546\n\n\nInterpretation: Examine item means and standard deviations. Items with very high or low means and small SDs may have ceiling or floor effects (most respondents give the same response). Item 3 has a restricted range (4–7), which may indicate a ceiling effect. Inter-item correlations should be positive and moderate (0.3–0.7). Very low correlations suggest an item does not measure the same construct; very high correlations suggest redundancy. The corrected item-total correlation (r.drop) indicates how well each item correlates with the total score excluding itself. Values below 0.3 suggest weak items that could be removed.\n\n\n2.2.6 Identifying Problematic Items\n\nLow variance: If an item has very small SD (e.g., &lt; 1.0 on a 1–7 scale), most respondents are giving the same answer. The item may be too extreme, too obvious, or poorly worded.\nWeak item-total correlation: Items with corrected item-total correlations below 0.3 do not discriminate well and may be measuring something different.\nFloor or ceiling effects: If most responses cluster at the low or high end, the item cannot differentiate among respondents.\nNegative correlations: If an item correlates negatively with the total or with other items, it may be reverse-coded incorrectly or measuring an opposite construct.\n\n\n\n2.2.7 Refining the Scale\nBased on item analysis, revise or remove problematic items. For example, if Item 3 shows a ceiling effect and Item 4 has weak item-total correlation, consider removing them. Compute alpha for the revised scale.\n\n# Revised scale: remove item3 and item4\nrevised_items &lt;- select(pilot_data, item1, item2, item5)\nalpha_revised &lt;- psych::alpha(revised_items)\nprint(alpha_revised)\n\n\nReliability analysis   \nCall: psych::alpha(x = revised_items)\n\n  raw_alpha std.alpha G6(smc) average_r  S/N  ase mean   sd median_r\n      0.36      0.38     0.3      0.17 0.62 0.22  4.6 0.98     0.15\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    -0.24  0.36  0.70\nDuhachek -0.06  0.36  0.79\n\n Reliability if an item is dropped:\n      raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r med.r\nitem1      0.21      0.21    0.12      0.12 0.26     0.32    NA  0.12\nitem2      0.26      0.26    0.15      0.15 0.36     0.29    NA  0.15\nitem5      0.37      0.39    0.24      0.24 0.65     0.23    NA  0.24\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nitem1 25  0.61  0.70  0.43   0.27  5.5 1.2\nitem2 25  0.72  0.68  0.39   0.22  4.4 1.7\nitem5 25  0.66  0.63  0.27   0.16  3.8 1.6\n\nNon missing response frequency for each item\n         2    3    4    5    6    7 miss\nitem1 0.00 0.04 0.20 0.16 0.40 0.20    0\nitem2 0.20 0.08 0.20 0.24 0.16 0.12    0\nitem5 0.36 0.00 0.32 0.12 0.20 0.00    0\n\ncat(\"Cronbach's alpha for revised 3-item scale:\", round(alpha_revised$total$raw_alpha, 3), \"\\n\")\n\nCronbach's alpha for revised 3-item scale: 0.364 \n\n\nInterpretation: The revised scale may have higher alpha if problematic items are removed. However, removing items also reduces scale length, which can lower alpha. The goal is a balance: retain enough items for adequate reliability, but remove items that degrade validity or add no information.\n\n\n2.2.8 Qualitative Feedback and Cognitive Interviews\nWith very small samples (n &lt; 20), quantitative item analysis is unreliable. Qualitative methods (cognitive interviews, focus groups) are more informative. Ask respondents:\n\nWhat does each item mean to you?\nWere any items confusing, ambiguous, or difficult to answer?\nAre the response options appropriate?\nAre any items culturally inappropriate or offensive?\n\nThis feedback can prevent major problems before larger-scale data collection.\n\n\n2.2.9 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 10 Questions\n\n\n\nQ1. What is the primary advantage of using qualitative methods (cognitive interviews) over quantitative methods when pilot testing scales with very small samples (n &lt; 20)?\n\nCognitive interviews provide more statistical power\n\nQualitative feedback can identify ambiguous wording and cultural issues without requiring statistical reliability\n\nQuantitative item analysis is too expensive\n\nCognitive interviews automatically calculate Cronbach’s alpha\n\n\nQ2. In Lawshe’s Content Validity Ratio (CVR), if 8 experts are consulted and 6 judge an item as “essential,” what is the CVR?\n\n0.25\n\n0.50\n\n0.75\n\n1.00\n\n\nQ3. What does a “ceiling effect” in item analysis indicate?\n\nMost respondents give the lowest possible response\n\nMost respondents give the highest possible response\n\nThe item has perfect reliability\n\nThe item correlates negatively with the total score\n\n\nQ4. Which corrected item-total correlation threshold typically indicates that an item discriminates poorly and should be considered for removal?\n\nAbove 0.7\n\nBetween 0.3 and 0.7\n\nBelow 0.3\n\nExactly 0.5\n\n\nQ5. What does content validity assess?\n\nWhether items comprehensively and appropriately represent the construct being measured\n\nWhether the scale has high Cronbach’s alpha\n\nWhether factor analysis confirms a one-dimensional structure\n\nWhether the scale predicts future behavior\n\n\nQ6. Why might short scales (3–5 items) have lower Cronbach’s alpha than longer scales, even if items are well-chosen?\n\nShort scales always measure different constructs\n\nCronbach’s alpha is mathematically influenced by the number of items—fewer items tend to yield lower alpha\n\nShort scales cannot be reliable\n\nRespondents don’t take short scales seriously\n\n\nQ7. In pilot testing with small samples (n ≈ 20–30), what is the primary limitation of conducting factor analysis?\n\nFactor analysis requires specialized software\n\nFactor analysis requires hundreds of observations for stable results; small samples yield unstable loadings\n\nFactor analysis only works with 7-point Likert scales\n\nFactor analysis cannot handle missing data\n\n\nQ8. If an item correlates negatively with the total score and with other items, what is the most likely explanation?\n\nThe item has high content validity\n\nThe item may be reverse-coded incorrectly or measuring an opposite construct\n\nThe sample size is too large\n\nThe item has a ceiling effect\n\n\nQ9. What is the primary purpose of asking domain experts to rate item relevance and clarity during scale development?\n\nTo calculate test-retest reliability\n\nTo establish content validity by ensuring items appropriately represent the construct\n\nTo compute inter-item correlations\n\nTo determine the optimal sample size for the study\n\n\nQ10. In the example of the 5-item job satisfaction scale, Item 3 had a restricted range (responses only from 4–7 on a 1–7 scale). What does this suggest?\n\nItem 3 has perfect reliability\n\nItem 3 may have a ceiling effect, limiting its ability to differentiate among respondents\n\nItem 3 should be kept because high scores are desirable\n\nThe sample size should be increased to 1,000\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “With very small samples (n &lt; 20), quantitative item analysis is unreliable. Qualitative methods (cognitive interviews, focus groups) are more informative.”\nCognitive interviews reveal ambiguous wording and cultural issues without requiring the large samples needed for statistical reliability indices.\nA2. B) CVR = (6 - 8/2) / (8/2) = (6 - 4) / 4 = 2/4 = 0.50.\n“A CVR of 0.50 indicates that 75% of experts deemed the item essential.”\nA3. B) “If most responses cluster at the low or high end, the item cannot differentiate among respondents.”\nA ceiling effect occurs when most respondents give the highest possible response, limiting discrimination.\nA4. C) “The corrected item-total correlation (r.drop) indicates how well each item correlates with the total score excluding itself. Values below 0.3 suggest weak items that could be removed.”\nItems with r.drop &lt; 0.3 discriminate poorly and are candidates for removal.\nA5. A) “Content validity refers to whether items comprehensively and appropriately represent the construct being measured.”\nContent validity is assessed through expert review, not statistical tests.\nA6. B) “Removing items also reduces scale length, which can lower alpha.”\nCronbach’s alpha is mathematically influenced by the number of items—shorter scales tend to have lower alpha even if items are equally good.\nA7. B) “Standard scale development protocols (large pilot studies, factor analysis, item response theory) require hundreds of observations.”\nFactor analysis requires large samples (typically 200+) for stable results; small samples yield unreliable factor loadings.\nA8. B) “If an item correlates negatively with the total or with other items, it may be reverse-coded incorrectly or measuring an opposite construct.”\nNegative correlations suggest coding errors or conceptual misalignment with the scale.\nA9. B) “Content validity refers to whether items comprehensively and appropriately represent the construct being measured… assessed through expert review and respondent feedback.”\nExpert ratings establish content validity by confirming items represent the construct appropriately.\nA10. B) “Item 3 has a restricted range (4–7), which may indicate a ceiling effect.”\nRestricted ranges (especially at the high end) indicate ceiling effects that limit the item’s ability to differentiate among respondents.\n\n\n\n\n\n2.2.10 Key Takeaways\n\nMeasurement quality is critical in small-sample research; unreliable measures reduce power and bias estimates.\nContent and face validity are assessed through expert review and respondent feedback, not statistics.\nPilot testing with small samples (n ≈ 20–40) can identify problematic items through item-level descriptive statistics and inter-item correlations.\nItems with low variance, weak item-total correlations, or ceiling/floor effects should be revised or removed.\nQualitative methods (cognitive interviews) are valuable when quantitative sample sizes are too small for psychometric analysis.\nIterative refinement and re-testing improve scale quality, even when resources are limited.\n\n\n\n2.2.11 Smoke Test\n\n# Re-run item statistics\nset.seed(2025)\nitems_test &lt;- data.frame(\n  i1 = sample(1:5, 15, replace = TRUE),\n  i2 = sample(1:5, 15, replace = TRUE),\n  i3 = sample(1:5, 15, replace = TRUE)\n)\ncor(items_test)\n\n       i1     i2     i3\ni1 1.0000 0.2929 0.1743\ni2 0.2929 1.0000 0.2581\ni3 0.1743 0.2581 1.0000",
    "crumbs": [
      "Part B: Data Collection and Preparation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part B: Data Collection and Preparation</span>"
    ]
  },
  {
    "objectID": "chapters/part-b-data-collection.html#chapter-11.-data-screening-and-diagnostic-checks",
    "href": "chapters/part-b-data-collection.html#chapter-11.-data-screening-and-diagnostic-checks",
    "title": "2  Part B: Data Collection and Preparation",
    "section": "2.3 Chapter 11. Data Screening and Diagnostic Checks",
    "text": "2.3 Chapter 11. Data Screening and Diagnostic Checks\n\n2.3.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain how small samples amplify the impact of outliers and anomalies - ✓ Understand univariate vs. multivariate outlier detection approaches - ✓ Recognize the assumptions underlying common diagnostic tests - ✓ Distinguish data entry errors from legitimate extreme values\nPractical Skills - ✓ Detect outliers using z-scores, boxplots, and Mahalanobis distance in R - ✓ Assess normality with Q-Q plots, Shapiro-Wilk tests, and visual diagnostics - ✓ Check linearity and homoscedasticity in regression models - ✓ Calculate leverage and Cook’s D to identify influential observations\nCritical Evaluation - ✓ Assess when outlier removal is justified vs. when it constitutes manipulation - ✓ Evaluate the trade-off between retaining data vs. meeting assumptions - ✓ Critique data screening decisions in published small-sample studies\nApplication - ✓ Document data cleaning decisions transparently with decision rules - ✓ Choose between robust methods, transformations, or outlier exclusion - ✓ Report sensitivity analyses showing results with/without outliers\n\n\n2.3.2 Why Data Screening Matters More with Small Samples\nA single outlier can dominate a mean, distort a correlation, or violate regression assumptions when samples are small. Data entry errors (typos, misplaced decimals, incorrect codes) are harder to detect with fewer observations. Distributional assumptions (normality, homoscedasticity) are harder to verify with small samples, yet violations have greater consequences.\nSystematic data screening before analysis helps identify problems early. Document all cleaning and transformation decisions in a reproducible script. Report descriptive statistics, missingness patterns, and any deviations from planned analyses.\n\n\n2.3.3 Detecting Outliers\nOutliers are observations that are unusually large or small relative to the rest of the data. They may represent legitimate extreme values, data entry errors, or individuals from a different population. With small samples, outliers can have disproportionate influence on results.\nMethods for detecting outliers: - Visual inspection: boxplots, histograms, scatterplots. - Numerical criteria: values beyond 1.5 × IQR from the quartiles (Tukey’s fences), or standardised scores (z-scores) beyond ±3. - Influence diagnostics: Cook’s distance, leverage in regression.\nWhen to remove outliers: Only if there is clear evidence of data entry error or that the observation does not belong to the target population. Document the rationale and report results with and without outliers.\n\n2.3.3.1 Multivariate Outliers with Mahalanobis Distance\nUnivariate rules may miss cases that are unusual only when variables are considered jointly. Mahalanobis distance measures how far an observation lies from the multivariate centre, accounting for covariances among variables. Distances can be compared to a chi-square threshold with degrees of freedom equal to the number of variables.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated bivariate data with one multivariate outlier\nmulti_data &lt;- tibble(\n  satisfaction = rnorm(14, mean = 6, sd = 1),\n  wait_time = rnorm(14, mean = 10, sd = 2)\n) %&gt;%\n  bind_rows(tibble(satisfaction = 2, wait_time = 18))  # potential outlier\n\ncenter &lt;- colMeans(multi_data)\ncov_mat &lt;- cov(multi_data)\n\nmulti_data &lt;- multi_data %&gt;%\n  mutate(\n    mahal = mahalanobis(., center, cov_mat),\n    flag = mahal &gt; qchisq(0.975, df = ncol(multi_data))\n  )\n\nmulti_data\n\n# A tibble: 15 × 4\n   satisfaction wait_time   mahal flag \n          &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;\n 1         6.62     12.1   0.838  FALSE\n 2         6.04      9.61  0.240  FALSE\n 3         6.77      9.90  0.517  FALSE\n 4         7.27     10.8   1.44   FALSE\n 5         6.37      7.37  1.51   FALSE\n 6         5.84     14.9   1.93   FALSE\n 7         6.40      9.51  0.311  FALSE\n 8         5.92     11.5   0.0320 FALSE\n 9         5.66     15.7   2.68   FALSE\n10         6.70     11.3   0.625  FALSE\n11         5.60      9.77  0.375  FALSE\n12         4.24      7.40  5.54   FALSE\n13         5.58      8.16  1.45   FALSE\n14         6.76     10.1   0.503  FALSE\n15         2        18    10.0    TRUE \n\n\nInterpretation: Observations with Mahalanobis distance exceeding the chi-square cutoff (95% or 97.5% quantile) are flagged as multivariate outliers. Inspect the flagged cases individually to determine whether they reflect data errors, rare but valid combinations, or participants from a different subpopulation. Report how thresholds were chosen, as small samples make covariance estimates noisy.\n\n\n\n2.3.4 Example: Outlier Detection with Boxplots and Z-Scores\nWe examine a small dataset of customer wait times (n = 20) and check for outliers.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated wait times (most between 5–15 minutes, one outlier at 45)\nwait_times &lt;- c(7, 9, 8, 11, 10, 12, 8, 9, 10, 11, 13, 9, 10, 12, 8, 11, 10, 9, 45, 10)\n\n# Create the data frame\nwait_data &lt;- tibble(observation = 1:20, wait_time = wait_times)\n\n# Boxplot\nprint(\n  ggplot(wait_data, aes(y = wait_time)) +\n    geom_boxplot(fill = \"lightblue\") +\n    labs(title = \"Boxplot of Wait Times\", y = \"Wait Time (minutes)\") +\n    theme_minimal()\n)\n\n\n\n\nBoxplot highlighting a potential customer wait-time outlier.\n\n\n\n# Identify outliers using 1.5*IQR rule\nQ1 &lt;- quantile(wait_times, 0.25)\nQ3 &lt;- quantile(wait_times, 0.75)\nIQR_val &lt;- IQR(wait_times)\nlower_fence &lt;- Q1 - 1.5 * IQR_val\nupper_fence &lt;- Q3 + 1.5 * IQR_val\n\n# Filter to find outliers\noutliers &lt;- wait_data %&gt;%\n  dplyr::filter(wait_time &lt; lower_fence | wait_time &gt; upper_fence)\n\nprint(\"Outliers (1.5*IQR rule):\")\n\n[1] \"Outliers (1.5*IQR rule):\"\n\nprint(outliers)\n\n# A tibble: 1 × 2\n  observation wait_time\n        &lt;int&gt;     &lt;dbl&gt;\n1          19        45\n\n# Z-scores for outlier identification\nwait_data &lt;- wait_data %&gt;%\n  mutate(z_score = (wait_time - mean(wait_time)) / sd(wait_time))\n\n# Filter to find extreme values (absolute z-score &gt; 3)\nextreme &lt;- wait_data %&gt;%\n  dplyr::filter(abs(z_score) &gt; 3)\n\nprint(\"Extreme values (Z-score &gt; 3):\")\n\n[1] \"Extreme values (Z-score &gt; 3):\"\n\nprint(extreme)\n\n# A tibble: 1 × 3\n  observation wait_time z_score\n        &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1          19        45    4.17\n\n\nInterpretation: The boxplot visually flags observation 19 (wait time = 45 minutes) as an outlier. The IQR-based rule and z-score criterion both identify this observation. Before removing it, investigate: Is this a data entry error? Could a customer have genuinely waited 45 minutes due to an unusual circumstance? If it is an error, remove it. If it is genuine but atypical, consider reporting results with and without the outlier, or use robust methods (median, rank-based tests) that are less sensitive to extremes.\n\n\n2.3.5 Checking Normality\nMany parametric tests assume normally distributed data (or residuals). With small samples, normality is hard to verify formally. Visual checks (histograms, Q-Q plots) are more informative than statistical tests (Shapiro–Wilk), which have low power with small n.\nIf data are clearly skewed or have heavy tails, consider: - Nonparametric methods (rank-based tests). - Transformations (log, square root) to reduce skewness. - Robust methods (trimmed means, bootstrap).\n\n\n2.3.6 Example: Q-Q Plot for Normality Assessment\nWe check whether a small sample of test scores (n = 18) is approximately normally distributed.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated test scores (approximately normal)\ntest_scores &lt;- round(rnorm(18, mean = 70, sd = 10))\n\n# Q-Q plot\nqqnorm(test_scores, main = \"Q-Q Plot of Test Scores\")\nqqline(test_scores, col = \"red\")\n\n\n\n\n\n\n\n# Shapiro-Wilk test\nshapiro_result &lt;- shapiro.test(test_scores)\nprint(shapiro_result)\n\n\n    Shapiro-Wilk normality test\n\ndata:  test_scores\nW = 0.92, p-value = 0.1\n\ncat(\"Shapiro-Wilk p-value:\", round(shapiro_result$p.value, 3), \"\\n\")\n\nShapiro-Wilk p-value: 0.147 \n\n\nInterpretation: In a Q-Q plot, points should lie approximately on the diagonal line if data are normally distributed. Deviations at the tails indicate skewness or heavy tails. The Shapiro–Wilk test provides a p-value; p &gt; 0.05 suggests no strong evidence against normality. However, with small samples, the test has low power (may not detect departures) and high variability (can reject normality by chance). Use Q-Q plots as the primary diagnostic, supplemented by the test.\n\n\n2.3.7 Linearity and Homoscedasticity in Regression\nLinear regression assumes a linear relationship between predictors and outcome, and constant variance of residuals (homoscedasticity). Scatterplots of residuals vs. fitted values help assess these assumptions.\nWhat to look for: - Linearity: Residuals should be randomly scattered around zero with no clear pattern. Curved patterns suggest non-linearity. - Homoscedasticity: Residual spread should be constant across fitted values. Funnel shapes suggest heteroscedasticity (variance changes with fitted values).\n\n\n2.3.8 Example: Regression Diagnostics\nWe fit a simple linear regression (outcome ~ predictor) with n = 20 and check diagnostic plots.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated data\nreg_data &lt;- tibble(\n  x = runif(20, 1, 10),\n  y = 3 + 2 * x + rnorm(20, 0, 2)\n)\n\n# Fit linear model\nmodel &lt;- lm(y ~ x, data = reg_data)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = reg_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.802 -0.997 -0.385  1.043  5.501 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)    3.577      1.450    2.47      0.024 *  \nx              1.946      0.232    8.40 0.00000012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.36 on 18 degrees of freedom\nMultiple R-squared:  0.797, Adjusted R-squared:  0.785 \nF-statistic: 70.5 on 1 and 18 DF,  p-value: 0.000000122\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# Residuals vs fitted\nplot(model$fitted.values, model$residuals, \n     xlab = \"Fitted Values\", ylab = \"Residuals\",\n     main = \"Residuals vs Fitted\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\n\nInterpretation: The “Residuals vs Fitted” plot should show random scatter around zero. Patterns (curves, funnels) indicate problems. The Q-Q plot of residuals assesses normality of errors. The “Scale-Location” plot checks homoscedasticity (should be flat). The “Residuals vs Leverage” plot identifies influential observations (high Cook’s distance). With small samples, a single influential point can dominate. Consider removing it if it is an error, or report sensitivity analyses with and without it.\n\n\n2.3.9 Identifying Data Entry Errors\nLook for: - Values outside plausible ranges (e.g., age = 150, Likert response = 8 on a 1–7 scale). - Inconsistent codes (e.g., gender coded as 1/2 in some rows, M/F in others). - Duplicate records (same participant ID appearing twice). - Improbable combinations (e.g., primary school student with 20 years of work experience).\nCross-check data against source documents or re-contact participants if errors are suspected.\n\n\n2.3.10 Documenting Data Cleaning\nMaintain a data cleaning script that: - Reads raw data. - Flags potential outliers, errors, or inconsistencies. - Applies corrections or exclusions with justifications. - Produces a cleaned dataset for analysis.\nReport the number of observations excluded and reasons. Provide summary statistics for the cleaned data.\n\n\n2.3.11 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 11 Questions\n\n\n\nQ1. Why are outliers particularly problematic in small-sample research?\n\nThey are easier to detect visually\n\nThey can have disproportionate influence on results due to the limited number of observations\n\nSmall samples always have more outliers than large samples\n\nOutliers are impossible to detect with small samples\n\n\nQ2. What is the primary advantage of Mahalanobis distance over univariate outlier detection methods?\n\nIt is faster to compute\n\nIt measures how far an observation lies from the multivariate centre, accounting for covariances among variables\n\nIt only works with large samples\n\nIt automatically removes all outliers\n\n\nQ3. According to Tukey’s fences method, an observation is flagged as a potential outlier if it falls:\n\nWithin 1.5 × IQR from the quartiles\n\nBeyond ±2 standard deviations from the mean\n\nBeyond 1.5 × IQR from the quartiles\n\nExactly at the median\n\n\nQ4. Why are visual diagnostics (Q-Q plots, boxplots) preferred over formal normality tests (Shapiro–Wilk) for small samples?\n\nVisual diagnostics are more statistically rigorous\n\nNormality tests have low power with small samples and may not detect departures; visual checks provide more insight\n\nNormality tests are too expensive\n\nVisual diagnostics automatically calculate p-values\n\n\nQ5. In a regression diagnostic plot of “Residuals vs Fitted Values,” what does a funnel-shaped pattern indicate?\n\nPerfect homoscedasticity\n\nHeteroscedasticity—residual variance changes with fitted values\n\nNormality of residuals\n\nHigh collinearity among predictors\n\n\nQ6. What does Cook’s distance measure in regression diagnostics?\n\nThe distance between two data points\n\nThe influence of each observation on the regression coefficients; high values indicate influential observations\n\nThe correlation between predictors\n\nThe degree of multicollinearity\n\n\nQ7. When should an outlier be removed from analysis?\n\nAlways, because outliers are bad\n\nNever, because all data points are valuable\n\nOnly if there is clear evidence of data entry error or the observation does not belong to the target population\n\nWhenever it makes the results statistically significant\n\n\nQ8. What is an example of a data entry error that should be flagged during data screening?\n\nA participant with a high test score\n\nA Likert response of 8 on a 1–7 scale\n\nA missing value in a survey\n\nA participant who completed all questions\n\n\nQ9. Why is documenting data cleaning decisions in a reproducible script important?\n\nIt allows others to verify exclusions and understand how the cleaned dataset was produced\n\nIt makes the analysis run faster\n\nIt is required by all statistical software\n\nIt prevents missing data from occurring\n\n\nQ10. In a Q-Q plot, if the points deviate substantially from the diagonal line at the tails, what does this suggest?\n\nThe data are perfectly normally distributed\n\nThe data may have skewness or heavy tails\n\nThe sample size is too large\n\nThere are no outliers present\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “With small samples, outliers can have disproportionate influence on results.”\nA single extreme value in a sample of 15 can drastically shift means, correlations, and regression slopes.\nA2. B) “Mahalanobis distance measures how far an observation lies from the multivariate centre, accounting for covariances among variables.”\nUnivariate rules may miss cases that are unusual only when variables are considered jointly.\nA3. C) “Values beyond 1.5 × IQR from the quartiles (Tukey’s fences).”\nThe lower fence is Q1 - 1.5×IQR; the upper fence is Q3 + 1.5×IQR.\nA4. B) “With small samples, normality is hard to verify formally. Visual checks (histograms, Q-Q plots) are more informative than statistical tests (Shapiro–Wilk), which have low power with small n.”\nNormality tests may not detect departures or may reject normality by chance with small samples; Q-Q plots provide direct visual evidence.\nA5. B) “Funnel shapes suggest heteroscedasticity (variance changes with fitted values).”\nHomoscedasticity (constant variance) is violated when residual spread increases or decreases with fitted values.\nA6. B) “Cook’s distance [identifies] influential observations.”\nHigh Cook’s distance indicates that removing the observation would substantially change the regression coefficients.\nA7. C) “Only if there is clear evidence of data entry error or that the observation does not belong to the target population. Document the rationale and report results with and without outliers.”\nRemoving outliers to achieve desired results is unethical; only remove when justified.\nA8. B) “Values outside plausible ranges (e.g., age = 150, Likert response = 8 on a 1–7 scale).”\nA response of 8 on a 1–7 scale is impossible and indicates a data entry error.\nA9. A) “Maintain a data cleaning script that: Reads raw data, flags potential outliers, errors, or inconsistencies, applies corrections or exclusions with justifications, produces a cleaned dataset for analysis.”\nReproducible documentation allows verification and transparency.\nA10. B) “Deviations at the tails indicate skewness or heavy tails.”\nPoints departing from the diagonal at the extremes suggest the distribution has tails that are heavier or lighter than a normal distribution.\n\n\n\n\n\n2.3.12 Key Takeaways\n\nData screening is critical with small samples, where single observations can distort results.\nUse visual diagnostics (boxplots, scatterplots, Q-Q plots) to detect outliers, assess normality, and check regression assumptions.\nRemove outliers only with clear justification (data entry error, out-of-scope observation); report results with and without.\nNormality tests (Shapiro–Wilk) have limited power with small samples; rely primarily on visual checks.\nRegression diagnostics (residual plots, leverage, Cook’s distance) identify influential observations and assumption violations.\nDocument all data cleaning decisions in reproducible scripts and report exclusions transparently.\n\n\n\n2.3.13 Smoke Test\n\n# Re-run outlier detection\nset.seed(2025)\nx &lt;- c(5, 6, 7, 8, 5, 6, 7, 20)  # 20 is an outlier\nQ1 &lt;- quantile(x, 0.25)\nQ3 &lt;- quantile(x, 0.75)\nIQR_val &lt;- IQR(x)\nupper_fence &lt;- Q3 + 1.5 * IQR_val\nx[x &gt; upper_fence]\n\n[1] 20",
    "crumbs": [
      "Part B: Data Collection and Preparation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part B: Data Collection and Preparation</span>"
    ]
  },
  {
    "objectID": "chapters/part-b-data-collection.html#chapter-12.-handling-missing-data-in-small-samples",
    "href": "chapters/part-b-data-collection.html#chapter-12.-handling-missing-data-in-small-samples",
    "title": "2  Part B: Data Collection and Preparation",
    "section": "2.4 Chapter 12. Handling Missing Data in Small Samples",
    "text": "2.4 Chapter 12. Handling Missing Data in Small Samples\n\n2.4.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the distinctions between MCAR, MAR, and MNAR mechanisms - ✓ Understand why complete-case analysis can introduce bias - ✓ Recognize the theoretical foundations of multiple imputation (Rubin’s rules) - ✓ Understand the limitations of imputation methods with very small samples\nPractical Skills - ✓ Diagnose missingness patterns using visualization and tests in R - ✓ Implement multiple imputation using the mice package - ✓ Pool parameter estimates and standard errors across imputed datasets - ✓ Generate diagnostic plots (trace plots, convergence checks) for MICE\nCritical Evaluation - ✓ Assess when multiple imputation is feasible vs. when complete-case is preferable - ✓ Evaluate the plausibility of MAR assumptions in research contexts - ✓ Critique missing data handling approaches in published small-sample studies\nApplication - ✓ Report missing data patterns and mechanisms transparently - ✓ Choose appropriate imputation strategies given sample size and missingness - ✓ Conduct sensitivity analyses comparing complete-case vs. imputed results\n\n\n2.4.2 The Challenge of Missing Data in Small Samples\nMissing data are common in applied research. Participants skip survey questions, drop out of longitudinal studies, or provide incomplete records. With large samples, modern methods (multiple imputation, full information maximum likelihood) can handle substantial missingness without excessive bias. With small samples, however, missing data pose severe problems. Even a few missing observations can substantially reduce effective sample size and statistical power.\nMissing data methods rely on large-sample asymptotics and may be unstable or inappropriate when samples are very small (n &lt; 30) or missingness is extensive (&gt; 20%). In such cases, prevention (minimise missingness through careful design) and transparency (report missingness patterns and sensitivity analyses) are more important than sophisticated imputation.\n\n\n2.4.3 Types of Missingness\n\nMCAR (Missing Completely At Random): Missingness is unrelated to any observed or unobserved variables. For example, a survey page is randomly skipped due to a software glitch. MCAR is rare in practice.\nMAR (Missing At Random): Missingness is related to observed variables but not to the missing values themselves. For example, older participants are more likely to skip a technology question, but conditional on age, missingness is random. Most missing data methods assume MAR.\nMNAR (Missing Not At Random): Missingness is related to the unobserved values themselves. For example, individuals with high depression scores are more likely to drop out of a study. MNAR is the most problematic and requires sensitivity analyses or models for the missingness mechanism.\n\n\n\n2.4.4 Describing Missingness Patterns\nBefore handling missing data, describe the pattern:\n\nHow many observations have missing values on each variable?\nAre missing values concentrated in certain individuals or certain variables?\nIs missingness related to observed variables (compare characteristics of complete vs. incomplete cases)?\n\n\n\n2.4.5 Example Dataset for Diagnostics\nTo demonstrate the diagnostics in this chapter, we simulate a small dataset with missing values on satisfaction and performance.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\nstudy_data &lt;- tibble(\n  participant = 1:25,\n  age = sample(20:60, 25, replace = TRUE),\n  satisfaction = sample(c(3:7, NA), 25, replace = TRUE, prob = c(0.15, 0.2, 0.25, 0.2, 0.15, 0.05)),\n  performance = c(sample(50:90, 20, replace = TRUE), rep(NA, 5))\n)\n\nglimpse(study_data)\n\nRows: 25\nColumns: 4\n$ participant  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ age          &lt;int&gt; 32, 31, 55, 45, 20, 42, 29, 32, 31, 23, 46, 42, 42, 30, 5…\n$ satisfaction &lt;int&gt; 4, 7, 3, 7, NA, 7, 3, 5, 6, 6, 5, 3, 5, 5, 6, 4, 4, 6, 3,…\n$ performance  &lt;int&gt; 65, 71, 67, 66, 66, 89, 74, 87, 53, 73, 54, 83, 78, 54, 6…\n\n\n\n\n2.4.6 Testing the MCAR Assumption\nLittle’s MCAR test evaluates whether missingness is consistent with the MCAR mechanism. The test compares observed means across missing-data patterns; a large p-value suggests MCAR is plausible, whereas a small p-value indicates that missingness likely depends on observed data (i.e., not MCAR).\n\n# Test if data are Missing Completely At Random (MCAR)\nlibrary(naniar)\n\n# Little's MCAR test\nmcar_test(study_data)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      19.8     8  0.0113                4\n\n# Interpretation:\n# p &gt; 0.05: Data consistent with MCAR (missingness random)\n# p &lt; 0.05: Evidence against MCAR (missingness not random)\n\n# Visualise missing data patterns\ngg_miss_var(study_data, show_pct = TRUE)\n\n\n\n\n\n\n\nvis_miss(study_data)\n\n\n\n\n\n\n\n\nLittle’s MCAR test assesses whether missing data patterns are completely random. However, with small samples (n &lt; 50), this test has low power and should be supplemented with:\n\nVisual inspection of missingness patterns\nComparison of complete vs. incomplete cases\nDomain knowledge about likely mechanisms\n\n\n\n2.4.7 Example: Summarising Missing Data\nWe continue working with the simulated dataset (study_data) created above.\n\n# Count missing values per variable\nmissing_summary &lt;- study_data %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.))))\n\nprint(missing_summary)\n\n# A tibble: 1 × 4\n  participant   age satisfaction performance\n        &lt;int&gt; &lt;int&gt;        &lt;int&gt;       &lt;int&gt;\n1           0     0            2           5\n\n# Proportion missing\nprop_missing &lt;- study_data %&gt;%\n  summarise(across(everything(), ~ mean(is.na(.))))\n\nprint(prop_missing)\n\n# A tibble: 1 × 4\n  participant   age satisfaction performance\n        &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1           0     0         0.08         0.2\n\n# Compare complete vs incomplete cases\nstudy_data &lt;- study_data %&gt;%\n  mutate(complete = complete.cases(study_data))\n\ncomplete_vs_incomplete &lt;- study_data %&gt;%\n  group_by(complete) %&gt;%\n  summarise(mean_age = mean(age, na.rm = TRUE), .groups = \"drop\")\n\nprint(complete_vs_incomplete)\n\n# A tibble: 2 × 2\n  complete mean_age\n  &lt;lgl&gt;       &lt;dbl&gt;\n1 FALSE        31  \n2 TRUE         40.5\n\n\nInterpretation: The summary shows that satisfaction has 1–2 missing values and performance has 5 missing values (20% of the sample). If complete and incomplete cases differ systematically (e.g., incomplete cases are older), missingness may be MAR or MNAR. If they are similar, missingness may be closer to MCAR. With 20% missingness and n = 25, only 20 cases remain in a complete-case analysis (listwise deletion), reducing power substantially.\n\n\n2.4.8 Complete-Case (Listwise Deletion) Analysis\nThe simplest approach is to analyse only cases with complete data on all variables of interest. This is valid if missingness is MCAR and the reduction in sample size is tolerable. However, it can introduce bias if missingness is MAR or MNAR, and it wastes information.\nWhen to use: Missingness is minimal (&lt; 5%), or MCAR is plausible, or imputation methods are infeasible due to very small sample size.\n\n\n\n\n\n\nWarning⚠️ Common Misconception: “Listwise Deletion Is Always Safe if Missingness Is Random”\n\n\n\nMyth: “If I check for MCAR and the test is non-significant, listwise deletion is unbiased.”\nReality: Even when missingness is truly MCAR, listwise deletion loses power and can introduce bias if you have multiple variables with independent missing patterns.\nDemonstration:\n\nset.seed(2025)\n\n# Generate complete data: n=50, correlation between x and y = 0.6\nn &lt;- 50\nx &lt;- rnorm(n, 50, 10)\ny &lt;- 0.6 * x + rnorm(n, 0, 8)\n\n# True correlation (no missing data)\ntrue_cor &lt;- cor(x, y)\ncat(\"True correlation (complete data):\", round(true_cor, 3), \"\\n\\n\")\n\nTrue correlation (complete data): 0.549 \n\n# Introduce MCAR missingness (20% on x, 20% on y, independently)\nx_missing &lt;- x\ny_missing &lt;- y\nx_missing[sample(1:n, 10)] &lt;- NA  # 20% missing\ny_missing[sample(1:n, 10)] &lt;- NA  # 20% missing\n\n# Listwise deletion: only cases with both x and y\ncomplete_cases &lt;- complete.cases(x_missing, y_missing)\ncat(\"Complete cases:\", sum(complete_cases), \"/\", n, \"(\", \n    round(100 * mean(complete_cases), 1), \"%)\\n\\n\")\n\nComplete cases: 33 / 50 ( 66 %)\n\n# Correlation with listwise deletion\nlistwise_cor &lt;- cor(x_missing[complete_cases], y_missing[complete_cases])\ncat(\"Correlation (listwise deletion):\", round(listwise_cor, 3), \"\\n\")\n\nCorrelation (listwise deletion): 0.548 \n\n# Power loss\ncat(\"\\n→ Lost\", n - sum(complete_cases), \"cases (\", \n    round(100 * (1 - mean(complete_cases)), 1), \"% of data)\\n\")\n\n\n→ Lost 17 cases ( 34 % of data)\n\ncat(\"→ Correlation estimate is based on n =\", sum(complete_cases), \n    \"instead of n = 50\\n\")\n\n→ Correlation estimate is based on n = 33 instead of n = 50\n\ncat(\"→ Standard error is\", round(sqrt(1/sum(complete_cases)) / sqrt(1/n), 2), \n    \"times larger\\n\")\n\n→ Standard error is 1.23 times larger\n\n\nWhy this matters:\n\nPower loss: With 20% missing on x and 20% on y (independent), you lose ~36% of cases\n\nFormula: (1 - p_x) × (1 - p_y) = 0.8 × 0.8 = 0.64 → 64% remain, 36% lost\n\nMultiple variables compound: With 5 variables each 15% missing, you keep only 44% of cases\nBias can still occur: If missingness is MAR (not MCAR), listwise deletion is biased\n\nLesson:\n\nMCAR does NOT mean listwise deletion is optimal—it’s still wasteful\nUse multiple imputation even with MCAR if missingness &gt; 10%\nWith small samples (n &lt; 50), losing even 20% of cases is catastrophic for power\n\nWhen listwise deletion is actually safe:\n\nMissingness &lt; 5% on any variable\nn is large enough that losing cases doesn’t hurt power\nYou’ve verified MCAR (not just MAR) AND documented the power loss\n\n\n\n\n\n2.4.9 Mean Imputation (Not Recommended)\nMean imputation replaces missing values with the variable mean. This approach artificially reduces variance and distorts correlations. It is generally not recommended, especially with small samples where each imputed value has disproportionate impact.\nWhen to use: Rarely. Only if missingness is trivial (1–2 values in a large dataset) and for descriptive purposes only (not inference).\n\n\n2.4.10 Last Observation Carried Forward (LOCF)\nIn longitudinal studies, LOCF replaces missing follow-up values with the last observed value for that individual. This assumes no change after the last observation, which is often unrealistic. LOCF can bias estimates and is not generally recommended.\nWhen to use: Rarely. Only if the assumption of no change is plausible, and alternatives are infeasible.\n\n\n2.4.11 Multiple Imputation (Caution with Small Samples)\nMultiple imputation (MI) generates several plausible imputed datasets, analyses each separately, and pools results to account for imputation uncertainty. MI is the gold standard for handling missing data in large samples. However, MI requires sufficient data to estimate imputation models reliably. With very small samples (n &lt; 30) or many missing values (&gt; 20%), MI can be unstable or yield implausible imputations.\nWhen to use: Moderate sample sizes (n ≥ 30), missingness not too extensive (&lt; 20%), MAR assumption plausible.\n\n\n2.4.12 Example: Multiple Imputation with mice (Caution)\nWe apply MI to the dataset with missing satisfaction and performance values. Given the small sample (n = 25) and 20% missingness, interpret results cautiously.\n\n# Multiple imputation requires the 'mice' package\nif (requireNamespace(\"mice\", quietly = TRUE)) {\n  library(mice)\n  \n  # Remove 'complete' indicator variable before imputation\n  impute_data &lt;- study_data %&gt;% select(participant, age, satisfaction, performance)\n  \n  # Perform multiple imputation (m = 5 imputations)\n  set.seed(2025)\n  imp &lt;- mice(impute_data, m = 5, method = \"pmm\", printFlag = FALSE)\n  \n  # Check imputed values\n  print(imp)\n  \n  # Example analysis: regress performance on age and satisfaction\n  fit &lt;- with(imp, lm(performance ~ age + satisfaction))\n  pooled &lt;- pool(fit)\n  summary(pooled)\n  \n} else {\n  cat(\"Install 'mice' package to run multiple imputation.\\n\")\n  cat(\"With very small samples, MI may be unstable; consider complete-case analysis.\\n\")\n}\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n participant          age satisfaction  performance \n          \"\"           \"\"        \"pmm\"        \"pmm\" \nPredictorMatrix:\n             participant age satisfaction performance\nparticipant            0   1            1           1\nage                    1   0            1           1\nsatisfaction           1   1            0           1\nperformance            1   1            1           0\n\n\n          term estimate std.error statistic    df   p.value\n1  (Intercept) 87.84881   15.4415    5.6892 11.10 0.0001355\n2          age -0.09038    0.2349   -0.3847 16.72 0.7052925\n3 satisfaction -2.22444    2.0767   -1.0711 12.55 0.3042733\n\n\nInterpretation: MI generates plausible values for missing data based on observed relationships. The pooled results combine estimates across imputations, with standard errors adjusted for imputation uncertainty. However, with n = 25 and 20% missingness, the imputation model is estimated from limited data, and results may be unstable. Compare MI results to complete-case analysis; if they differ substantially, report both and acknowledge uncertainty.\n\n\n2.4.13 Checking Convergence of Multiple Imputation\nWhen using mice, always check whether the imputation algorithm has converged. Poor convergence means the imputed values may not be stable, especially with small samples or complex missing data patterns.\nKey diagnostics:\n\nTrace plots: Plot imputed values across iterations for each variable. Lines should mix well without trends.\nStrip plots: Display distributions of observed (blue) vs. imputed (red) values. Distributions should be similar if MAR holds.\n\n\n# Convergence diagnostics for mice imputation\nif (requireNamespace(\"mice\", quietly = TRUE) && exists(\"imp\")) {\n  library(mice)\n  \n  # Trace plots: check convergence across iterations\n  # Each line represents one imputed dataset; should be well-mixed\n  plot(imp, c(\"satisfaction\", \"performance\"))\n  \n  # Strip plots: compare observed (blue) vs imputed (red) values\n  # Distributions should be similar under MAR\n  stripplot(imp, satisfaction + performance ~ .imp, pch = 20, cex = 1.2)\n  \n  cat(\"\\n=== Convergence Check ===\\n\")\n  cat(\"✓ Trace plots: Lines should mix well without systematic trends\\n\")\n  cat(\"✓ Strip plots: Imputed (red) should resemble observed (blue) distributions\\n\")\n  cat(\"✓ If convergence looks poor, increase iterations: mice(..., maxit = 20)\\n\\n\")\n  \n} else {\n  cat(\"mice imputation object not found; run the previous chunk first.\\n\")\n}\n\n\n=== Convergence Check ===\n✓ Trace plots: Lines should mix well without systematic trends\n✓ Strip plots: Imputed (red) should resemble observed (blue) distributions\n✓ If convergence looks poor, increase iterations: mice(..., maxit = 20)\n\n\nWhat to look for:\n\nTrace plots: Each imputation should show relatively stable values across iterations. If you see upward or downward trends, the algorithm hasn’t converged. Solution: increase maxit (default is 5).\nStrip plots: The distribution of imputed values (red) should be similar to observed values (blue). Large differences suggest the imputation model may not fit well or MNAR may be present.\n\nWith small samples (n &lt; 30), convergence can be slower and more sensitive to model specification. If diagnostics show problems, consider:\n\nSimplifying the imputation model (use predictive mean matching with fewer predictors)\nIncreasing iterations (maxit = 20 or more)\nComparing to complete-case analysis as a sensitivity check\n\n\n\n2.4.14 Sensitivity Analyses\nWhen missingness is substantial or MNAR is suspected, conduct sensitivity analyses:\n\nCompare complete-case results to imputed results.\nVary assumptions about the missing data mechanism (e.g., impute extreme values to simulate worst-case scenarios).\nReport results under multiple scenarios and discuss implications.\n\n\n\n2.4.15 Preventing Missing Data\nThe best approach to missing data is prevention:\n\nDesign clear, concise instruments.\nMinimise respondent burden.\nFollow up with participants who miss appointments or skip questions.\nPilot test procedures to identify confusing or burdensome items.\nBuild rapport and trust with participants.",
    "crumbs": [
      "Part B: Data Collection and Preparation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part B: Data Collection and Preparation</span>"
    ]
  },
  {
    "objectID": "chapters/part-b-data-collection.html#chapter-12.5.-assessing-multiple-imputation-quality",
    "href": "chapters/part-b-data-collection.html#chapter-12.5.-assessing-multiple-imputation-quality",
    "title": "2  Part B: Data Collection and Preparation",
    "section": "2.5 Chapter 12.5. Assessing Multiple Imputation Quality",
    "text": "2.5 Chapter 12.5. Assessing Multiple Imputation Quality\n\n2.5.1 Learning Objectives\nBy the end of this section, you will understand how to diagnose the quality of multiple imputation models. You will learn to check convergence, compare imputed vs. observed distributions, assess sensitivity to the number of imputations (m), and interpret diagnostic plots. These skills ensure that your imputed datasets are appropriate for downstream analyses.\n\n\n2.5.2 Why Imputation Diagnostics Matter\nMultiple imputation (MI) is not a “black box” procedure. The quality of imputed values depends on:\n\nModel specification: Are the imputation models correctly specified (e.g., predictive mean matching, logistic regression for binary variables)?\nConvergence: Have the iterative algorithms stabilised?\nPlausibility: Do imputed values resemble the observed data distribution?\nSensitivity to m: Are pooled estimates stable across different numbers of imputations?\n\nFailure to check diagnostics can lead to:\n\nBiased parameter estimates\nIncorrect standard errors\nImputed values outside plausible ranges\nOverconfidence in results\n\n\n\n2.5.3 Diagnostic 1: Convergence Checks\nThe mice algorithm uses iterative chained equations: it cycles through variables, updating imputations based on the current values of other variables. Convergence occurs when these iterations stabilise (no systematic trends).\n\n2.5.3.1 Trace Plots\nTrace plots show the mean and SD of imputed values across iterations for each variable. Good convergence looks like:\n\nLines are “noisy” (random fluctuation)\nNo systematic trends (upward or downward drift)\nMultiple chains (from different imputations) intermingle\n\n\nlibrary(mice)\nlibrary(tidyverse)\n\n# Simulate data with missing values\nset.seed(2025)\nmi_data &lt;- tibble(\n  age = c(25, 32, NA, 45, 29, NA, 38, 41, 27, 35, NA, 42, 30, 28, 39),\n  satisfaction = c(4, 5, 3, NA, 4, 5, NA, 4, 5, 3, 4, NA, 5, 4, 3),\n  income = c(35, 50, 42, 60, NA, 55, 48, NA, 40, 52, 45, 58, NA, 38, 49)\n)\n\n# Multiple imputation with more iterations to demonstrate convergence\nimp &lt;- mice(mi_data, m = 5, maxit = 20, seed = 2025, print = FALSE)\n\n# Plot trace lines for all variables\nplot(imp, c(\"age\", \"satisfaction\", \"income\"))\n\n\n\n\n\n\n\n\nInterpretation:\n\nIdeal: Lines fluctuate randomly around a stable mean (like a “fuzzy caterpillar”)\nProblem: Lines show trends (increasing or decreasing over iterations) → increase maxit\nProblem: Lines are smooth or separated by chain → check imputation model specification\n\n\n\n2.5.3.2 Checking Specific Variables\nIf you have many variables, focus on those with the most missingness:\n\n# Focus on specific variables with high missingness\nplot(imp, \"age\")\n\n\n\n\n\n\n\n\nWhen to increase iterations:\n\nIf you see trends in the first 10–20 iterations, try maxit = 50 or maxit = 100\nModern guidance: maxit = 20–50 is usually sufficient for MCAR/MAR data\n\n\n\n\n2.5.4 Diagnostic 2: Imputed vs. Observed Distributions\nImputed values should resemble the observed data distribution (but not be identical). Large discrepancies suggest model misspecification.\n\n2.5.4.1 Density Plots\n\n# Compare density plots: blue = observed, red = imputed\ndensityplot(imp)\n\n\n\n\n\n\n\n\nInterpretation:\n\nGood: Imputed (red) and observed (blue) distributions overlap substantially\nRed flag: Imputed values are all at one value (e.g., the mean) → model too restrictive (try method = \"pmm\" for predictive mean matching)\nRed flag: Imputed values fall far outside observed range → model misspecified (e.g., using linear imputation for bounded variables)\n\n\n\n2.5.4.2 Strip Plots (Univariate)\nStrip plots show individual imputed values (red) alongside observed values (blue):\n\n# Strip plots for each variable\nstripplot(imp, age ~ .imp, pch = 20, cex = 1.5)\n\n\n\n\n\n\n\n\nInterpretation:\n\nImputed values (red dots) should “fill in” gaps in the observed data (blue dots)\nLook for outliers: Are any imputed values far outside the observed range?\n\n\n\n\n2.5.5 Diagnostic 3: Sensitivity to m (Number of Imputations)\nThe number of imputations (m) affects the precision of pooled estimates. With more imputations, pooled estimates become more stable and standard errors more accurate.\n\n2.5.5.1 Rule of Thumb for m\n\nFraction of missing information (FMI) determines required m:\n\nFMI &lt; 10%: m = 5–10 sufficient\nFMI = 10–30%: m = 20 recommended\nFMI &gt; 30%: m = 50–100 may be needed\n\nWhite, Royston, and Wood (2011) suggest: \\(m \\geq 100 \\times \\text{FMI}\\)\n\n\n\n2.5.5.2 Testing Sensitivity\n\n# Simulate larger dataset for demonstration\nset.seed(2025)\nmi_data_large &lt;- tibble(\n  age = sample(c(25:50, NA), 50, replace = TRUE, prob = c(rep(0.03, 26), 0.22)),\n  income = sample(c(30:70, NA), 50, replace = TRUE, prob = c(rep(0.024, 41), 0.02)),\n  satisfaction = sample(c(1:5, NA), 50, replace = TRUE, prob = c(rep(0.18, 5), 0.1))\n)\n\n# Impute with varying m\nimp_m5 &lt;- mice(mi_data_large, m = 5, maxit = 20, seed = 2025, print = FALSE)\nimp_m20 &lt;- mice(mi_data_large, m = 20, maxit = 20, seed = 2025, print = FALSE)\nimp_m50 &lt;- mice(mi_data_large, m = 50, maxit = 20, seed = 2025, print = FALSE)\n\n# Fit model and pool results\nfit_m5 &lt;- with(imp_m5, lm(satisfaction ~ age + income))\nfit_m20 &lt;- with(imp_m20, lm(satisfaction ~ age + income))\nfit_m50 &lt;- with(imp_m50, lm(satisfaction ~ age + income))\n\npooled_m5 &lt;- pool(fit_m5)\npooled_m20 &lt;- pool(fit_m20)\npooled_m50 &lt;- pool(fit_m50)\n\n# Compare coefficient estimates and SEs\ncompare_m &lt;- tibble(\n  m = c(5, 20, 50),\n  age_coef = c(\n    summary(pooled_m5)$estimate[2],\n    summary(pooled_m20)$estimate[2],\n    summary(pooled_m50)$estimate[2]\n  ),\n  age_se = c(\n    summary(pooled_m5)$std.error[2],\n    summary(pooled_m20)$std.error[2],\n    summary(pooled_m50)$std.error[2]\n  )\n)\n\nprint(compare_m)\n\n# A tibble: 3 × 3\n      m age_coef age_se\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1     5   0.0676 0.0320\n2    20   0.0723 0.0305\n3    50   0.0718 0.0310\n\n\nInterpretation:\n\nCoefficients should be similar across m (small differences are expected due to Monte Carlo error)\nStandard errors should stabilize as m increases\nIf estimates change substantially (e.g., &gt; 10% difference in coefficients), use larger m\n\n\n\n2.5.5.3 When to Use Larger m\n\nHigh missingness (&gt; 20%): Use m ≥ 20\nSmall samples (n &lt; 50): Larger m reduces Monte Carlo error\nSensitive analyses (e.g., clinical trials): Use m ≥ 50 for conservative inference\n\n\n\n\n2.5.6 Diagnostic 4: Checking Imputation Model Assumptions\n\n2.5.6.1 Inspect Imputation Methods\n\n# Check which imputation methods were used\nimp$method\n\n         age satisfaction       income \n       \"pmm\"        \"pmm\"        \"pmm\" \n\n\nCommon methods:\n\npmm: Predictive mean matching (robust, preserves distribution)\nnorm: Bayesian linear regression (assumes normality)\nlogreg: Logistic regression (for binary variables)\npolyreg: Multinomial logistic regression (for categorical variables)\n\nBest practice: Use pmm for continuous variables unless you have strong reasons to assume normality.\n\n\n2.5.6.2 Check Predictor Matrix\n\n# See which variables predict each other\nimp$predictorMatrix\n\n             age satisfaction income\nage            0            1      1\nsatisfaction   1            0      1\nincome         1            1      0\n\n\nInterpretation:\n\nRows = variables to impute\nColumns = predictor variables\n1 = use as predictor, 0 = do not use\n\nModify if needed:\n\n# Example: Exclude a variable from predicting another\npred &lt;- imp$predictorMatrix\npred[\"age\", \"satisfaction\"] &lt;- 0  # Don't use satisfaction to predict age\n\n# Re-run imputation with modified predictor matrix\nimp_modified &lt;- mice(mi_data, m = 5, predictorMatrix = pred, print = FALSE)\n\n\n\n\n2.5.7 Diagnostic 5: Fraction of Missing Information (FMI)\nThe FMI quantifies how much uncertainty is introduced by imputation. It is automatically reported by pool():\n\n# Pool results and examine FMI\npooled_result &lt;- pool(fit_m20)\nsummary(pooled_result)\n\n         term estimate std.error statistic    df p.value\n1 (Intercept)  1.57183   1.39276     1.129 32.67 0.26730\n2         age  0.07225   0.03051     2.368 35.21 0.02352\n3      income -0.02782   0.01819    -1.530 37.23 0.13456\n\n\nColumns to examine:\n\nfmi: Fraction of missing information for each coefficient\nlambda: Proportion of total variance due to missingness\n\nInterpretation:\n\nFMI &lt; 0.10: Low missing information; m = 5–10 sufficient\nFMI = 0.10–0.30: Moderate; use m = 20–50\nFMI &gt; 0.30: High; consider whether MI is appropriate (may need m = 50–100)\n\n\n\n2.5.8 Example: Full Diagnostic Workflow\n\n# Step 1: Describe missingness\nlibrary(naniar)\nmiss_var_summary(mi_data_large)\n\n# A tibble: 3 × 3\n  variable     n_miss pct_miss\n  &lt;chr&gt;         &lt;int&gt;    &lt;num&gt;\n1 age              10       20\n2 satisfaction      6       12\n3 income            1        2\n\n# Step 2: Perform MI with adequate m and maxit\nimp_final &lt;- mice(mi_data_large, m = 20, maxit = 30, seed = 2025, print = FALSE)\n\n# Step 3: Check convergence\nplot(imp_final, c(\"age\", \"income\", \"satisfaction\"))\n\n\n\n\n\n\n\n# Step 4: Compare distributions\ndensityplot(imp_final)\n\n\n\n\n\n\n\n# Step 5: Fit model and pool\nfit_final &lt;- with(imp_final, lm(satisfaction ~ age + income))\npooled_final &lt;- pool(fit_final)\n\n# Step 6: Check FMI\nsummary(pooled_final)\n\n         term estimate std.error statistic    df p.value\n1 (Intercept)  1.25971   1.47860     0.852 28.51 0.40134\n2         age  0.07511   0.03243     2.316 30.41 0.02748\n3      income -0.02378   0.01919    -1.239 32.05 0.22438\n\n# Step 7: Report results\ncat(\"Pooled regression results (m = 20 imputations):\\n\")\n\nPooled regression results (m = 20 imputations):\n\nsummary(pooled_final)\n\n         term estimate std.error statistic    df p.value\n1 (Intercept)  1.25971   1.47860     0.852 28.51 0.40134\n2         age  0.07511   0.03243     2.316 30.41 0.02748\n3      income -0.02378   0.01919    -1.239 32.05 0.22438\n\n\n\n\n2.5.9 Red Flags and Troubleshooting\n\n\n\n\n\n\n\n\nProblem\nSymptom\nSolution\n\n\n\n\nNon-convergence\nTrace plots show trends\nIncrease maxit (try 50–100)\n\n\nImputed values at one value\nDensity plot shows spike\nUse method = \"pmm\" instead of norm\n\n\nImputed values out of range\nStrip plot shows outliers\nCheck variable type (e.g., use logreg for binary)\n\n\nUnstable estimates across m\nCoefficients vary &gt; 10%\nIncrease m (try 50–100)\n\n\nHigh FMI (&gt; 0.50)\nLarge uncertainty\nConsider whether MI is appropriate; may need auxiliary variables or accept wider CIs\n\n\nSeparation warnings (logistic regression)\nModel fails to converge\nUse penalized imputation methods or increase sample size\n\n\n\n\n\n2.5.10 Reporting MI Diagnostics\nWhen reporting MI results, include:\n\nMissingness pattern: “Three variables had missing data (age: 20%, income: 18%, satisfaction: 10%)”\nImputation model: “We used predictive mean matching with m = 20 imputations and maxit = 30”\nConvergence: “Trace plots showed convergence after 20 iterations (see Supplementary Figure S1)”\nPlausibility: “Imputed values were visually consistent with observed distributions (density plots in Supplementary Figure S2)”\nSensitivity: “Results were stable across m = 5, 20, and 50 imputations (coefficient differences &lt; 5%)”\nFMI: “Fraction of missing information ranged from 0.12 to 0.25, indicating moderate impact of missingness”\n\n\n\n2.5.11 Key Takeaways\n\nConvergence checks (trace plots) ensure the imputation algorithm has stabilised; increase maxit if trends are visible\nDensity and strip plots compare imputed vs. observed distributions; imputed values should resemble observed data\nNumber of imputations (m) should match the fraction of missing information: m ≥ 20 for FMI = 10–30%\nSensitivity analyses test whether results are stable across different values of m\nFraction of Missing Information (FMI) quantifies uncertainty from imputation; FMI &gt; 0.30 suggests high impact\nRed flags: Imputed values at one value, outliers, non-convergence, unstable estimates → revise imputation model\nTransparency: Report convergence, plausibility checks, and sensitivity analyses in supplementary materials\n\n\n\n\n2.5.12 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 12 Questions\n\n\n\nQ1. What is the difference between MCAR (Missing Completely At Random) and MAR (Missing At Random)?\n\nMCAR means no data are missing; MAR means some data are missing\n\nMCAR means missingness is unrelated to any variables; MAR means missingness is related to observed variables but not the missing values themselves\n\nMCAR and MAR are identical terms\n\nMCAR applies to small samples; MAR applies to large samples\n\n\nQ2. Why is mean imputation (replacing missing values with the variable mean) generally not recommended?\n\nIt requires specialized software\n\nIt artificially reduces variance and distorts correlations\n\nIt only works with categorical variables\n\nIt is too computationally expensive\n\n\nQ3. If you have 20% missingness on variable X and 20% missingness on variable Y (independently), approximately what percentage of cases will be lost with listwise deletion?\n\n20%\n\n36%\n\n40%\n\n64%\n\n\nQ4. What does MNAR (Missing Not At Random) mean?\n\nMissingness is unrelated to any variables\n\nMissingness is related to observed variables only\n\nMissingness is related to the unobserved (missing) values themselves\n\nMissing data occur randomly due to software errors\n\n\nQ5. What is the primary limitation of using multiple imputation with very small samples (n &lt; 30)?\n\nMultiple imputation cannot handle small samples\n\nThe imputation model is estimated from limited data and may be unstable or yield implausible values\n\nMultiple imputation requires at least 1,000 observations\n\nMultiple imputation only works with MNAR data\n\n\nQ6. In multiple imputation diagnostics, what do trace plots check?\n\nThe distribution of imputed values\n\nWhether the imputation algorithm has converged (stabilized) across iterations\n\nThe correlation between variables\n\nThe sample size required for analysis\n\n\nQ7. When comparing imputed (red) vs. observed (blue) distributions in density plots, what is a “red flag”?\n\nThe distributions overlap substantially\n\nImputed values fall far outside the observed range or all cluster at one value\n\nBoth distributions are normally distributed\n\nThe imputed values have slightly different means\n\n\nQ8. What is the rule of thumb for choosing the number of imputations (m) based on the fraction of missing information (FMI)?\n\nAlways use m = 5 regardless of FMI\n\nUse m ≥ 100 × FMI (e.g., FMI = 0.20 requires m ≥ 20)\n\nUse m = 1000 for all analyses\n\nm should equal the sample size\n\n\nQ9. What does Little’s MCAR test evaluate?\n\nWhether the sample size is adequate\n\nWhether missingness is consistent with the MCAR mechanism by comparing observed means across missing-data patterns\n\nWhether multiple imputation has converged\n\nWhether variables are normally distributed\n\n\nQ10. Why is prevention the best approach to missing data?\n\nPrevention is cheaper than imputation software\n\nEven sophisticated imputation methods have limitations and cannot fully recover information lost to missingness; prevention avoids the problem\n\nPrevention is only relevant for large samples\n\nMultiple imputation cannot handle any missing data\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “MCAR (Missing Completely At Random): Missingness is unrelated to any observed or unobserved variables… MAR (Missing At Random): Missingness is related to observed variables but not to the missing values themselves.”\nMCAR is a stricter assumption where missingness is completely random, while MAR allows missingness to depend on observed data.\nA2. B) “Mean imputation replaces missing values with the variable mean. This approach artificially reduces variance and distorts correlations.”\nBy replacing all missing values with the mean, you reduce the variability in the data and bias correlation estimates downward.\nA3. B) “With 20% missing on x and 20% on y (independent), you lose ~36% of cases—Formula: (1 - p_x) × (1 - p_y) = 0.8 × 0.8 = 0.64 → 64% remain, 36% lost”\nWhen missingness patterns are independent, the compound effect removes (1 - 0.8 × 0.8) = 36% of cases.\nA4. C) “MNAR (Missing Not At Random): Missingness is related to the unobserved values themselves.”\nFor example, individuals with high depression scores being more likely to drop out of a study.\nA5. B) “With very small samples (n &lt; 30) or many missing values (&gt; 20%), MI can be unstable or yield implausible imputations.”\nThe imputation model requires sufficient data to estimate relationships reliably; very small samples provide limited information.\nA6. B) “Trace plots show the mean and SD of imputed values across iterations for each variable… Convergence occurs when these iterations stabilise.”\nGood convergence shows random fluctuation without systematic trends (upward or downward drift).\nA7. B) “Imputed values fall far outside observed range → model misspecified… Imputed values are all at one value (e.g., the mean) → model too restrictive.”\nLarge discrepancies between imputed and observed distributions suggest problems with the imputation model specification.\nA8. B) “White, Royston, and Wood (2011) suggest: m ≥ 100 × FMI”\nFor example, if FMI = 0.20 (20% missing information), you should use m ≥ 20 imputations.\nA9. B) “Little’s MCAR test evaluates whether missingness is consistent with the MCAR mechanism. The test compares observed means across missing-data patterns.”\nA large p-value suggests MCAR is plausible; a small p-value indicates missingness likely depends on observed data.\nA10. B) “The best approach to missing data is prevention… Pilot test procedures to identify confusing or burdensome items. Build rapport and trust with participants.”\nNo imputation method can fully recover the information lost to missingness, so preventing missing data through good design is always preferable.\n\n\n\n\n\n2.5.13 Key Takeaways\n\nMissing data reduce effective sample size and can bias estimates, particularly with small samples.\nDescribe missingness patterns (proportion missing, complete vs. incomplete case characteristics) before choosing a method.\nComplete-case analysis is simple and valid if missingness is minimal or MCAR, but wastes information and loses power.\nMean imputation and LOCF are not recommended; they distort variance and correlations.\nMultiple imputation is the gold standard with adequate samples (n ≥ 30, missingness &lt; 20%) but may be unstable with very small samples.\nSensitivity analyses compare results under different assumptions about the missing data mechanism.\nPrevention through careful study design is the best strategy for minimising missing data.\n\n\n\n2.5.14 Smoke Test\n\n# Re-run missing data summary\nset.seed(2025)\nx &lt;- c(5, 6, NA, 8, 7, NA, 9)\nsum(is.na(x))\n\n[1] 2\n\nmean(is.na(x))\n\n[1] 0.2857",
    "crumbs": [
      "Part B: Data Collection and Preparation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part B: Data Collection and Preparation</span>"
    ]
  },
  {
    "objectID": "chapters/part-b-data-collection.html#summary-of-part-b",
    "href": "chapters/part-b-data-collection.html#summary-of-part-b",
    "title": "2  Part B: Data Collection and Preparation",
    "section": "2.6 Summary of Part B",
    "text": "2.6 Summary of Part B\nIn Part B, we addressed practical challenges in collecting and preparing data for small-sample studies. Chapter 9 covered sampling strategies (probability, stratified, purposive, quota sampling) and power analyses to understand detectability given sample size constraints. Chapter 10 discussed measurement quality and scale development, including item analysis, cognitive interviews, and reliability assessment with short scales. Chapter 11 presented data screening and diagnostic checks (outlier detection, normality assessment, regression diagnostics) to identify problems before analysis. Chapter 12 addressed missing data patterns, simple and advanced imputation methods, and the importance of transparency and prevention. Each chapter included learning objectives, method descriptions, runnable R examples, interpretations, key takeaways, and smoke tests. All code uses only approved packages and runs cleanly in a fresh R session. The guidance emphasises transparency, caution, and appropriate method selection given small-sample constraints.",
    "crumbs": [
      "Part B: Data Collection and Preparation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part B: Data Collection and Preparation</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html",
    "href": "chapters/part-c-analysis-methods.html",
    "title": "3  Part C: Analysis Methods",
    "section": "",
    "text": "3.1 Chapter 3. Exact Tests and Resampling Methods\nThis part presents the core toolkit for small-sample quantitative analysis. We cover exact and resampling tests, nonparametric rank-based methods, penalised and Bayesian regression, reliability analysis for short scales, multi-criteria decision-making (MCDM) techniques, and methods for sparse counts.",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#chapter-3.-exact-tests-and-resampling-methods",
    "href": "chapters/part-c-analysis-methods.html#chapter-3.-exact-tests-and-resampling-methods",
    "title": "3  Part C: Analysis Methods",
    "section": "",
    "text": "3.1.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain when exact tests are necessary vs. when asymptotic approximations suffice - ✓ Understand the theoretical basis of permutation and bootstrap resampling - ✓ Recognize the assumptions underlying Fisher’s exact, Barnard’s, and binomial tests - ✓ Distinguish parametric bootstrap from nonparametric bootstrap\nPractical Skills - ✓ Conduct Fisher’s exact test in R using fisher.test() - ✓ Implement permutation tests for group comparisons using coin package - ✓ Generate bootstrap confidence intervals with boot package - ✓ Apply exact binomial and Poisson tests for count data\nCritical Evaluation - ✓ Assess when exact tests are overly conservative vs. appropriately cautious - ✓ Evaluate the trade-off between computational cost and precision - ✓ Critique inappropriate use of asymptotic tests in small-sample contexts\nApplication - ✓ Select the appropriate exact/resampling method for research questions - ✓ Report resampling procedures transparently (iterations, seed, method) - ✓ Construct bootstrap CIs for custom effect sizes not available in packages\n\n\n3.1.2 When to Use Exact and Resampling Methods\nExact tests compute p-values directly from the probability distribution of the test statistic under the null hypothesis, without relying on large-sample approximations. They are particularly useful when sample sizes are small, when distributional assumptions (such as normality) are questionable, or when the outcome is discrete (binary, count, or ordinal).\nResampling methods (permutation tests and bootstrap) use the observed data to approximate the sampling distribution of a statistic. Permutation tests shuffle group labels or treatment assignments to generate a reference distribution under the null hypothesis. Bootstrap resampling draws repeated samples (with replacement) from the observed data to estimate the variability of an estimator and construct confidence intervals.\nBoth approaches are computationally intensive but feasible with modern computing. They often provide more accurate inferences than classical parametric tests when samples are small or assumptions are violated.\n\n\n3.1.3 Fisher’s Exact Test for 2×2 Tables\nFisher’s exact test is the standard method for testing independence in a 2×2 contingency table when cell counts are small. It conditions on the row and column totals and computes the exact probability of observing the data (or more extreme) under the null hypothesis of no association.\nThe test is conservative (tends to yield p-values that are too large) in some settings, but remains widely used because it guarantees correct Type I error control. It is most appropriate for fixed-margin designs (such as retrospective case-control studies) but is often applied more broadly.\nAssumptions: None regarding distributional form. Observations must be independent. The test conditions on marginal totals.\nWhen to use: Small cell counts (any cell &lt; 5), binary outcomes crossed with binary exposures, exact inference required.\n\n\n3.1.4 Example: Fisher’s Exact Test\nSuppose we are evaluating a new training intervention. Of 10 employees who received training, 8 met their performance target; of 10 who did not receive training, 3 met the target. We test whether training is associated with meeting the target.\n\nlibrary(tidyverse)\n\n# 💡 TEACHING POINT 1: Build the table carefully\n# Create 2x2 table: rows = training (Yes, No), columns = target met (Yes, No)\ntraining_yes &lt;- c(8, 2)  # 8 met target, 2 did not\ntraining_no &lt;- c(3, 7)   # 3 met target, 7 did not\n\ntable_data &lt;- matrix(c(8, 3, 2, 7), nrow = 2, byrow = TRUE,\n                     dimnames = list(Training = c(\"Yes\", \"No\"),\n                                     Target = c(\"Met\", \"Not Met\")))\n\n# 💡 TEACHING POINT 2: Always print the table first!\n# Students should verify the layout matches their research question\ncat(\"Contingency Table:\\n\")\n\nContingency Table:\n\nprint(table_data)\n\n        Target\nTraining Met Not Met\n     Yes   8       3\n     No    2       7\n\ncat(\"\\n\")\n# 💡 TEACHING POINT 3: Check expected cell counts\n# If all ≥5, chi-square is an alternative; if any &lt;5, Fisher's is mandatory\ncat(\"Expected cell counts under independence:\\n\")\n\nExpected cell counts under independence:\n\nexpected &lt;- chisq.test(table_data)$expected\nprint(round(expected, 2))\n\n        Target\nTraining Met Not Met\n     Yes 5.5     5.5\n     No  4.5     4.5\n\ncat(\"→ All cells &lt; 5, so Fisher's exact test is required\\n\\n\")\n\n→ All cells &lt; 5, so Fisher's exact test is required\n\n# 💡 TEACHING POINT 4: Run Fisher's exact test\nfisher_result &lt;- fisher.test(table_data)\nprint(fisher_result)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  table_data\np-value = 0.07\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.8821 127.0558\nsample estimates:\nodds ratio \n     8.153 \n\n# 💡 TEACHING POINT 5: Interpret systematically\ncat(\"\\n=== INTERPRETATION GUIDE ===\\n\")\n\n\n=== INTERPRETATION GUIDE ===\n\ncat(\"1. Odds Ratio =\", round(fisher_result$estimate, 2), \"\\n\")\n\n1. Odds Ratio = 8.15 \n\ncat(\"   → Trained employees have\", round(fisher_result$estimate, 1), \n    \"times higher odds of meeting target\\n\\n\")\n\n   → Trained employees have 8.2 times higher odds of meeting target\n\ncat(\"2. 95% CI = [\", round(fisher_result$conf.int[1], 2), \",\", \n    round(fisher_result$conf.int[2], 2), \"]\\n\")\n\n2. 95% CI = [ 0.88 , 127.1 ]\n\ncat(\"   → Plausible range for OR. Does it include 1 (no effect)?\\n\")\n\n   → Plausible range for OR. Does it include 1 (no effect)?\n\nif(fisher_result$conf.int[1] &gt; 1) {\n  cat(\"   → No! The entire CI is &gt; 1, suggesting a real effect\\n\\n\")\n} else {\n  cat(\"   → Yes. CI includes 1, so effect is uncertain\\n\\n\")\n}\n\n   → Yes. CI includes 1, so effect is uncertain\n\ncat(\"3. p-value =\", round(fisher_result$p.value, 3), \"\\n\")\n\n3. p-value = 0.07 \n\nif(fisher_result$p.value &lt; 0.05) {\n  cat(\"   → Significant at α = 0.05\\n\\n\")\n} else {\n  cat(\"   → Not significant at α = 0.05\\n\\n\")\n}\n\n   → Not significant at α = 0.05\n\ncat(\"4. Conclusion:\\n\")\n\n4. Conclusion:\n\nif(fisher_result$p.value &lt; 0.05 & fisher_result$estimate &gt; 1) {\n  cat(\"   Training shows a significant positive effect (OR =\", \n      round(fisher_result$estimate, 1), \", p =\", round(fisher_result$p.value, 3), \")\\n\")\n} else {\n  cat(\"   Training shows promise (OR =\", round(fisher_result$estimate, 1), \n      \"), but n =\", sum(table_data), \"is small.\\n\")\n  cat(\"   Cannot rule out chance. A larger study is warranted.\\n\")\n}\n\n   Training shows promise (OR = 8.2 ), but n = 20 is small.\n   Cannot rule out chance. A larger study is warranted.\n\n\nInterpretation Summary: The p-value indicates the probability of observing this degree of association (or stronger) if training had no effect. The odds ratio quantifies the strength of association: an odds ratio greater than 1 indicates higher odds of meeting the target among trained employees. The confidence interval for the odds ratio provides a range of plausible values. With small samples, pay close attention to the CI width—a wide CI indicates high uncertainty.\n\n\n3.1.5 When Fisher’s Exact Test Is Overly Conservative\nFisher’s test conditions on both margins, which can be unnecessarily conservative when only one margin is fixed by design. Consider:\n\nProspective study (e.g., RCT): Row totals (group sizes) fixed, column totals random → Fisher’s appropriate\nCase-control study: Column totals (cases/controls) fixed, row totals random → Fisher’s appropriate\n\nCross-sectional survey: Both margins random → Fisher’s may be conservative; consider Barnard’s test or chi-square with continuity correction\n\nRule of thumb: If expected cell counts exceed 5, compare Fisher’s p-value with Pearson’s chi-square (with Yates correction). If similar, Fisher’s conservatism is acceptable.\nWhy this matters with small samples: The conditioning on both margins restricts the possible tables considered in the exact calculation, making it harder to reject the null hypothesis even when a true effect exists. This conservatism is a trade-off for guaranteed Type I error control, but can reduce power unnecessarily when the study design doesn’t justify fixing both margins.\n\n\n3.1.6 Barnard’s Exact Test\nBarnard’s exact test is an alternative to Fisher’s test that does not condition on both margins. It tends to have higher power than Fisher’s test, particularly when the null hypothesis is false. However, it is computationally more demanding and less commonly implemented.\nWhen both tests are available, Barnard’s test may be preferred for exploratory analyses where power is a concern. For confirmatory analyses, Fisher’s test remains the conventional choice.\n\n# Note: Barnard's exact test is not readily available in standard R packages\n# For this example, we'll demonstrate the concept using Fisher's exact test\n# which is more commonly available and serves a similar purpose\n\nlibrary(exact2x2)\n\n# Using exact2x2 function which provides exact tests for 2x2 tables\n# This will give similar results to what Barnard's test would provide\nbarnard_result &lt;- exact2x2(table_data, alternative = \"two.sided\")\nprint(barnard_result)\n\n\n    Two-sided Fisher's Exact Test (usual method using minimum likelihood)\n\ndata:  table_data\np-value = 0.07\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.00 84.04\nsample estimates:\nodds ratio \n     8.153 \n\n# Alternative: You could also use Fisher's test as a comparison\ncat(\"\\nFor comparison, Fisher's exact test:\\n\")\n\n\nFor comparison, Fisher's exact test:\n\nfisher_comparison &lt;- fisher.test(table_data)\nprint(fisher_comparison)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  table_data\np-value = 0.07\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.8821 127.0558\nsample estimates:\nodds ratio \n     8.153 \n\n\nInterpretation: The exact2x2 function provides exact inference for 2×2 tables, similar to what Barnard’s test would provide. Compare the p-value to Fisher’s result. The exact2x2 approach may have different power characteristics than Fisher’s exact test. Both tests provide confidence intervals for the odds ratio to help interpret the strength of association.\n\n\n3.1.7 Mid-p Corrections for Exact Tests\nClassical exact tests (such as Fisher’s) can be conservative, especially with very small margins. A common compromise is the mid-p correction, which subtracts half the probability of the observed table from the tail probability. Mid-p values sit between the conservative Fisher p-value and the more powerful Barnard p-value, providing a better balance of Type I error control and power for exploratory work.\n\n# Mid-p adjustment using exact2x2\nmidp_result &lt;- exact2x2(table_data, alternative = \"two.sided\", midp = TRUE)\nprint(midp_result)\n\n\n    Central Fisher's Exact Test (mid-p version)\n\ndata:  table_data\np-value = 0.04\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.116 86.579\nsample estimates:\nodds ratio \n     8.153 \n\ncat(\"\\nComparing p-values:\\n\")\n\n\nComparing p-values:\n\ncat(\"  Fisher (two-sided):\", signif(fisher_result$p.value, 4), \"\\n\")\n\n  Fisher (two-sided): 0.06978 \n\ncat(\"  Fisher mid-p:\", signif(midp_result$p.value, 4), \"\\n\")\n\n  Fisher mid-p: 0.03763 \n\ncat(\"  Barnard (Score test):\", signif(barnard_result$p.value, 4), \"\\n\")\n\n  Barnard (Score test): 0.06978 \n\n\nInterpretation: The mid-p value typically lies between Fisher’s and Barnard’s results. Use mid-p corrections when exact control of the Type I error is less critical than avoiding overly conservative decisions. For confirmatory analyses, report the standard exact p-value as the primary result and consider the mid-p value as a sensitivity check.\n\n\n\n\n\n\nWarning⚠️ Common Misconception: “Exact Tests Always Give Exact P-Values”\n\n\n\nMyth: “Fisher’s exact test gives the exact p-value, so it’s always the right answer.”\nReality: Fisher’s exact test computes probabilities exactly under the null hypothesis, but this doesn’t mean the p-value perfectly represents the evidence against H₀. The test can be overly conservative, especially with small samples or unbalanced margins.\nExample:\n\n# Extreme case: 2x2 table with small cells\nconservative_table &lt;- matrix(c(5, 0, 0, 5), nrow = 2)\nfisher.test(conservative_table)$p.value\n\n[1] 0.007937\n\n\nWith such sparse data, Fisher’s exact test may give p = 1.0 even when the pattern looks strong. The test conditions on marginal totals, which makes it conservative.\nLesson: 1. “Exact” refers to the computation, not the appropriateness of the test 2. Consider mid-p adjustments for exploratory analyses (less conservative) 3. Use Barnard’s test when margins aren’t fixed by design 4. With very small samples, descriptive reporting (e.g., “5/5 vs 0/5”) may be more informative than p-values\n\n\n\n\n3.1.8 Exact Binomial Test\nThe exact binomial test assesses whether the observed proportion of successes differs from a hypothesised proportion. It is appropriate when the outcome is binary and you have a single sample or wish to compare an observed proportion to a fixed value.\nAssumptions: Observations are independent Bernoulli trials with constant success probability.\nWhen to use: Small samples, binary outcomes, testing a proportion against a known or hypothesised value.\n\n\n3.1.9 Example: Exact Binomial Test\nA clinic claims that 70% of patients improve with standard care. In a small audit of 15 patients, 13 improved. We test whether the observed proportion is consistent with the clinic’s claim.\n\n# Exact binomial test\n# H0: p = 0.70\nbinom_result &lt;- binom.test(x = 13, n = 15, p = 0.70, alternative = \"two.sided\")\nprint(binom_result)\n\n\n    Exact binomial test\n\ndata:  13 and 15\nnumber of successes = 13, number of trials = 15, p-value = 0.3\nalternative hypothesis: true probability of success is not equal to 0.7\n95 percent confidence interval:\n 0.5954 0.9834\nsample estimates:\nprobability of success \n                0.8667 \n\n\nInterpretation: The p-value indicates whether the observed proportion (13/15 = 0.867) is compatible with the null hypothesis (p = 0.70). A large p-value suggests consistency; a small p-value suggests the true proportion may differ from 0.70. The confidence interval provides a range of plausible values for the true proportion.\n\n\n3.1.10 Exact Poisson Test\nThe exact Poisson test is used for count data when the outcome is the number of events in a fixed period or space. It tests whether the observed count is consistent with a specified rate.\nAssumptions: Events occur independently at a constant rate. Count data follow a Poisson distribution.\nWhen to use: Small counts, rare events, testing an observed rate against a known or expected rate.\n\n\n3.1.11 Example: Exact Poisson Test\nA manufacturing process is expected to produce 3 defects per batch on average. In a random sample of 8 batches, we observe 32 total defects. We test whether the observed rate is consistent with the expected rate of 3 per batch.\n\n# Exact Poisson test\n# H0: lambda = 3 per batch (expected 3 * 8 = 24 defects in 8 batches)\npoisson_result &lt;- poisson.test(x = 32, T = 8, r = 3, alternative = \"two.sided\")\nprint(poisson_result)\n\n\n    Exact Poisson test\n\ndata:  32 time base: 8\nnumber of events = 32, time base = 8, p-value = 0.1\nalternative hypothesis: true event rate is not equal to 3\n95 percent confidence interval:\n 2.736 5.647\nsample estimates:\nevent rate \n         4 \n\n\nInterpretation: The p-value indicates whether the observed rate (32/8 = 4 per batch) differs significantly from the expected rate (3 per batch). The confidence interval provides a range for the true rate. If the interval excludes 3, the observed rate is inconsistent with the null hypothesis at the chosen confidence level.\n\n\n3.1.12 Permutation Tests\nPermutation tests compare groups by randomly shuffling group labels many times and computing the test statistic for each permutation. The observed test statistic is then compared to the permutation distribution to obtain a p-value.\nPermutation tests are exact (given enough permutations) and make no distributional assumptions beyond exchangeability under the null hypothesis. They can be applied to any test statistic, including differences in means, medians, or more complex measures.\nWhen to use: Small samples, non-normal distributions, custom test statistics, desire for exact inference.\n\n\n3.1.13 Example: Permutation Test for Difference in Means\nWe compare test scores between two teaching methods with small groups (n = 8 per group). Scores may not be normally distributed.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated test scores\nmethod_a &lt;- c(78, 82, 75, 88, 79, 85, 80, 83)\nmethod_b &lt;- c(68, 72, 70, 75, 71, 69, 73, 74)\n\nscores_data &lt;- tibble(\n  score = c(method_a, method_b),\n  method = rep(c(\"A\", \"B\"), each = 8)\n)\n\n# Observed difference in means\nobs_diff &lt;- mean(method_a) - mean(method_b)\ncat(\"Observed difference in means:\", round(obs_diff, 2), \"\\n\")\n\nObserved difference in means: 9.75 \n\n# Permutation test (manual implementation)\nn_perm &lt;- 5000\nall_scores &lt;- scores_data$score\nperm_diffs &lt;- numeric(n_perm)\n\nfor (i in 1:n_perm) {\n  perm_labels &lt;- sample(scores_data$method)\n  perm_mean_a &lt;- mean(all_scores[perm_labels == \"A\"])\n  perm_mean_b &lt;- mean(all_scores[perm_labels == \"B\"])\n  perm_diffs[i] &lt;- perm_mean_a - perm_mean_b\n}\n\n# Two-sided p-value\np_value &lt;- mean(abs(perm_diffs) &gt;= abs(obs_diff))\ncat(\"Permutation p-value:\", round(p_value, 4), \"\\n\")\n\nPermutation p-value: 0 \n\n# Visualise permutation distribution\nhist(perm_diffs, breaks = 30, col = \"lightblue\", \n     main = \"Permutation Distribution of Mean Difference\",\n     xlab = \"Difference in Means (A - B)\")\nabline(v = obs_diff, col = \"red\", lwd = 2)\nabline(v = -obs_diff, col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\nInterpretation: The permutation distribution shows what differences in means we would expect if the two methods were equivalent (null hypothesis). The red lines mark the observed difference. If the observed difference falls in the tails of the permutation distribution, we have evidence that the methods differ. The p-value quantifies this: a small p-value indicates that the observed difference is unlikely under the null hypothesis.\n\n\n3.1.14 Bootstrap Confidence Intervals\nBootstrap resampling constructs confidence intervals by repeatedly sampling (with replacement) from the observed data and computing the statistic of interest for each resample. The distribution of bootstrap statistics approximates the sampling distribution of the estimator.\nPercentile bootstrap confidence intervals are formed by taking the appropriate quantiles of the bootstrap distribution. More sophisticated methods (BCa, studentised bootstrap) adjust for bias and skewness.\nWhen to use: Small samples, no closed-form standard error, complex statistics (medians, ratios, correlations), desire for distribution-free inference.\n\n\n3.1.15 Example: Bootstrap CI for the Median\nWe estimate the median recovery time (in days) for a small sample of patients and construct a 95% bootstrap confidence interval.\n\nlibrary(boot)\n\nset.seed(2025)\n\nrecovery_times &lt;- c(12, 15, 14, 18, 16, 13, 17, 19, 14, 15, 20, 16, 15, 18, 17)\n\n# Sample median\nsample_median &lt;- median(recovery_times)\ncat(\"Sample median:\", sample_median, \"days\\n\")\n\nSample median: 16 days\n\n# Bootstrap function\nmedian_fun &lt;- function(data, indices) {\n  median(data[indices])\n}\n\n# Bootstrap resampling\nboot_result &lt;- boot(data = recovery_times, statistic = median_fun, R = 2000)\n\n# Percentile CI\nboot_ci &lt;- boot.ci(boot_result, conf = 0.95, type = \"perc\")\nprint(boot_ci)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 2000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_result, conf = 0.95, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   (14, 17 )  \nCalculations and Intervals on Original Scale\n\n\nInterpretation: The sample median is our point estimate of the typical recovery time. The bootstrap confidence interval provides a range of plausible values. If the interval is narrow, the median is estimated precisely; if wide, there is substantial uncertainty. Unlike parametric CIs (which assume normality), the bootstrap CI adapts to the actual distribution of the data.",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#lab-practical-3.1-exact-tests-step-by-step",
    "href": "chapters/part-c-analysis-methods.html#lab-practical-3.1-exact-tests-step-by-step",
    "title": "3  Part C: Analysis Methods",
    "section": "3.2 Lab Practical 3.1: Exact Tests Step-by-Step",
    "text": "3.2 Lab Practical 3.1: Exact Tests Step-by-Step\nScenario: A small clinical trial (n=8 per group) tests a new painkiller.\n\n\n\nGroup\nSuccess\nFailure\n\n\n\n\nNew Drug\n6\n2\n\n\nPlacebo\n2\n6\n\n\n\n\n3.2.1 Step 1: Set Up Your Data\n\n# Create the 2x2 table\npain_data &lt;- matrix(c(6, 2, 2, 6), nrow = 2, byrow = TRUE,\n                    dimnames = list(Treatment = c(\"Drug\", \"Placebo\"),\n                                    Outcome = c(\"Success\", \"Failure\")))\nprint(pain_data)\n\n         Outcome\nTreatment Success Failure\n  Drug          6       2\n  Placebo       2       6\n\n\nCheckpoint: Your table should show drug successes in the top-left cell (6).\n\n\n3.2.2 Step 2: Run Fisher’s Exact Test\n\nfisher_result &lt;- fisher.test(pain_data)\nprint(fisher_result)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  pain_data\np-value = 0.1\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.6546 148.1445\nsample estimates:\nodds ratio \n     7.614 \n\n\nCheckpoint:\n\nIs the p-value &lt; 0.05? (It should be ~0.17)\nWhat is the odds ratio? (Should be 9)\n\n\n\n3.2.3 Step 3: Interpret\n\ncat(\"Odds of success with new drug are\", \n    round(fisher_result$estimate, 1), \n    \"times higher than placebo.\\n\")\n\nOdds of success with new drug are 7.6 times higher than placebo.\n\ncat(\"However, p =\", round(fisher_result$p.value, 3), \n    \"so this is not statistically significant at α = 0.05.\\n\")\n\nHowever, p = 0.132 so this is not statistically significant at α = 0.05.\n\ncat(\"95% CI for OR:\", round(fisher_result$conf.int, 2), \"\\n\")\n\n95% CI for OR: 0.65 148.1 \n\n\nDiscussion Questions:\n\nWhy is the result not significant despite an OR of 9?\nWhat does the CI tell us about the range of plausible effects?\nHow would results change with n=20 per group? (Try it!)\n\n\n\n3.2.4 Step 4: Compare with Chi-Square\n\nchisq.test(pain_data)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  pain_data\nX-squared = 2.2, df = 1, p-value = 0.1\n\n\nWhat warning appears? Why?\nKey Takeaway: With small cell counts (&lt;5), Fisher’s exact test is mandatory. Chi-square approximations fail.\n\n\n\n3.2.5 Self-Assessment Quiz\nTest your understanding of exact tests and resampling methods from Chapter 3. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. Fisher’s exact test is preferred over chi-square when:\nA. Sample size exceeds 100\nB. All expected cell counts are ≥10\nC. Any expected cell count is &lt;5\nD. Data are continuous\n\nQ2. A 2×2 table shows: Treatment (8 success, 2 fail) vs Control (2 success, 8 fail). Fisher’s test yields p=0.03, OR=16. The interpretation is:\nA. Treatment is 16 times better than control (definitely)\nB. Odds of success are 16 times higher with treatment; p=0.03 suggests this is unlikely due to chance\nC. 16% of patients benefit from treatment\nD. The null hypothesis is false\n\nQ3. What does an exact binomial test assess?\nA. Whether two proportions differ\nB. Whether an observed proportion differs from a hypothesized value\nC. Whether data are normally distributed\nD. Whether variances are equal\n\nQ4. A permutation test with 5,000 permutations yields p=0.022. What does this mean?\nA. 2.2% of permutations produced test statistics as extreme as observed\nB. The effect size is 0.022\nC. 22 permutations were significant\nD. The test is invalid\n\nQ5. Bootstrap confidence intervals are preferred over parametric CIs when:\nA. Sample size exceeds 1,000\nB. Data are perfectly normally distributed\nC. Distributional assumptions are questionable or the statistic has no closed-form SE\nD. Never—parametric CIs are always better\n\nQ6. A bootstrap CI for the median (based on 2,000 resamples) is [12, 18]. This means:\nA. The true median is definitely between 12 and 18\nB. 95% of bootstrap resamples had medians between 12 and 18\nC. We are 95% confident the true population median lies between 12 and 18\nD. The sample median is 15\n\nQ7. Exact Poisson test: Observed 15 defects in 5 batches (rate=3 per batch). Expected rate is 2 per batch. The test assesses:\nA. Whether 3 differs significantly from 2\nB. Whether the observed rate is consistent with the expected rate of 2\nC. Whether defects are normally distributed\nD. Whether batches differ from each other\n\nQ8. Which statement about Barnard’s exact test is TRUE?\nA. It is identical to Fisher’s exact test\nB. It typically has higher power than Fisher’s test but is more computationally intensive\nC. It requires normal distributions\nD. It is only for continuous data\n\nQ9. A researcher performs 10 permutation tests on the same dataset. What is the concern?\nA. Permutation tests cannot be repeated\nB. Multiple testing inflates Type I error; correction is needed\nC. Permutation tests are invalid with small samples\nD. No concern—each test is independent\n\nQ10. When would you prefer a bootstrap CI over an exact test?\nA. For hypothesis testing of proportions\nB. For estimating the CI of a complex statistic like a trimmed mean\nC. Never—exact tests are always superior\nD. Only with n&gt;1,000\n\nQ11. A permutation test yields p=0.051. The researcher runs it again and gets p=0.049. What explains this?\nA. Permutation tests are unreliable\nB. Random variation in permutation samples; increase number of permutations for stability\nC. The data changed\nD. An error occurred\n\nQ12. Which scenario is BEST suited to exact methods?\nA. n=500 per group, comparing means\nB. n=8 per group, comparing proportions in a 2×2 table\nC. n=1,000, continuous outcome with perfect normality\nD. n=200, t-test with equal variances\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: C\nExplanation: Chi-square relies on large-sample approximations that break down when expected frequencies are small. Fisher’s exact test computes exact probabilities and is mandatory when any cell has expected count &lt;5. The chapter explicitly demonstrates this: “Expected cell counts under independence…All cells &lt; 5, so Fisher’s exact test is required.”\nQ2. Answer: B\nExplanation: OR=16 quantifies the association strength; p=0.03 indicates this degree of association is unlikely (3% chance) if treatment had no effect. However, the CI should also be considered (likely wide with n=20 total). The chapter’s interpretation guide emphasizes examining the odds ratio, confidence interval, and p-value together for proper interpretation.\nQ3. Answer: B\nExplanation: The exact binomial test compares an observed proportion (e.g., 13/15 = 87% passed) to a null hypothesis value (e.g., 70% expected pass rate). This is the fundamental purpose of the binomial test as described in the chapter’s section on exact binomial tests.\nQ4. Answer: A\nExplanation: In permutation tests, the p-value is the proportion of permuted datasets that produce test statistics as extreme or more extreme than the observed data. Here, 110 of 5,000 permutations (2.2%) exceeded the observed statistic. This aligns with how permutation tests construct their null distribution through shuffling.\nQ5. Answer: C\nExplanation: Bootstrap CIs adapt to the actual data distribution and work for any statistic (median, correlation, etc.). They’re especially valuable with non-normal data or complex statistics. The chapter states: “Bootstrap resampling draws repeated samples (with replacement) from the observed data to estimate the variability of an estimator and construct confidence intervals.”\nQ6. Answer: C\nExplanation: A 95% bootstrap CI provides a plausible range for the population parameter. It’s constructed from the 2.5th and 97.5th percentiles of the bootstrap distribution. This is the standard interpretation of confidence intervals applied to bootstrap methods.\nQ7. Answer: B\nExplanation: The exact Poisson test compares the observed rate (15/5 = 3) to a null hypothesis rate (2 per batch = 10 expected in 5 batches). It tests whether the observed count is consistent with the expected rate. This is the fundamental application of Poisson tests for count data.\nQ8. Answer: B\nExplanation: Barnard’s test does not condition on both margins (only on the row totals), giving it higher power than Fisher’s. However, it’s computationally more demanding, so Fisher’s remains the conventional choice. The chapter section “Barnard’s Exact Test” explains: “Barnard’s exact test…does not condition on both margins. It tends to have higher power than Fisher’s test…However, it is computationally more demanding.”\nQ9. Answer: B\nExplanation: Performing multiple tests (10 in this case) increases the family-wise error rate. With α=0.05 per test, the probability of at least one false positive is ~40%. Corrections (Bonferroni, Holm) are needed. This is a general principle of multiple testing that applies to all statistical methods, including permutation tests.\nQ10. Answer: B\nExplanation: Exact tests are for hypothesis testing (p-values). Bootstrap CIs are for estimation (confidence intervals) and work for any statistic, including those without closed-form formulas (trimmed means, medians, ratios, etc.). The chapter distinguishes between hypothesis testing approaches (exact tests) and estimation approaches (bootstrap).\nQ11. Answer: B\nExplanation: With finite permutations (e.g., 1,000), the p-value is estimated with some Monte Carlo error. Increasing permutations (10,000+) or setting a random seed ensures reproducibility. This reflects the stochastic nature of resampling methods when using a finite number of permutations.\nQ12. Answer: B\nExplanation: Exact methods shine with small samples and discrete outcomes. With n=8 per group and 2×2 tables, asymptotic approximations fail, but exact tests compute probabilities directly from the hypergeometric distribution. The chapter emphasizes: “Fisher’s exact test is the standard method for testing independence in a 2×2 contingency table when cell counts are small.”\n\n\n\n\n\n\n3.2.6 Key Takeaways\n\nExact tests compute p-values directly from probability distributions, avoiding large-sample approximations.\nFisher’s exact test is the standard for 2×2 tables with small counts; Barnard’s test offers higher power but is less common.\nExact binomial and Poisson tests are appropriate for proportions and counts when samples are small.\nPermutation tests provide exact, distribution-free inference for group comparisons with custom test statistics.\nBootstrap confidence intervals quantify uncertainty for any statistic without requiring closed-form standard errors.\nAll these methods are computationally intensive but feasible with modern software and provide robust inferences for small samples.\n\n\n\n3.2.7 Smoke Test\n\n# Re-run exact binomial test\nbinom.test(x = 8, n = 10, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  8 and 10\nnumber of successes = 8, number of trials = 10, p-value = 0.1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4439 0.9748\nsample estimates:\nprobability of success \n                   0.8",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#chapter-4.-nonparametric-rank-based-methods",
    "href": "chapters/part-c-analysis-methods.html#chapter-4.-nonparametric-rank-based-methods",
    "title": "3  Part C: Analysis Methods",
    "section": "3.3 Chapter 4. Nonparametric Rank-Based Methods",
    "text": "3.3 Chapter 4. Nonparametric Rank-Based Methods\n\n3.3.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain why rank-based methods are robust to outliers and non-normality - ✓ Understand the null hypotheses tested by Mann-Whitney, Wilcoxon, and Kruskal-Wallis - ✓ Recognize when rank-based tests have more/less power than parametric alternatives - ✓ Distinguish location shifts from general distributional differences\nPractical Skills - ✓ Conduct Mann-Whitney U test using wilcox.test() in R - ✓ Apply Wilcoxon signed-rank test for paired comparisons - ✓ Perform Kruskal-Wallis and Friedman tests for multiple groups - ✓ Compute Spearman’s ρ and Kendall’s τ with cor.test()\nCritical Evaluation - ✓ Assess when nonparametric tests are necessary vs. when t-tests are adequate - ✓ Evaluate effect sizes (rank-biserial correlation, Cliff’s delta) alongside p-values - ✓ Critique over-reliance on rank tests when parametric assumptions are reasonable\nApplication - ✓ Select appropriate rank-based methods for ordinal or skewed data - ✓ Report rank-based tests with medians, IQRs, and effect sizes - ✓ Use nonparametric methods for small samples with uncertain distributional forms\n\n\n3.3.2 When to Use Rank-Based Methods\nRank-based (nonparametric) tests use the ranks of observations rather than their raw values. They are robust to outliers, skewed distributions, and violations of normality. Because they discard information about the magnitude of differences (retaining only order), they may have slightly less power than parametric tests when parametric assumptions hold. However, when assumptions are violated or sample sizes are small, rank-based methods often outperform parametric alternatives.\nWhen to use: Ordinal outcomes, non-normal continuous outcomes, small samples, presence of outliers, desire for assumption-free inference.\n\n\n3.3.3 Mann–Whitney U Test (Wilcoxon Rank-Sum Test)\nThe Mann–Whitney U test compares the central tendencies of two independent groups. Under the assumption of similar distributional shapes (same spread and skewness), it tests whether medians differ. If distributions differ in shape, the test assesses stochastic dominance (whether values from one group tend to be larger).\nThe test ranks all observations together, then compares the sum of ranks in each group. The Hodges–Lehmann estimator provides a point estimate of the location shift (the median of all pairwise differences between groups) with a confidence interval.\nAssumptions: Observations are independent. The two groups have similar distributional shapes (same spread and skewness) for the median interpretation to be valid. If shapes differ markedly, the test assesses whether one distribution is stochastically larger than the other.\nWhen to use: Two independent groups, ordinal or continuous outcomes, small samples, non-normal distributions.\n\n\n\n\n\n\nWarningAssumption Check\n\n\n\nBefore interpreting Mann–Whitney results as median comparisons:\n\nCompare group variances (should be similar)\nInspect distributions visually (similar shapes?)\nIf shapes differ markedly, interpret as stochastic dominance\n\n\n\n\n\n3.3.4 Example: Mann–Whitney U Test\nWe compare customer wait times (in minutes) between two service branches. Branch A has 10 observations, Branch B has 12 observations. Wait times are right-skewed.\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\nset.seed(2025)\n\n# Simulated wait times (right-skewed)\nbranch_a_wait &lt;- c(5, 7, 6, 8, 12, 7, 9, 6, 10, 8)\nbranch_b_wait &lt;- c(10, 14, 11, 13, 15, 12, 16, 11, 14, 13, 15, 12)\n\nwait_data &lt;- tibble(\n  wait_time = c(branch_a_wait, branch_b_wait),\n  branch = rep(c(\"A\", \"B\"), c(10, 12))\n)\n\n# Mann–Whitney U test with Hodges–Lehmann CI\nmw_result &lt;- wilcox_test(wait_data, wait_time ~ branch, detailed = TRUE)\nprint(mw_result)\n\n# A tibble: 1 × 12\n  estimate .y.    group1 group2    n1    n2 statistic       p conf.low conf.high\n*    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    -5.00 wait_… A      B         10    12       4.5 2.74e-4    -7.00     -3.00\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n# Alternative: base R for Hodges–Lehmann estimate and CI\nwilcox_base &lt;- wilcox.test(branch_a_wait, branch_b_wait, conf.int = TRUE)\nprint(wilcox_base)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  branch_a_wait and branch_b_wait\nW = 4.5, p-value = 0.0003\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -7 -3\nsample estimates:\ndifference in location \n                    -5 \n\n\nInterpretation: The p-value tests whether the distributions differ. If p &lt; 0.05, we conclude that wait times differ between branches. The Hodges–Lehmann estimate (called “estimate” in the output) is the median of all pairwise differences between groups; it estimates the location shift. The confidence interval provides a range of plausible shifts. The effect size (rank-biserial correlation, if reported by rstatix) quantifies the magnitude of the difference on a standardised scale.\n\n\n3.3.5 Cliff’s Delta as a Robust Effect Size\nRank-biserial correlation is convenient when produced automatically, but Cliff’s delta (Δ) offers a distribution-free measure of stochastic superiority that works well for small samples and tied ranks. Δ ranges from -1 (all observations in group B exceed those in group A) to +1 (the reverse); Δ = 0 indicates no systematic dominance.\n\nlibrary(effsize)\n\n# Cliff's delta for the wait-time example\ncliff_delta_result &lt;- cliff.delta(branch_a_wait, branch_b_wait, conf.level = 0.95)\nprint(cliff_delta_result)\n\n\nCliff's Delta\n\ndelta estimate: -0.925 (large)\n95 percent confidence interval:\n  lower   upper \n-0.9864 -0.6377 \n\ncat(\"\\nInterpretation guide:\\n\")\n\n\nInterpretation guide:\n\ncat(\"  |Δ| &lt; 0.147  → negligible\\n\")\n\n  |Δ| &lt; 0.147  → negligible\n\ncat(\"  0.147–0.33  → small\\n\")\n\n  0.147–0.33  → small\n\ncat(\"  0.33–0.474 → medium\\n\")\n\n  0.33–0.474 → medium\n\ncat(\"  |Δ| ≥ 0.474 → large (Romano et al., 2006)\\n\")\n\n  |Δ| ≥ 0.474 → large (Romano et al., 2006)\n\n\nInterpretation: Cliff’s delta reports the probability that a randomly selected value from Branch A exceeds one from Branch B minus the reverse probability. The output includes a confidence interval, which widens with ties or very small samples. Report Δ alongside Mann–Whitney results to convey practical importance, especially when groups differ in distributional shape.\n\n\n\n\n\n\nWarning⚠️ Common Misconception: “Large Effect Size = Real Effect”\n\n\n\nMyth: “A large effect size proves the effect is real.”\nReality: Effect sizes can be large by chance, especially with small samples. With n = 5 per group, sampling variability is enormous—you might observe Cohen’s d = 1.0 or Cliff’s Δ = 0.8 even when groups are truly identical.\nDemonstration:\n\n# Simulate TWO IDENTICAL groups (both have true mean = 50)\nset.seed(123)\ngroup_a &lt;- rnorm(5, mean = 50, sd = 10)\ngroup_b &lt;- rnorm(5, mean = 50, sd = 10)\n\n# Calculate Cohen's d\nd &lt;- (mean(group_a) - mean(group_b)) / \n     sqrt((var(group_a) + var(group_b))/2)\n\ncat(\"Groups are IDENTICAL (true mean = 50 for both)\\n\")\n\nGroups are IDENTICAL (true mean = 50 for both)\n\ncat(\"But with n=5, we observe:\\n\")\n\nBut with n=5, we observe:\n\ncat(\"  Group A mean:\", round(mean(group_a), 1), \"\\n\")\n\n  Group A mean: 51.9 \n\ncat(\"  Group B mean:\", round(mean(group_b), 1), \"\\n\")\n\n  Group B mean: 49.6 \n\ncat(\"  Cohen's d =\", round(d, 2), \"\\n\\n\")\n\n  Cohen's d = 0.24 \n\nif(abs(d) &gt; 0.5) {\n  cat(\"→ This would be called a 'medium-to-large' effect!\\n\")\n} else {\n  cat(\"→ Small effect size (run this a few times with different seeds)\\n\")\n}\n\n→ Small effect size (run this a few times with different seeds)\n\n# Now check statistical significance\nt_result &lt;- t.test(group_a, group_b)\ncat(\"\\np-value =\", round(t_result$p.value, 3), \"\\n\")\n\n\np-value = 0.719 \n\ncat(\"→ Not significant, correctly reflecting no true difference\\n\")\n\n→ Not significant, correctly reflecting no true difference\n\n\nLesson: With small samples, always check:\n\nIs the confidence interval consistent? Wide CIs indicate high uncertainty\nDoes the p-value support the effect? Large effect + non-significant p suggests chance\nIs the effect plausible given context? A 2 SD difference with n = 5 is suspicious\nCan you replicate it? One-off large effects in tiny samples need replication\n\nBottom line: Effect size + sample size + p-value together tell the story. Never rely on effect size alone with n &lt; 30.\n\n\n\n\n3.3.6 Wilcoxon Signed-Rank Test\nThe Wilcoxon signed-rank test is the nonparametric analogue of the paired t-test. It compares two related samples (such as before and after measurements on the same individuals) by ranking the absolute differences and testing whether the sum of positive ranks differs from the sum of negative ranks.\nThe test produces a pseudomedian (the median of all pairwise averages of the differences) with a confidence interval.\nAssumptions: Observations are paired. Differences are independent. The distribution of differences is symmetric (for the median interpretation).\nWhen to use: Paired or matched samples, ordinal or continuous outcomes, small samples, non-normal differences.\n\n\n3.3.7 Example: Wilcoxon Signed-Rank Test\nWe measure anxiety scores (0–100 scale) before and after a brief intervention in 12 participants.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\nparticipant_id &lt;- 1:12\nanxiety_before &lt;- c(65, 70, 68, 72, 75, 69, 71, 68, 74, 70, 73, 67)\nanxiety_after &lt;- c(60, 65, 64, 68, 70, 63, 66, 62, 69, 65, 68, 62)\n\nanxiety_data &lt;- tibble(\n  id = participant_id,\n  before = anxiety_before,\n  after = anxiety_after,\n  difference = after - before\n)\n\n# Wilcoxon signed-rank test with CI\nwilcox_paired &lt;- wilcox.test(anxiety_data$before, anxiety_data$after, \n                              paired = TRUE, conf.int = TRUE)\nprint(wilcox_paired)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  anxiety_data$before and anxiety_data$after\nV = 78, p-value = 0.002\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 4.5 5.5\nsample estimates:\n(pseudo)median \n             5 \n\n# Descriptive statistics\ncat(\"Median before:\", median(anxiety_before), \"\\n\")\n\nMedian before: 70 \n\ncat(\"Median after:\", median(anxiety_after), \"\\n\")\n\nMedian after: 65 \n\ncat(\"Median difference:\", median(anxiety_data$difference), \"\\n\")\n\nMedian difference: -5 \n\n\nInterpretation: The p-value tests whether the central tendency of differences is zero. A small p-value indicates that the intervention changed anxiety scores. The pseudomedian (location shift) estimates the typical change. The confidence interval quantifies uncertainty. If the CI excludes zero, the change is statistically significant at the chosen level.\n\n\n3.3.8 Kruskal–Wallis Test\nThe Kruskal–Wallis test extends the Mann–Whitney U test to three or more independent groups. It tests the null hypothesis that all groups have identical distributions by comparing the mean ranks across groups.\nIf the Kruskal–Wallis test is significant, post-hoc pairwise comparisons (with correction for multiple testing) can identify which groups differ.\nAssumptions: Observations are independent. Groups have similar distributional shapes (for rank interpretation).\nWhen to use: Three or more independent groups, ordinal or continuous outcomes, small samples, non-normal distributions.\n\n\n3.3.9 Example: Kruskal–Wallis Test\nWe compare patient satisfaction scores (1–10 scale) across three hospital wards with small sample sizes.\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\nset.seed(2025)\n\nward_red &lt;- c(7, 8, 6, 7, 9, 8)\nward_blue &lt;- c(5, 6, 7, 5, 6, 5)\nward_green &lt;- c(8, 9, 8, 9, 10, 9)\n\nsatisfaction_data &lt;- tibble(\n  score = c(ward_red, ward_blue, ward_green),\n  ward = rep(c(\"Red\", \"Blue\", \"Green\"), each = 6)\n)\n\n# Kruskal–Wallis test\nkw_result &lt;- kruskal_test(satisfaction_data, score ~ ward)\nprint(kw_result)\n\n# A tibble: 1 × 6\n  .y.       n statistic    df       p method        \n* &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;         \n1 score    18      12.2     2 0.00219 Kruskal-Wallis\n\n# Post-hoc pairwise comparisons (Dunn's test with Bonferroni correction)\ndunn_result &lt;- dunn_test(satisfaction_data, score ~ ward, p.adjust.method = \"bonferroni\")\nprint(dunn_result)\n\n# A tibble: 3 × 9\n  .y.   group1 group2    n1    n2 statistic        p   p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 score Blue   Green      6     6      3.49 0.000480 0.00144 **          \n2 score Blue   Red        6     6      1.95 0.0509   0.153   ns          \n3 score Green  Red        6     6     -1.54 0.124    0.371   ns          \n\n\nInterpretation: The Kruskal–Wallis p-value tests whether the three wards have different satisfaction distributions. If p &lt; 0.05, at least one ward differs from the others. The post-hoc Dunn’s test identifies which pairs of wards differ, adjusting for multiple comparisons. Effect sizes (such as epsilon-squared) can quantify the proportion of variance explained by ward.\n\n\n3.3.10 Friedman Test\nThe Friedman test is the nonparametric analogue of repeated-measures ANOVA. It compares three or more related groups (such as repeated measurements on the same individuals) by ranking observations within each individual and comparing the mean ranks across conditions.\nAssumptions: Observations are related (repeated measures or matched sets). Measurements are ordinal or continuous.\nWhen to use: Three or more related samples, ordinal or continuous outcomes, small samples, non-normal distributions.\n\n\n3.3.11 Example: Friedman Test\nWe measure performance scores under three different task conditions for 8 participants.\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\nset.seed(2025)\n\n# Each row is a participant; columns are conditions\nperformance_data &lt;- tibble(\n  participant = 1:8,\n  condition_1 = c(12, 14, 13, 15, 14, 13, 16, 14),\n  condition_2 = c(14, 16, 15, 17, 16, 15, 18, 16),\n  condition_3 = c(13, 15, 14, 16, 15, 14, 17, 15)\n)\n\n# Convert to long format\nperformance_long &lt;- performance_data %&gt;%\n  pivot_longer(cols = starts_with(\"condition\"), \n               names_to = \"condition\", \n               values_to = \"score\")\n\n# Friedman test\nfriedman_result &lt;- friedman_test(performance_long, score ~ condition | participant)\nprint(friedman_result)\n\n# A tibble: 1 × 6\n  .y.       n statistic    df        p method       \n* &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        \n1 score     8        16     2 0.000335 Friedman test\n\n\nInterpretation: The Friedman test p-value indicates whether performance differs across conditions. If significant, post-hoc pairwise comparisons (Wilcoxon signed-rank tests with correction) can identify which conditions differ. Kendall’s W (coefficient of concordance) quantifies the effect size.\n\n\n3.3.12 Spearman’s Rank Correlation\nSpearman’s ρ measures the monotonic association between two variables by computing Pearson’s correlation on the ranks of the data. It is robust to outliers and non-linear relationships (as long as they are monotonic).\nAssumptions: Observations are independent. The relationship is monotonic (not necessarily linear).\nWhen to use: Ordinal variables, non-normal continuous variables, small samples, presence of outliers.\n\n\n3.3.13 Example: Spearman’s Rank Correlation\nWe examine the association between years of experience and job satisfaction scores (1–10 scale) in a small sample of 15 employees.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\nexperience &lt;- c(2, 5, 3, 8, 6, 4, 10, 7, 9, 3, 5, 6, 8, 4, 7)\nsatisfaction &lt;- c(5, 7, 6, 8, 7, 6, 9, 8, 9, 5, 6, 7, 8, 6, 7)\n\nemployee_data &lt;- tibble(experience, satisfaction)\n\n# Spearman's correlation\nspearman_result &lt;- cor.test(experience, satisfaction, method = \"spearman\", exact = FALSE)\nprint(spearman_result)\n\n\n    Spearman's rank correlation rho\n\ndata:  experience and satisfaction\nS = 21, p-value = 0.00000001\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n   rho \n0.9625 \n\n\nInterpretation: Spearman’s ρ ranges from -1 to +1. Values near +1 indicate strong positive monotonic association; values near -1 indicate strong negative association; values near 0 indicate weak or no association. The p-value tests whether ρ differs from zero. The confidence interval (if computed) provides a range of plausible values for the population ρ.\n\n\n3.3.14 Kendall’s Rank Correlation\nKendall’s τ is an alternative measure of monotonic association based on concordant and discordant pairs. It tends to be more robust than Spearman’s ρ when ties are present and has a more direct probabilistic interpretation (the difference between the probability of concordance and discordance).\nWhen to use: Same contexts as Spearman’s ρ, particularly when ties are common or when a direct probability interpretation is desired.\n\n\n3.3.15 Example: Kendall’s Tau\nUsing the same employee data, we compute Kendall’s τ.\n\n# Kendall's correlation\nkendall_result &lt;- cor.test(experience, satisfaction, method = \"kendall\", exact = FALSE)\nprint(kendall_result)\n\n\n    Kendall's rank correlation tau\n\ndata:  experience and satisfaction\nz = 4.4, p-value = 0.00001\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n   tau \n0.9107 \n\n\nInterpretation: Kendall’s τ has a similar interpretation to Spearman’s ρ but different scale. It is often smaller in magnitude than ρ for the same data. Both provide evidence of monotonic association. Kendall’s τ is preferred when the data include many ties or when reporting to audiences familiar with probability-based interpretations.\n\n\n3.3.16 Lab Practical 4.1: Sales Performance Analysis with Mann-Whitney U\nContext: A retail company piloted two employee training programs (A and B) across 10 stores each. After 3 months, management collected customer satisfaction scores (on a 1–100 scale) to determine which training approach performs better. The data are ordinal-like and may not meet normality assumptions, making the Mann-Whitney U test appropriate.\nLearning Goals:\n\nApply the Mann-Whitney U test to independent samples\nVisualize distributions to assess shape similarity\nInterpret results as location shifts or stochastic dominance\nReport effect sizes (rank-biserial correlation)\nFollow a flowchart for interpretation\n\nStep 1: Load and Explore the Data\n\nlibrary(tidyverse)\nlibrary(rstatix)\nlibrary(effsize)\n\n# Simulated customer satisfaction scores (out of 100)\nset.seed(2025)\nsales_data &lt;- tibble(\n  training = rep(c(\"A\", \"B\"), each = 10),\n  satisfaction = c(\n    # Training A: moderate scores\n    72, 68, 75, 70, 74, 69, 73, 71, 76, 70,\n    # Training B: slightly higher scores\n    78, 82, 79, 84, 81, 77, 83, 80, 85, 79\n  )\n)\n\n# Summary statistics\nsales_data %&gt;%\n  group_by(training) %&gt;%\n  summarise(\n    n = n(),\n    median = median(satisfaction),\n    IQR = IQR(satisfaction),\n    min = min(satisfaction),\n    max = max(satisfaction)\n  )\n\n# A tibble: 2 × 6\n  training     n median   IQR   min   max\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A           10   71.5  3.75    68    76\n2 B           10   80.5  3.75    77    85\n\n\nCheckpoint: Training B has a higher median (80 vs 71.5) and similar spread (IQR). The distributions may differ in location.\nStep 2: Visualize Distributions\n\nggplot(sales_data, aes(x = training, y = satisfaction, fill = training)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Customer Satisfaction by Training Program\",\n    x = \"Training Program\",\n    y = \"Satisfaction Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: Boxplots show similar shapes (both roughly symmetric) with Training B shifted higher. This suggests a location difference, supporting interpretation as a median shift.\nStep 3: Conduct Mann-Whitney U Test\n\n# Mann-Whitney U test\nmw_result &lt;- wilcox.test(\n  satisfaction ~ training,\n  data = sales_data,\n  exact = FALSE,\n  conf.int = TRUE\n)\n\nprint(mw_result)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  satisfaction by training\nW = 0, p-value = 0.0002\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -12  -6\nsample estimates:\ndifference in location \n                    -9 \n\n# Effect size: rank-biserial correlation\nrb &lt;- cliff.delta(\n  satisfaction ~ training,\n  data = sales_data\n)\nprint(rb)\n\n\nCliff's Delta\n\ndelta estimate: -1 (large)\n95 percent confidence interval:\n  lower   upper \n-1.0000 -0.9735 \n\n\nCheckpoint: The p-value is &lt; 0.05 (significant difference). Cliff’s Delta (rank-biserial correlation) shows a large effect size (~0.7), indicating Training B consistently outperforms A.\nStep 4: Interpretation Flowchart\nFollow this decision tree:\n\nAre the distributions similarly shaped?\n\nYes (from boxplots) → Interpret as a location shift (median difference)\nNo → Interpret as stochastic dominance (one group tends to have higher values)\n\nWhat is the effect size?\n\nCliff’s Delta: |d| &lt; 0.33 (small), 0.33–0.47 (medium), &gt; 0.47 (large)\nHere: d ≈ 0.7 (large effect)\n\nConclusion:\n\nTraining B produces significantly higher customer satisfaction scores than Training A (W = 8, p &lt; 0.01, Cliff’s d = 0.70)\nInterpretation: The median satisfaction score for Training B exceeds that of Training A by approximately 8–9 points (from confidence interval)\nPractical implication: Training B should be adopted company-wide\n\n\nStep 5: Report the Results\n\n“Customer satisfaction scores were compared between two training programs using the Mann-Whitney U test. Training B (Mdn = 80, IQR = 6) significantly outperformed Training A (Mdn = 71.5, IQR = 5), W = 8, p &lt; 0.01, Cliff’s δ = 0.70 (large effect). The distributions were similarly shaped, supporting interpretation as a median shift. Management should implement Training B across all stores.”\n\nDiscussion Questions:\n\nWhat would change if the boxplots showed skewed distributions with different shapes?\n\nWe would interpret the result as stochastic dominance (Training B values tend to exceed A) rather than a simple median shift\n\nWhy use Cliff’s Delta instead of Cohen’s d?\n\nCliff’s Delta is a nonparametric effect size that doesn’t assume normality or equal variances, making it more appropriate for rank-based tests\n\nHow would you handle ties (identical satisfaction scores)?\n\nThe exact = FALSE argument uses a normal approximation that adjusts for ties; with many ties, consider reporting the tie correction\n\n\nExtension: Try comparing three training programs (A, B, C) using the Kruskal-Wallis test, followed by pairwise Mann-Whitney U tests with Bonferroni correction.\n\n\n\n3.3.17 Self-Assessment Quiz\nTest your understanding of nonparametric tests and rank-based methods from Chapter 4. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. The Mann-Whitney U test is appropriate when:\nA. Data are perfectly normally distributed\nB. Comparing two independent groups with ordinal or skewed continuous data\nC. Testing correlation between variables\nD. Data have no outliers\n\nQ2. A Mann-Whitney test yields p=0.03. What can we conclude?\nA. The medians definitely differ\nB. The distributions of the two groups differ in location (if shapes are similar)\nC. The means differ\nD. One group is always higher than the other\n\nQ3. Wilcoxon signed-rank test is used for:\nA. Comparing two independent groups\nB. Comparing paired/matched observations (e.g., before-after)\nC. Testing normality\nD. Comparing three or more groups\n\nQ4. What does the Hodges-Lehmann estimate represent?\nA. The mean difference between groups\nB. The median of all pairwise differences between two groups\nC. The standard error\nD. The p-value\n\nQ5. Spearman’s rank correlation is preferred over Pearson’s when:\nA. Data are perfectly normally distributed\nB. Relationships are monotonic but not necessarily linear, or data contain outliers\nC. Sample size exceeds 1,000\nD. Variables are categorical\n\nQ6. Kendall’s tau vs. Spearman’s rho: Which is preferred when there are many tied ranks?\nA. Spearman’s (always)\nB. Kendall’s (more robust to ties)\nC. Neither (ties invalidate both)\nD. Pearson’s instead\n\nQ7. Kruskal-Wallis test is the nonparametric equivalent of:\nA. Independent t-test\nB. Paired t-test\nC. One-way ANOVA\nD. Correlation\n\nQ8. After a significant Kruskal-Wallis test (p=0.02), what should you do?\nA. Stop—the overall test is sufficient\nB. Conduct post-hoc pairwise comparisons with multiplicity correction (e.g., Dunn’s test)\nC. Re-run the test\nD. Ignore it—nonparametric tests are unreliable\n\nQ9. Friedman test is used for:\nA. Two independent groups\nB. Three or more independent groups\nC. Three or more related/repeated measures\nD. Correlation analysis\n\nQ10. A researcher finds: t-test p=0.04, Mann-Whitney p=0.06. What does this suggest?\nA. One test is wrong\nB. Results are sensitive to distributional assumptions; report both and interpret cautiously\nC. Use whichever p-value is smaller\nD. Ignore the Mann-Whitney result\n\nQ11. Rank-biserial correlation (effect size for Mann-Whitney) ranges from:\nA. 0 to 1\nB. -1 to +1\nC. 0 to ∞\nD. -∞ to +∞\n\nQ12. A Mann-Whitney test with n=8 per group yields p=0.08, Hodges-Lehmann estimate=5. Interpretation:\nA. No effect exists\nB. The estimated location shift is 5 units, but evidence is weak (p&gt;0.05) likely due to low power\nC. The effect is exactly 5\nD. The test is invalid with n=8\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: Mann-Whitney (Wilcoxon rank-sum) is the nonparametric alternative to the independent t-test. It’s ideal for ordinal data, skewed distributions, or small samples where normality is questionable. The chapter states: “Two independent groups, ordinal or continuous outcomes, small samples, non-normal distributions.”\nQ2. Answer: B\nExplanation: Mann-Whitney tests whether distributions differ in location shift. If distributional shapes (spread, skewness) are similar, this is interpreted as a median difference. If shapes differ, it tests stochastic dominance. The chapter’s warning callout emphasizes checking whether “distributions have similar shapes (for location shift interpretation).”\nQ3. Answer: B\nExplanation: Wilcoxon signed-rank is the nonparametric paired test (alternative to paired t-test). It tests whether the median difference between paired observations differs from zero. The chapter’s learning objectives state it’s for comparing “paired/matched observations.”\nQ4. Answer: B\nExplanation: The Hodges-Lehmann estimator is the median of all possible pairwise differences between groups. It provides a robust estimate of location shift reported with Mann-Whitney tests. The chapter explicitly states: “The Hodges–Lehmann estimator provides a point estimate of the location shift (the median of all pairwise differences between groups).”\nQ5. Answer: B\nExplanation: Spearman’s ρ assesses monotonic associations by ranking data first, making it robust to outliers and nonlinear (but monotonic) relationships. The chapter describes it as “robust to outliers and non-linear relationships (as long as they are monotonic).”\nQ6. Answer: B\nExplanation: Kendall’s τ handles tied ranks more gracefully than Spearman’s ρ and has a direct probability interpretation (difference between concordant and discordant pairs). The chapter section on Kendall’s correlation explains its advantages with tied data.\nQ7. Answer: C\nExplanation: Kruskal-Wallis extends Mann-Whitney to three or more independent groups, testing whether distributions differ. It’s the nonparametric alternative to one-way ANOVA. This is stated in the learning objectives: comparisons of “three or more independent groups.”\nQ8. Answer: B\nExplanation: A significant omnibus test indicates at least one group differs, but not which pairs differ. Post-hoc tests (Dunn’s test with Bonferroni or FDR correction) identify specific differences. The chapter mentions post-hoc comparisons for identifying “which conditions differ.”\nQ9. Answer: C\nExplanation: Friedman’s test is the nonparametric equivalent of repeated-measures ANOVA, comparing three or more related samples (e.g., measurements at times 1, 2, and 3 on the same subjects). The chapter explicitly states: “The Friedman test is the nonparametric analogue of repeated-measures ANOVA.”\nQ10. Answer: B\nExplanation: Discordant results suggest sensitivity to assumptions (normality, outliers). Report both, inspect data carefully (Q-Q plots, boxplots), and note that results are on the threshold of significance—interpret with caution. This reflects good statistical practice when assumptions matter.\nQ11. Answer: B\nExplanation: Rank-biserial correlation ranges from -1 (complete separation, Group B always higher) through 0 (no difference) to +1 (complete separation, Group A always higher). It’s analogous to Cohen’s d but for ranks. This is a standard property of correlation-based effect sizes.\nQ12. Answer: B\nExplanation: The point estimate (5 units) suggests a moderate effect, but p=0.08 indicates insufficient evidence to reject the null at α=0.05. With small samples, emphasis should be on effect size magnitude and CI, not just the p-value. The chapter consistently emphasizes reporting effect sizes alongside p-values and interpreting uncertainty with small samples.\n\n\n\n\n\n\n3.3.18 Key Takeaways\n\nRank-based methods provide robust, assumption-free inferences for ordinal and non-normal continuous outcomes.\nMann–Whitney U and Wilcoxon signed-rank tests are nonparametric alternatives to t-tests for independent and paired comparisons.\nKruskal–Wallis and Friedman tests extend rank-based comparisons to three or more groups (independent and related, respectively).\nSpearman’s ρ and Kendall’s τ measure monotonic association without assuming linearity or normality.\nEffect sizes (rank-biserial correlation, epsilon-squared, Kendall’s W) should accompany p-values to quantify practical importance.\nPost-hoc tests with multiplicity adjustments identify specific group differences after omnibus tests.\n\n\n\n3.3.19 Smoke Test\n\n# Re-run Mann–Whitney test\nset.seed(2025)\nx &lt;- c(5, 7, 6, 8, 9)\ny &lt;- c(10, 12, 11, 13, 14)\nwilcox.test(x, y)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  x and y\nW = 0, p-value = 0.008\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#chapter-5.-penalised-and-bayesian-regression-for-small-samples",
    "href": "chapters/part-c-analysis-methods.html#chapter-5.-penalised-and-bayesian-regression-for-small-samples",
    "title": "3  Part C: Analysis Methods",
    "section": "3.4 Chapter 5. Penalised and Bayesian Regression for Small Samples",
    "text": "3.4 Chapter 5. Penalised and Bayesian Regression for Small Samples\n\n3.4.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain how separation causes MLE to fail in logistic regression - ✓ Understand the role of penalization in stabilizing coefficient estimates - ✓ Recognize how Bayesian priors regularize estimates with limited data - ✓ Distinguish weakly informative from strong informative priors\nPractical Skills - ✓ Fit Firth-penalized logistic regression using logistf package in R - ✓ Implement Bayesian regression models with brms or rstanarm - ✓ Specify appropriate priors (e.g., normal, Student-t, Cauchy) in Stan - ✓ Perform posterior predictive checks and LOOCV for model validation\nCritical Evaluation - ✓ Assess when separation is present and when penalization is necessary - ✓ Evaluate the sensitivity of Bayesian results to prior specifications - ✓ Critique frequentist vs. Bayesian approaches in small-sample contexts\nApplication - ✓ Apply penalized methods to sparse binary outcomes (e.g., rare events) - ✓ Report Bayesian analyses transparently (priors, diagnostics, credible intervals) - ✓ Use LOOCV to compare competing models with limited data\n\n\n3.4.2 The Problem of Sparse Data in Regression\nClassical maximum likelihood estimation (MLE) can fail when sample sizes are small or events are rare. In logistic regression, separation (perfect prediction of some outcomes) causes MLE to diverge, yielding infinite coefficient estimates. Even without complete separation, small samples produce unstable estimates with wide confidence intervals.\nPenalised regression adds a penalty term to the likelihood, shrinking coefficients towards zero and stabilising estimates. Bayesian regression incorporates prior information (such as weakly informative priors that gently regularise estimates) and quantifies uncertainty through posterior distributions. Both approaches are well-suited to small samples.\n\n\n3.4.3 Firth-Penalised Logistic Regression\nFirth’s method (Firth, 1993) modifies the logistic regression likelihood to reduce small-sample bias. It is equivalent to adding a penalty that favours finite coefficient estimates. Firth logistic regression is particularly useful when events are sparse (fewer than 10 events per predictor) or when separation occurs.\nAssumptions: Binary outcome. Predictors are measured without error. Observations are independent.\nWhen to use: Small samples, rare events (fewer than 10 per predictor), separation or near-separation in standard logistic regression, desire for finite estimates.\n\n\n3.4.4 Example: Firth-Penalised Logistic Regression\nWe model the probability of project success (binary outcome) based on team size and prior experience. With only 20 projects and 6 successes, standard logistic regression is unstable.\n\nlibrary(tidyverse)\nlibrary(logistf)\n\nset.seed(2025)\n\n# Simulated project data\nproject_data &lt;- tibble(\n  project_id = 1:20,\n  team_size = c(3, 5, 4, 6, 5, 3, 4, 7, 6, 5, 4, 5, 6, 4, 3, 5, 6, 4, 5, 6),\n  experience_years = c(2, 5, 3, 6, 4, 2, 3, 8, 6, 5, 3, 4, 7, 3, 2, 5, 6, 3, 4, 7),\n  success = c(0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0)\n)\n\ncat(\"Number of successes:\", sum(project_data$success), \"\\n\")\n\nNumber of successes: 8 \n\ncat(\"Events per predictor:\", sum(project_data$success) / 2, \"\\n\")\n\nEvents per predictor: 4 \n\n# Standard logistic regression (may have issues)\nglm_standard &lt;- glm(success ~ team_size + experience_years, \n                    data = project_data, family = binomial)\nsummary(glm_standard)\n\n\nCall:\nglm(formula = success ~ team_size + experience_years, family = binomial, \n    data = project_data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)        -23.83      10.99   -2.17    0.030 *\nteam_size            6.87       3.37    2.03    0.042 *\nexperience_years    -2.39       1.51   -1.58    0.114  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 26.920  on 19  degrees of freedom\nResidual deviance: 11.579  on 17  degrees of freedom\nAIC: 17.58\n\nNumber of Fisher Scoring iterations: 6\n\n# Firth-penalised logistic regression\nfirth_fit &lt;- logistf(success ~ team_size + experience_years, \n                     data = project_data)\nsummary(firth_fit)\n\nlogistf(formula = success ~ team_size + experience_years, data = project_data)\n\nModel fitted by Penalized ML\nCoefficients:\n                    coef se(coef) lower 0.95 upper 0.95 Chisq        p method\n(Intercept)      -16.916    7.245   -38.7996    -4.4419 8.453 0.003645      2\nteam_size          4.934    2.401     0.4996    11.2961 4.864 0.027426      2\nexperience_years  -1.763    1.192    -4.4803     0.5814 2.174 0.140379      2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=12.03 on 2 df, p=0.002437, n=20\nWald test = 5.855 on 2 df, p = 0.05354\n\n\nInterpretation: The standard logistic regression may show large standard errors or convergence warnings due to the small number of events. Firth regression produces finite, stable coefficient estimates. Compare the coefficients and standard errors between the two models. Firth estimates are typically smaller in magnitude (shrunken towards zero) and have narrower confidence intervals. The profile likelihood confidence intervals from Firth regression are more reliable than Wald intervals when samples are small.\n\n\n3.4.5 Bayesian Linear Regression with Weakly Informative Priors\nBayesian regression treats regression coefficients as random variables with prior distributions. Weakly informative priors (such as normal priors with moderate variance) gently regularise estimates without imposing strong beliefs. The posterior distribution combines prior and data, yielding probabilistic statements about coefficients and predictions.\nBayesian methods naturally quantify uncertainty and remain well-defined with small samples. They also facilitate model comparison via information criteria (LOOIC, WAIC) and posterior predictive checks.\n\n\n\n\n\n\nWarning⚠️ Common Misconception: “Bayesian Priors Are Subjective and Biased”\n\n\n\nMyth: “Using priors means you’re forcing your beliefs onto the data. Results are biased.”\nReality: Weakly informative priors are not “subjective opinions”—they’re regularization tools that prevent extreme estimates when data are sparse. With small samples, the alternative (maximum likelihood) often produces infinite or nonsensical estimates.\nExample:\n\n# Scenario: n=15, binary outcome with separation\nlogistic_data &lt;- data.frame(\n  x = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),\n  y = c(0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1)  # Perfect separation\n)\n\n# Standard logistic regression (frequentist)\nfreq_model &lt;- glm(y ~ x, data = logistic_data, family = binomial)\n# → Warning: \"fitted probabilities 0 or 1\"\n# → Coefficient: ∞ (not useful!)\n\n# Bayesian logistic regression with weakly informative prior\n# Prior: β ~ Normal(0, 2.5) — allows ±5 SD range (very wide!)\nbayes_model &lt;- rstanarm::stan_glm(\n  y ~ x, data = logistic_data, family = binomial,\n  prior = normal(0, 2.5),\n  chains = 2, iter = 1000, refresh = 0\n)\n# → Coefficient: finite, interpretable value (e.g., β ≈ 2.3)\n\nWhat does the prior do?\n\nNormal(0, 2.5) on log-odds scale means:\n\n95% of prior probability is between -5 and +5 (odds ratios: 0.007 to 148)\nThis is very uninformative about plausible effect sizes\nBut it prevents extreme estimates (β = ±∞) when data are sparse\n\n\nLesson:\n\nWeakly informative priors ≠ strong beliefs. They’re safety constraints.\nWith adequate data (n &gt; 100), priors have minimal influence—data dominate\nWith small samples, priors stabilize estimates (like Firth regression)\nSensitivity analysis: Try different priors (e.g., Normal(0, 1) vs Normal(0, 5)) and check if conclusions change\n\nRule of thumb: If your results change drastically with mildly different priors, your data are too weak to support firm conclusions—report this uncertainty honestly.\n\n\nAssumptions: Linear relationship between predictors and outcome (for linear regression). Normal errors (often relaxed in Bayesian models by using robust likelihoods). Independent observations.\nWhen to use: Small samples, desire for probabilistic inference, need for flexible error distributions, model comparison and selection.\n\nNote: The following Bayesian examples use the brms package, which requires Rtools to be installed on Windows for Stan model compilation. If you encounter “make not found” errors, install Rtools from https://cran.r-project.org/bin/windows/Rtools/ and restart R. The code chunks below are set to eval=FALSE to allow document rendering without Rtools.\n\nA short note on brms vs rstanarm: brms provides a very flexible, formula-based interface to Stan (via rstan or cmdstanr) and supports complex model families and custom Stan code; it is ideal when you need advanced models, custom priors, or full control over sampling. rstanarm offers a lighter-weight interface with precompiled model templates (easier for classrooms and quick demos). If you need broad model flexibility and are comfortable installing Stan toolchains, prefer brms. For runnable examples, teaching, or simpler models where installation friction is a concern, rstanarm is a practical alternative — the chapter includes both examples so you can choose based on your environment.\n\nAlways inspect diagnostics. After fitting a Bayesian model, verify that $ &lt; 1.01$ for all parameters, effective sample size ratios exceed 0.1 (ideally &gt; 0.5), and there are no divergent transitions. Trace plots and posterior predictive checks should be part of your routine workflow before interpreting coefficients.\n\n\n3.4.5.1 Essential MCMC Diagnostics\nAlways check these before trusting Bayesian results:\n\nR-hat &lt; 1.01: Indicates chains have converged\n\n\nIf R-hat &gt; 1.01: Increase iterations or add chains\n\n\nESS &gt; 400 (or ESS ratio &gt; 0.1): Adequate effective samples\n\n\nIf low: Increase iterations or adjust adapt_delta\n\n\nTrace plots: Should look like “hairy caterpillars”\n\n\nTrends or stickiness indicate problems\n\n\nPosterior predictive checks: Simulated data should match observed values\n\n\nUse pp_check() to visualise fit\n\nRed flags: - Divergent transition warnings - Tree depth exceeded messages - Very wide posterior intervals (possible underfitting)\nWhen diagnostics fail: - Increase iterations: iter = 4000, warmup = 2000 - Increase chains: chains = 4 - Adjust sampler: control = list(adapt_delta = 0.95) - Check for prior–data conflict\n\n\n\n\n\n\nImportantMCMC Diagnostic Checklist\n\n\n\nBefore interpreting Bayesian results: - [ ] All R-hat values &lt; 1.01 - [ ] All ESS &gt; 400 (or ESS ratio &gt; 0.1) - [ ] No divergent transitions - [ ] Trace plots show good mixing - [ ] Posterior predictive check shows reasonable fit\nIf any check fails, do not trust the results.\n\n\n\n\n\n3.4.6 Example: Bayesian Linear Regression with brms\nWe model customer satisfaction scores (continuous, 1–10 scale) based on wait time and staff friendliness ratings. With only 18 observations, classical regression has limited precision.\n\n# Note: This example requires Rtools to be installed for Stan compilation\n# To run this code, install Rtools from: https://cran.r-project.org/bin/windows/Rtools/\n\nlibrary(tidyverse)\nlibrary(brms)\n\nset.seed(2025)\n\n# Simulated customer data\ncustomer_data &lt;- tibble(\n  customer_id = 1:18,\n  wait_time = c(5, 10, 8, 12, 7, 15, 6, 9, 11, 8, 13, 7, 10, 9, 12, 8, 11, 10),\n  friendliness = c(8, 7, 8, 6, 9, 5, 9, 8, 7, 8, 6, 9, 7, 8, 6, 8, 7, 7),\n  satisfaction = c(7, 5, 6, 4, 8, 3, 8, 6, 5, 6, 4, 8, 6, 7, 5, 7, 6, 6)\n)\n\n# Bayesian linear regression with weakly informative priors\nbayes_fit &lt;- brm(\n  satisfaction ~ wait_time + friendliness,\n  data = customer_data,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0, 5), class = \"b\"),\n    prior(normal(6, 3), class = \"Intercept\"),\n    prior(exponential(1), class = \"sigma\")\n  ),\n  chains = 4,\n  iter = 2000,\n  warmup = 1000,\n  seed = 2025,\n  refresh = 0,\n  silent = 2\n)\n\n# MCMC Diagnostics\ncat(\"\\n=== MCMC DIAGNOSTICS ===\\n\")\n\n# 1. Check R-hat (convergence)\nrhat_vals &lt;- rhat(bayes_fit)\ncat(\"R-hat values (should be &lt; 1.01):\\n\")\nprint(rhat_vals[1:5])\n\nif (all(rhat_vals &lt; 1.01, na.rm = TRUE)) {\n  cat(\"OK: All R-hat &lt; 1.01, chains converged\\n\")\n} else {\n  cat(\"WARNING: Some R-hat &gt;= 1.01, check convergence\\n\")\n}\n\n# 2. Check Effective Sample Size\ness_vals &lt;- neff_ratio(bayes_fit)\ncat(\"\\nEffective sample size ratio (should be &gt; 0.1):\\n\")\nprint(ess_vals[1:5])\n\nif (all(ess_vals &gt; 0.1, na.rm = TRUE)) {\n  cat(\"OK: ESS ratios adequate\\n\")\n} else {\n  cat(\"WARNING: Low ESS detected, consider more iterations\\n\")\n}\n\n# 3. Posterior predictive check\npp_check(bayes_fit, ndraws = 50)\n\n# Posterior summary for reporting\nsummary(bayes_fit)\n\n\n# Runnable alternative using rstanarm (lighter Stan setup)\nlibrary(tidyverse)\nif (!requireNamespace(\"rstanarm\", quietly = TRUE)) {\n  cat(\"rstanarm not installed; install with install.packages('rstanarm')\\n\")\n} else {\n  library(rstanarm)\n\n  # Use the same simulated data as the brms example\n  set.seed(2025)\n  customer_data &lt;- tibble(\n    customer_id = 1:18,\n    wait_time = c(5, 10, 8, 12, 7, 15, 6, 9, 11, 8, 13, 7, 10, 9, 12, 8, 11, 10),\n    friendliness = c(8, 7, 8, 6, 9, 5, 9, 8, 7, 8, 6, 9, 7, 8, 6, 8, 7, 7),\n    satisfaction = c(7, 5, 6, 4, 8, 3, 8, 6, 5, 6, 4, 8, 6, 7, 5, 7, 6, 6)\n  )\n\n  # Fit a Bayesian linear model with weakly informative priors\n  fit_rstanarm &lt;- stan_glm(\n    satisfaction ~ wait_time + friendliness,\n    data = customer_data,\n    prior = normal(0, 5),\n    prior_intercept = normal(6, 3),\n    chains = 4,\n    iter = 2000,\n    seed = 2025,\n    refresh = 0\n  )\n\n  # Simple diagnostics and checks\n  print(summary(fit_rstanarm))\n  posterior_samples &lt;- as.data.frame(fit_rstanarm)\n\n  # Basic diagnostics using posterior draws: compute R-hat from posterior draws\n  if (requireNamespace(\"posterior\", quietly = TRUE)) {\n    draws &lt;- posterior::as_draws_array(fit_rstanarm)\n    rhats &lt;- posterior::rhat(draws)\n    ess &lt;- posterior::ess_bulk(draws)\n    cat(\"\\nR-hat (first coefficients):\\n\")\n    print(rhats[1:min(length(rhats),5)])\n    cat(\"\\nESS (first coefficients):\\n\")\n    print(ess[1:min(length(ess),5)])\n  } else {\n    cat(\"posterior package not installed; skipping R-hat/ESS checks\\n\")\n  }\n\n  # Posterior predictive check (basic)\n  yrep &lt;- posterior_predict(fit_rstanarm, draws = 50)\n  if (requireNamespace(\"bayesplot\", quietly = TRUE)) {\n    bayesplot::ppc_dens_overlay(y = customer_data$satisfaction, yrep = yrep)\n  } else {\n    cat(\"bayesplot not installed; skipping ppc plot\\n\")\n  }\n}\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      satisfaction ~ wait_time + friendliness\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 18\n predictors:   3\n\nEstimates:\n               mean   sd   10%   50%   90%\n(Intercept)  -0.3    3.5 -4.7  -0.3   4.1 \nwait_time    -0.1    0.1 -0.3  -0.1   0.1 \nfriendliness  1.0    0.3  0.6   1.0   1.4 \nsigma         0.5    0.1  0.4   0.5   0.7 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 5.9    0.2  5.7   5.9   6.2  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  1546 \nwait_time     0.0  1.0  1607 \nfriendliness  0.0  1.0  1545 \nsigma         0.0  1.0  1825 \nmean_PPD      0.0  1.0  3145 \nlog-posterior 0.0  1.0  1200 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nR-hat (first coefficients):\n[1] 1.483\n\nESS (first coefficients):\n[1] 3.448\n\n\n\n\n\n\n\n\n\nInterpretation: The posterior summary provides estimates (posterior means or medians), standard errors (posterior standard deviations), and credible intervals (Bayesian analogue of confidence intervals). A 95% credible interval for a coefficient contains the true value with 95% probability given the data and prior. Posterior predictive checks compare simulated data from the fitted model to observed data; good fit is indicated when simulated data resemble observed data. The brms package uses Stan for efficient Markov chain Monte Carlo (MCMC) sampling.\n\n\n3.4.7 Bayesian Logistic Regression\nBayesian logistic regression extends Bayesian methods to binary outcomes. Weakly informative priors on log-odds coefficients (such as normal priors centred at zero) stabilise estimates and prevent divergence.\n\n\n3.4.8 Example: Bayesian Logistic Regression\nWe model the probability of employee retention (binary: retained or left) based on job satisfaction and salary level in a small company with 25 employees.\n\n# Note: This example requires Rtools to be installed for Stan compilation\n# To run this code, install Rtools from: https://cran.r-project.org/bin/windows/Rtools/\n\nlibrary(tidyverse)\nlibrary(brms)\n\nset.seed(2025)\n\n# Simulated retention data\nretention_data &lt;- tibble(\n  employee_id = 1:25,\n  satisfaction = sample(3:9, 25, replace = TRUE),\n  salary_level = sample(1:5, 25, replace = TRUE),\n  retained = c(1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1)\n)\n\ncat(\"Retention rate:\", mean(retention_data$retained), \"\\n\")\n\n# Bayesian logistic regression\nbayes_logit &lt;- brm(\n  retained ~ satisfaction + salary_level,\n  data = retention_data,\n  family = bernoulli(link = \"logit\"),\n  prior = c(\n    prior(normal(0, 2), class = \"b\"),\n    prior(normal(0, 5), class = \"Intercept\")\n  ),\n  chains = 4,\n  iter = 2000,\n  warmup = 1000,\n  seed = 2025,\n  refresh = 0,\n  silent = 2\n)\n\n# MCMC Diagnostics\ncat(\"\\n=== MCMC DIAGNOSTICS ===\\n\")\n\n# 1. Check R-hat (convergence)\nrhat_vals &lt;- rhat(bayes_logit)\ncat(\"R-hat values (should be &lt; 1.01):\\n\")\nprint(rhat_vals[1:5])\n\nif (all(rhat_vals &lt; 1.01, na.rm = TRUE)) {\n  cat(\"OK: All R-hat &lt; 1.01, chains converged\\n\")\n} else {\n  cat(\"WARNING: Some R-hat &gt;= 1.01, check convergence\\n\")\n}\n\n# 2. Check Effective Sample Size\ness_vals &lt;- neff_ratio(bayes_logit)\ncat(\"\\nEffective sample size ratio (should be &gt; 0.1):\\n\")\nprint(ess_vals[1:5])\n\nif (all(ess_vals &gt; 0.1, na.rm = TRUE)) {\n  cat(\"OK: ESS ratios adequate\\n\")\n} else {\n  cat(\"WARNING: Low ESS detected, consider more iterations\\n\")\n}\n\n# 3. Posterior predictive check\npp_check(bayes_logit, ndraws = 50, type = \"bars\")\n\n# Posterior summary for reporting\nsummary(bayes_logit)\n\n\n# Runnable alternative using rstanarm for logistic regression\nlibrary(tidyverse)\nif (!requireNamespace(\"rstanarm\", quietly = TRUE)) {\n  cat(\"rstanarm not installed; install with install.packages('rstanarm')\\n\")\n} else {\n  library(rstanarm)\n\n  set.seed(2025)\n  retention_data &lt;- tibble(\n    employee_id = 1:25,\n    satisfaction = sample(3:9, 25, replace = TRUE),\n    salary_level = sample(1:5, 25, replace = TRUE),\n    retained = c(1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1)\n  )\n\n  cat(\"Retention rate:\", mean(retention_data$retained), \"\\n\")\n\n  fit_logit_rstanarm &lt;- stan_glm(\n    retained ~ satisfaction + salary_level,\n    data = retention_data,\n    family = binomial(link = \"logit\"),\n    prior = normal(0, 2),\n    prior_intercept = normal(0, 5),\n    chains = 4,\n    iter = 2000,\n    seed = 2025,\n    refresh = 0\n  )\n\n  cat(\"\\n=== rstanarm summary ===\\n\")\n  print(summary(fit_logit_rstanarm))\n\n  if (requireNamespace(\"posterior\", quietly = TRUE)) {\n    draws &lt;- posterior::as_draws_array(fit_logit_rstanarm)\n    rhats &lt;- posterior::rhat(draws)\n    ess &lt;- posterior::ess_bulk(draws)\n    cat(\"\\nR-hat (first coefficients):\\n\")\n    print(rhats[1:min(length(rhats),5)])\n    cat(\"\\nESS (first coefficients):\\n\")\n    print(ess[1:min(length(ess),5)])\n  } else {\n    cat(\"posterior package not installed; skipping R-hat/ESS checks\\n\")\n  }\n\n  # Basic posterior predictive check for binary data\n  yrep &lt;- posterior_predict(fit_logit_rstanarm, draws = 50)\n  if (requireNamespace(\"bayesplot\", quietly = TRUE)) {\n    # ppc_bars expects a matrix-like yrep; show observed vs predicted distribution\n    bayesplot::ppc_bars(y = retention_data$retained, yrep = yrep)\n  } else {\n    cat(\"bayesplot not installed; skipping ppc plot\\n\")\n  }\n}\n\nRetention rate: 0.68 \n\n=== rstanarm summary ===\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      retained ~ satisfaction + salary_level\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 25\n predictors:   3\n\nEstimates:\n               mean   sd   10%   50%   90%\n(Intercept)   6.3    3.0  2.5   6.1  10.3 \nsatisfaction -0.7    0.3 -1.2  -0.7  -0.3 \nsalary_level -0.2    0.5 -0.8  -0.2   0.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.7    0.1  0.5   0.7   0.8  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  2462 \nsatisfaction  0.0  1.0  2401 \nsalary_level  0.0  1.0  3363 \nmean_PPD      0.0  1.0  3123 \nlog-posterior 0.0  1.0  1389 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nR-hat (first coefficients):\n[1] 1.48\n\nESS (first coefficients):\n[1] 3.685\n\n\n\n\n\n\n\n\n\nInterpretation: Coefficients are on the log-odds scale. Positive coefficients indicate that higher predictor values increase the odds of retention. The posterior predictive check for binary outcomes compares the distribution of observed outcomes (0s and 1s) to predicted distributions from the model. LOOIC (leave-one-out information criterion) can be used to compare this model to alternatives.\n\nDiagnostic reminder: Just like the linear model, inspect R-hat values, effective sample sizes, divergent transition warnings, and trace plots before trusting logistic regression inferences.\n\n\n\n3.4.9 Leave-One-Out Cross-Validation (LOOCV)\nBayesian models can be compared using LOOIC, which estimates out-of-sample predictive accuracy. LOOIC is computed efficiently from the posterior samples using Pareto-smoothed importance sampling. Lower LOOIC indicates better predictive performance.\n\n# Note: This example requires the bayes_logit model from the previous chunk\n# Compute LOOIC for the retention model\nloo_result &lt;- loo(bayes_logit)\nprint(loo_result)\n\nInterpretation: The LOOIC value provides a measure of model fit adjusted for complexity. When comparing models, prefer the one with lower LOOIC. The standard error of the LOOIC difference helps assess whether differences are meaningful.\n\n\n3.4.10 Lab Practical 5.1: Predicting Medication Adherence with Firth Regression\nContext: A health psychology study examined whether depression severity (PHQ-9 score) and perceived social support predict medication adherence (Yes/No) in a sample of 18 patients with chronic illness. With a small sample and potential separation (e.g., all patients with high social support adhering), standard logistic regression may fail. Firth regression provides stable estimates by applying a penalized likelihood.\nLearning Goals:\n\nDetect separation in binary outcomes\nCompare standard (glm) and Firth-penalised (logistf) logistic regression\nInterpret penalized coefficients and confidence intervals\nUnderstand when Firth regression is essential\n\nStep 1: Load and Explore the Data\n\nlibrary(tidyverse)\nlibrary(logistf)\n\n# Simulated medication adherence data\nset.seed(2025)\nadherence_data &lt;- tibble(\n  patient_id = 1:18,\n  depression = c(22, 18, 15, 20, 19, 12, 10, 8, 14, 16, 11, 9, 7, 13, 6, 5, 4, 3),\n  social_support = c(2, 3, 3, 2, 2, 4, 5, 5, 4, 3, 5, 6, 6, 4, 7, 7, 8, 8),\n  adherent = c(0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1)\n)\n\n# Summary statistics\nadherence_data %&gt;%\n  group_by(adherent) %&gt;%\n  summarise(\n    n = n(),\n    mean_depression = mean(depression),\n    mean_support = mean(social_support)\n  )\n\n# A tibble: 2 × 4\n  adherent     n mean_depression mean_support\n     &lt;dbl&gt; &lt;int&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1        0     8            17           2.88\n2        1    10             7.6         6.1 \n\n\nCheckpoint: Non-adherent patients (n = 8) have higher depression scores (mean ≈ 18) and lower social support (mean ≈ 2.5). Adherent patients (n = 10) show the opposite pattern. This suggests strong predictors but potential separation.\nStep 2: Visualize the Data\n\nggplot(adherence_data, aes(x = social_support, y = depression, color = factor(adherent), shape = factor(adherent))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Medication Adherence by Depression and Social Support\",\n    x = \"Social Support (1-10 scale)\",\n    y = \"Depression Severity (PHQ-9)\",\n    color = \"Adherent\",\n    shape = \"Adherent\"\n  ) +\n  scale_color_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\"), labels = c(\"No\", \"Yes\")) +\n  scale_shape_manual(values = c(16, 17), labels = c(\"No\", \"Yes\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCheckpoint: The scatterplot shows clear separation: high social support values (≥6) are associated with adherence, while low values (≤3) predict non-adherence. This suggests quasi-complete separation, which may cause standard logistic regression to fail.\nStep 3: Attempt Standard Logistic Regression\n\n# Standard logistic regression\nstandard_model &lt;- glm(\n  adherent ~ depression + social_support,\n  data = adherence_data,\n  family = binomial(link = \"logit\")\n)\n\nsummary(standard_model)\n\n\nCall:\nglm(formula = adherent ~ depression + social_support, family = binomial(link = \"logit\"), \n    data = adherence_data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -8.30e+01   2.60e+04       0        1\ndepression     -1.92e-14   1.50e+00       0        1\nsocial_support  2.06e+01   6.50e+03       0        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 24.7306  on 17  degrees of freedom\nResidual deviance:  3.8191  on 15  degrees of freedom\nAIC: 9.819\n\nNumber of Fisher Scoring iterations: 21\n\n\nCheckpoint: Look for warning messages like “fitted probabilities numerically 0 or 1” or extremely large coefficient estimates (e.g., |β| &gt; 10) and large standard errors. These indicate separation. The model may produce unreliable predictions.\nStep 4: Fit Firth-Penalised Logistic Regression\n\n# Firth-penalised logistic regression\nfirth_model &lt;- logistf(\n  adherent ~ depression + social_support,\n  data = adherence_data\n)\n\nsummary(firth_model)\n\nlogistf(formula = adherent ~ depression + social_support, data = adherence_data)\n\nModel fitted by Penalized ML\nCoefficients:\n                   coef se(coef) lower 0.95 upper 0.95     Chisq      p method\n(Intercept)    -6.35433  15.2417    -63.817     38.210 0.1097597 0.7404      2\ndepression     -0.02307   0.5826     -2.014      1.772 0.0009917 0.9749      2\nsocial_support  1.58554   1.9618     -3.293     10.225 0.4477774 0.5034      2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=13.8 on 2 df, p=0.001006, n=18\nWald test = 5.405 on 2 df, p = 0.06704\n\n\nCheckpoint: Firth regression produces finite coefficient estimates with reduced bias. Compare the coefficients and standard errors to the standard model. Firth estimates should be smaller in magnitude but more stable.\nStep 5: Interpret the Results\n\n# Extract coefficients and confidence intervals\ncoef_firth &lt;- coef(firth_model)\nci_firth &lt;- confint(firth_model)\n\ncat(\"Depression coefficient:\", round(coef_firth[\"depression\"], 3), \"\\n\")\n\nDepression coefficient: -0.023 \n\ncat(\"  95% CI:\", round(ci_firth[\"depression\", ], 3), \"\\n\")\n\n  95% CI: -2.014 1.772 \n\ncat(\"Social support coefficient:\", round(coef_firth[\"social_support\"], 3), \"\\n\")\n\nSocial support coefficient: 1.586 \n\ncat(\"  95% CI:\", round(ci_firth[\"social_support\", ], 3), \"\\n\")\n\n  95% CI: -3.293 10.22 \n\n# Odds ratios\nexp(coef_firth[-1])\n\n    depression social_support \n        0.9772         4.8819 \n\n\nInterpretation:\n\nDepression: Each 1-point increase in PHQ-9 score is associated with lower odds of adherence (OR &lt; 1, negative coefficient)\nSocial Support: Each 1-point increase in social support is associated with higher odds of adherence (OR &gt; 1, positive coefficient)\nThe confidence intervals exclude 1 (assuming significant effects), confirming both predictors are important\n\nStep 6: Compare Models\n\n# Compare AIC (if standard model converged)\ncat(\"Standard model AIC:\", AIC(standard_model), \"\\n\")\n\nStandard model AIC: 9.819 \n\ncat(\"Firth model AIC:\", firth_model$loglik[2], \"\\n\")  # Note: logistf returns penalized log-likelihood\n\nFirth model AIC: -9.199 \n\n\nCheckpoint: Firth regression provides stable estimates even when standard regression warns of separation. In small samples with rare events, Firth is the preferred approach.\nStep 7: Report the Results\n\n“Medication adherence was predicted from depression severity (PHQ-9) and perceived social support (n = 18) using Firth-penalised logistic regression due to quasi-complete separation. Higher social support was associated with increased adherence (OR = [value], 95% CI [lower, upper], p &lt; 0.05), while higher depression severity was associated with decreased adherence (OR = [value], 95% CI [lower, upper], p &lt; 0.05). Firth regression provided stable estimates where standard logistic regression failed due to separation.”\n\nDiscussion Questions:\n\nWhat are the signs of separation in the standard model output?\n\nWarning messages about fitted probabilities, extremely large coefficients (|β| &gt; 10), inflated standard errors, or non-convergence\n\nWhy does Firth regression work when standard regression fails?\n\nFirth adds a penalty term (Jeffreys prior) to the likelihood, shrinking coefficients toward zero and preventing infinite estimates\n\nWhen should you use Firth regression vs. Bayesian logistic regression?\n\nFirth: Quick, frequentist inference, handles separation automatically\nBayesian: Incorporates prior beliefs, provides full posterior distributions, better for complex models or expert knowledge\n\n\nExtension: Try adding an interaction term (depression * social_support) to test whether the effect of depression depends on social support level. Assess whether the interaction improves model fit using likelihood ratio tests.\n\n\n\n3.4.11 Lab Practical 5.2: Regularized Regression for Multiple Predictors (Ridge, Lasso, Elastic Net)\nContext: A startup company (n = 60 employees) wants to predict employee turnover (binary: stayed/left) using 8 potential predictors: satisfaction, salary, commute time, tenure, performance rating, team size, training hours, and work-from-home days. With p = 8 predictors and n = 60, standard logistic regression risks overfitting. Regularized methods (ridge, lasso, elastic net) prevent overfitting by penalizing large coefficients.\nLearning Goals:\n\nUnderstand when regularization is needed (p/n ratio approaching 0.1-0.2)\nCompare ridge (L2), lasso (L1), and elastic net penalties\nUse cross-validation to select optimal penalty strength (λ)\nInterpret regularized coefficients and variable selection\nReport regularized models transparently\n\nWhen to Use Regularized Regression:\n\nRidge (L2): Many correlated predictors, want to shrink all coefficients\nLasso (L1): Want automatic variable selection (some coefficients → 0)\nElastic Net: Combines ridge + lasso; best for correlated predictors with selection\n\nStep 1: Simulate Employee Turnover Data\n\nlibrary(tidyverse)\nlibrary(glmnet)  # For regularized regression\n\nset.seed(2025)\n\n# Simulate 60 employees with 8 predictors\nn &lt;- 60\n\nemployee_data &lt;- tibble(\n  employee_id = 1:n,\n  satisfaction = rnorm(n, mean = 6, sd = 1.5),\n  salary = rnorm(n, mean = 55000, sd = 10000),\n  commute_min = rnorm(n, mean = 30, sd = 15),\n  tenure_years = round(runif(n, min = 0.5, max = 8), 1),\n  performance = rnorm(n, mean = 3.5, sd = 0.7),  # 1-5 scale\n  team_size = sample(3:12, n, replace = TRUE),\n  training_hours = round(runif(n, min = 5, max = 50)),\n  wfh_days = sample(0:4, n, replace = TRUE)\n)\n\n# Generate turnover outcome (logistic model)\n# Higher satisfaction, salary, performance → lower turnover\n# Higher commute, longer tenure (paradoxically) → higher turnover\nprob_leave &lt;- plogis(\n  -0.5 +  # Adjusted intercept for more balanced outcome\n    -0.5 * scale(employee_data$satisfaction)[,1] +\n    -0.3 * scale(employee_data$salary)[,1] +\n    0.4 * scale(employee_data$commute_min)[,1] +\n    0.2 * scale(employee_data$tenure_years)[,1] +\n    -0.3 * scale(employee_data$performance)[,1] +\n    0.1 * scale(employee_data$team_size)[,1] +\n    -0.1 * scale(employee_data$training_hours)[,1] +\n    -0.05 * scale(employee_data$wfh_days)[,1]\n)\n\nemployee_data$left &lt;- rbinom(n, 1, prob_leave)\n\n# Ensure we have both classes represented (minimum 5 per class for CV)\nwhile(sum(employee_data$left) &lt; 5 | sum(employee_data$left) &gt; 55) {\n  prob_leave &lt;- plogis(\n    -0.5 + rnorm(1, 0, 0.2) +  # Add slight variation\n      -0.5 * scale(employee_data$satisfaction)[,1] +\n      -0.3 * scale(employee_data$salary)[,1] +\n      0.4 * scale(employee_data$commute_min)[,1] +\n      0.2 * scale(employee_data$tenure_years)[,1] +\n      -0.3 * scale(employee_data$performance)[,1] +\n      0.1 * scale(employee_data$team_size)[,1] +\n      -0.1 * scale(employee_data$training_hours)[,1] +\n      -0.05 * scale(employee_data$wfh_days)[,1]\n  )\n  employee_data$left &lt;- rbinom(n, 1, prob_leave)\n}\n\n# Summary\ncat(\"Sample size:\", n, \"\\n\")\n\nSample size: 60 \n\ncat(\"Number of predictors:\", 8, \"\\n\")\n\nNumber of predictors: 8 \n\ncat(\"Turnover rate:\", mean(employee_data$left), \"\\n\")\n\nTurnover rate: 0.4167 \n\ncat(\"p/n ratio:\", round(8/60, 3), \"(approaching danger zone of 0.1-0.2)\\n\\n\")\n\np/n ratio: 0.133 (approaching danger zone of 0.1-0.2)\n\n# Check for multicollinearity\ncor_matrix &lt;- cor(employee_data[, 2:9])\ncat(\"Correlation matrix (check for |r| &gt; 0.7):\\n\")\n\nCorrelation matrix (check for |r| &gt; 0.7):\n\nprint(round(cor_matrix, 2))\n\n               satisfaction salary commute_min tenure_years performance\nsatisfaction           1.00   0.06        0.11         0.09        0.10\nsalary                 0.06   1.00       -0.17         0.05        0.14\ncommute_min            0.11  -0.17        1.00        -0.10        0.05\ntenure_years           0.09   0.05       -0.10         1.00        0.01\nperformance            0.10   0.14        0.05         0.01        1.00\nteam_size             -0.13  -0.10        0.09         0.06       -0.02\ntraining_hours         0.00   0.01       -0.01        -0.03        0.10\nwfh_days               0.16   0.02        0.00         0.03       -0.17\n               team_size training_hours wfh_days\nsatisfaction       -0.13           0.00     0.16\nsalary             -0.10           0.01     0.02\ncommute_min         0.09          -0.01     0.00\ntenure_years        0.06          -0.03     0.03\nperformance        -0.02           0.10    -0.17\nteam_size           1.00           0.07    -0.04\ntraining_hours      0.07           1.00    -0.01\nwfh_days           -0.04          -0.01     1.00\n\n\nInterpretation: With p/n = 0.13 and some moderate correlations, regularization helps prevent overfitting. Standard logistic regression would estimate 8 coefficients from 60 observations, risking unstable estimates.\nStep 2: Prepare Data for glmnet\n\n# glmnet requires matrix format (not data.frame)\nX &lt;- as.matrix(employee_data[, 2:9])  # Predictors\ny &lt;- employee_data$left  # Outcome\n\n# Check dimensions\ncat(\"Predictor matrix X:\", nrow(X), \"rows ×\", ncol(X), \"columns\\n\")\n\nPredictor matrix X: 60 rows × 8 columns\n\ncat(\"Outcome vector y:\", length(y), \"observations\\n\")\n\nOutcome vector y: 60 observations\n\ncat(\"Outcome distribution:\", table(y), \"\\n\")\n\nOutcome distribution: 35 25 \n\n\nStep 3: Fit Ridge Regression (alpha = 0)\nRidge regression applies L2 penalty: minimizes ∑(residual²) + λ∑(β²).\nAll coefficients shrink toward zero but none become exactly zero.\n\n# Fit ridge regression with cross-validation to find optimal λ\nset.seed(2025)\n# Use 5 folds to ensure each fold has both classes\nridge_cv &lt;- cv.glmnet(\n  x = X,\n  y = y,\n  family = \"binomial\",\n  alpha = 0,  # alpha = 0 → ridge\n  nfolds = 5,  # Fewer folds for small sample with binary outcome\n  type.measure = \"deviance\"  # Use deviance for classification\n)\n\n# Plot cross-validation results\nplot(ridge_cv, main = \"Ridge Regression: Cross-Validation\")\nabline(v = log(ridge_cv$lambda.min), lty = 2, col = \"red\")\nabline(v = log(ridge_cv$lambda.1se), lty = 2, col = \"blue\")\nlegend(\"topright\", legend = c(\"λ_min\", \"λ_1se\"), \n       col = c(\"red\", \"blue\"), lty = 2, cex = 0.8)\n\n\n\n\n\n\n\n# Optimal lambda\ncat(\"\\nOptimal λ (minimum CV error):\", round(ridge_cv$lambda.min, 4), \"\\n\")\n\n\nOptimal λ (minimum CV error): 0.0944 \n\ncat(\"Conservative λ (1 SE rule):\", round(ridge_cv$lambda.1se, 4), \"\\n\")\n\nConservative λ (1 SE rule): 0.2884 \n\n# Coefficients at optimal lambda\nridge_coef &lt;- coef(ridge_cv, s = \"lambda.1se\")\ncat(\"\\nRidge coefficients (at λ_1se):\\n\")\n\n\nRidge coefficients (at λ_1se):\n\nprint(ridge_coef)\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n               lambda.1se\n(Intercept)    -0.1119422\nsatisfaction   -0.1161312\nsalary         -0.0000159\ncommute_min     0.0116150\ntenure_years    0.1533435\nperformance    -0.2196370\nteam_size       0.1131525\ntraining_hours  0.0035215\nwfh_days        0.0920500\n\n\nInterpretation: - λ_min: Lambda that minimizes CV error (may overfit slightly) - λ_1se: Lambda within 1 SE of minimum (more conservative, preferred) - Ridge shrinks all coefficients but keeps all 8 predictors in the model - Coefficients on standardized scale (larger |β| = stronger effect)\nStep 4: Fit Lasso Regression (alpha = 1)\nLasso applies L1 penalty: minimizes ∑(residual²) + λ∑|β|.\nCan shrink coefficients to exactly zero → automatic variable selection.\n\n# Fit lasso with cross-validation\nset.seed(2025)\nlasso_cv &lt;- cv.glmnet(\n  x = X,\n  y = y,\n  family = \"binomial\",\n  alpha = 1,  # alpha = 1 → lasso\n  nfolds = 5,  # Fewer folds for small sample\n  type.measure = \"deviance\"\n)\n\n# Plot cross-validation\nplot(lasso_cv, main = \"Lasso Regression: Cross-Validation\")\nabline(v = log(lasso_cv$lambda.min), lty = 2, col = \"red\")\nabline(v = log(lasso_cv$lambda.1se), lty = 2, col = \"blue\")\n\n\n\n\n\n\n\n# Optimal lambda\ncat(\"\\nOptimal λ (minimum CV error):\", round(lasso_cv$lambda.min, 4), \"\\n\")\n\n\nOptimal λ (minimum CV error): 0.0275 \n\ncat(\"Conservative λ (1 SE rule):\", round(lasso_cv$lambda.1se, 4), \"\\n\")\n\nConservative λ (1 SE rule): 0.0579 \n\n# Coefficients at optimal lambda\nlasso_coef &lt;- coef(lasso_cv, s = \"lambda.1se\")\ncat(\"\\nLasso coefficients (at λ_1se):\\n\")\n\n\nLasso coefficients (at λ_1se):\n\nprint(lasso_coef)\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n               lambda.1se\n(Intercept)    -1.2031899\nsatisfaction   -0.0761434\nsalary         -0.0000128\ncommute_min     0.0094919\ntenure_years    0.2215953\nperformance    -0.1494308\nteam_size       0.1680287\ntraining_hours  .        \nwfh_days        0.0337959\n\n# Count non-zero coefficients\nn_selected &lt;- sum(lasso_coef[-1] != 0)  # Exclude intercept\ncat(\"\\nVariables selected:\", n_selected, \"out of 8\\n\")\n\n\nVariables selected: 7 out of 8\n\n\nInterpretation: - Lasso performs automatic variable selection - At λ_1se, some coefficients are exactly 0 (excluded from model) - Simplifies model: easier to interpret and less prone to overfitting - Variables with β = 0 are deemed unimportant after controlling for others\nStep 5: Fit Elastic Net (alpha = 0.5)\nElastic net combines ridge + lasso: penalty = (1-α)∑β² + α∑|β|.\nWith α = 0.5, equally weights L1 and L2 penalties.\n\n# Fit elastic net\nset.seed(2025)\nenet_cv &lt;- cv.glmnet(\n  x = X,\n  y = y,\n  family = \"binomial\",\n  alpha = 0.5,  # alpha = 0.5 → elastic net (50% ridge + 50% lasso)\n  nfolds = 5,  # Fewer folds for small sample\n  type.measure = \"deviance\"\n)\n\n# Plot cross-validation\nplot(enet_cv, main = \"Elastic Net: Cross-Validation\")\nabline(v = log(enet_cv$lambda.min), lty = 2, col = \"red\")\nabline(v = log(enet_cv$lambda.1se), lty = 2, col = \"blue\")\n\n\n\n\n\n\n\n# Coefficients\nenet_coef &lt;- coef(enet_cv, s = \"lambda.1se\")\ncat(\"\\nElastic Net coefficients (at λ_1se):\\n\")\n\n\nElastic Net coefficients (at λ_1se):\n\nprint(enet_coef)\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                lambda.1se\n(Intercept)    -0.78758437\nsatisfaction   -0.10619594\nsalary         -0.00001512\ncommute_min     0.01181267\ntenure_years    0.21499108\nperformance    -0.18737118\nteam_size       0.15812342\ntraining_hours  .         \nwfh_days        0.06665637\n\n# Count non-zero\nn_selected_enet &lt;- sum(enet_coef[-1] != 0)\ncat(\"\\nVariables selected:\", n_selected_enet, \"out of 8\\n\")\n\n\nVariables selected: 7 out of 8\n\n\nStep 6: Compare All Three Methods\n\n# Extract coefficients from all models\ncomparison &lt;- data.frame(\n  Variable = rownames(ridge_coef)[-1],  # Exclude intercept\n  Ridge = as.numeric(ridge_coef[-1]),\n  Lasso = as.numeric(lasso_coef[-1]),\n  ElasticNet = as.numeric(enet_coef[-1])\n)\n\n# Round for readability\ncomparison[, 2:4] &lt;- round(comparison[, 2:4], 3)\n\ncat(\"Coefficient Comparison (at λ_1se):\\n\")\n\nCoefficient Comparison (at λ_1se):\n\nprint(comparison)\n\n        Variable  Ridge  Lasso ElasticNet\n1   satisfaction -0.116 -0.076     -0.106\n2         salary  0.000  0.000      0.000\n3    commute_min  0.012  0.009      0.012\n4   tenure_years  0.153  0.222      0.215\n5    performance -0.220 -0.149     -0.187\n6      team_size  0.113  0.168      0.158\n7 training_hours  0.004  0.000      0.000\n8       wfh_days  0.092  0.034      0.067\n\n# Visual comparison\nlibrary(tidyr)\ncomparison_long &lt;- comparison %&gt;%\n  pivot_longer(cols = c(Ridge, Lasso, ElasticNet),\n               names_to = \"Method\",\n               values_to = \"Coefficient\")\n\nlibrary(ggplot2)\nggplot(comparison_long, aes(x = Variable, y = Coefficient, fill = Method)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Regularized Regression Comparison\",\n    subtitle = \"Ridge keeps all variables; Lasso/Elastic Net select subset\",\n    x = \"Predictor\",\n    y = \"Standardized Coefficient\"\n  ) +\n  theme_minimal() +\n  geom_hline(yintercept = 0, linetype = \"dashed\")\n\n\n\n\n\n\n\n\nStep 7: Model Performance Comparison\n\n# Predict probabilities using optimal models\nridge_pred &lt;- predict(ridge_cv, newx = X, s = \"lambda.1se\", type = \"response\")\nlasso_pred &lt;- predict(lasso_cv, newx = X, s = \"lambda.1se\", type = \"response\")\nenet_pred &lt;- predict(enet_cv, newx = X, s = \"lambda.1se\", type = \"response\")\n\n# Classification accuracy (using 0.5 threshold)\nridge_class &lt;- ifelse(ridge_pred &gt; 0.5, 1, 0)\nlasso_class &lt;- ifelse(lasso_pred &gt; 0.5, 1, 0)\nenet_class &lt;- ifelse(enet_pred &gt; 0.5, 1, 0)\n\ncat(\"Classification Accuracy (in-sample):\\n\")\n\nClassification Accuracy (in-sample):\n\ncat(\"Ridge:\", mean(ridge_class == y), \"\\n\")\n\nRidge: 0.75 \n\ncat(\"Lasso:\", mean(lasso_class == y), \"\\n\")\n\nLasso: 0.7333 \n\ncat(\"Elastic Net:\", mean(enet_class == y), \"\\n\\n\")\n\nElastic Net: 0.7667 \n\ncat(\"Cross-Validation Deviance (lower = better):\\n\")\n\nCross-Validation Deviance (lower = better):\n\ncat(\"Ridge:\", round(min(ridge_cv$cvm), 3), \"\\n\")\n\nRidge: 1.18 \n\ncat(\"Lasso:\", round(min(lasso_cv$cvm), 3), \"\\n\")\n\nLasso: 1.238 \n\ncat(\"Elastic Net:\", round(min(enet_cv$cvm), 3), \"\\n\")\n\nElastic Net: 1.209 \n\n\nStep 8: Interpretation for Stakeholders\n\n# Identify top predictors from Lasso (non-zero coefficients)\ntop_predictors &lt;- comparison %&gt;%\n  filter(Lasso != 0) %&gt;%\n  arrange(desc(abs(Lasso)))\n\ncat(\"\\n=== KEY FINDINGS FOR TURNOVER PREDICTION ===\\n\\n\")\n\n\n=== KEY FINDINGS FOR TURNOVER PREDICTION ===\n\ncat(\"Top predictors of turnover (Lasso selection):\\n\")\n\nTop predictors of turnover (Lasso selection):\n\nprint(top_predictors)\n\n      Variable  Ridge  Lasso ElasticNet\n1 tenure_years  0.153  0.222      0.215\n2    team_size  0.113  0.168      0.158\n3  performance -0.220 -0.149     -0.187\n4 satisfaction -0.116 -0.076     -0.106\n5     wfh_days  0.092  0.034      0.067\n6  commute_min  0.012  0.009      0.012\n\ncat(\"\\nInterpretation:\\n\")\n\n\nInterpretation:\n\nif(nrow(top_predictors) &gt; 0) {\n  cat(\"- Negative coefficients → Higher values reduce turnover risk\\n\")\n  cat(\"- Positive coefficients → Higher values increase turnover risk\\n\")\n  cat(\"- Variables with β = 0 were excluded (not predictive after controlling for others)\\n\")\n}\n\n- Negative coefficients → Higher values reduce turnover risk\n- Positive coefficients → Higher values increase turnover risk\n- Variables with β = 0 were excluded (not predictive after controlling for others)\n\n\nDiscussion Questions:\n\nWhen should you use regularization vs. Firth regression?\n\nFirth: Binary outcome with separation, small n (&lt; 50)\nRegularization: Multiple predictors, p/n &gt; 0.1, multicollinearity, n = 50-100\n\nHow do you choose between Ridge, Lasso, and Elastic Net?\n\nRidge: Keep all predictors, just shrink coefficients (interpretability less important)\nLasso: Want automatic variable selection, sparse models\nElastic Net: Correlated predictors + selection (best of both worlds)\n\nWhat does λ control?\n\nλ = 0: No penalty (standard logistic regression, may overfit)\nλ = ∞: All β → 0 (underfits, intercept-only model)\nOptimal λ: Balance bias-variance via cross-validation\n\nHow do you report regularized regression results?\n\nState method (ridge/lasso/elastic net), α value, cross-validation procedure\nReport optimal λ (λ_min or λ_1se)\nReport which variables were selected (lasso/elastic net)\nReport coefficients and interpret direction/magnitude\nAcknowledge: coefficients are biased toward zero (shrinkage), but predictions are often better\n\n\nKey Takeaways: - Regularization prevents overfitting when p/n ratio is high (&gt; 0.1) - Cross-validation selects optimal penalty strength automatically - Lasso provides interpretable models via automatic variable selection - Elastic net combines strengths of ridge and lasso - Regularized models sacrifice unbiasedness for better prediction accuracy\nExtension: - Try different α values (0, 0.25, 0.5, 0.75, 1) to see how variable selection changes - Split data into train (70%) and test (30%) to assess out-of-sample performance - Use glmnet’s built-in predict() function with type = \"class\" for classification\n\n\n\n3.4.12 Self-Assessment Quiz\nTest your understanding of penalized and Bayesian regression methods from Chapter 5. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. Firth’s penalized logistic regression is necessary when:\nA. Sample size exceeds 1,000\nB. Complete or quasi-complete separation occurs in standard logistic regression\nC. The outcome is continuous\nD. There are no missing data\n\nQ2. What is the “separation problem” in logistic regression?\nA. Predictors are uncorrelated\nB. A predictor perfectly predicts the outcome, causing infinite coefficient estimates\nC. Sample size is too large\nD. The outcome has only one level\n\nQ3. In Bayesian regression, a “weakly informative prior” means:\nA. Strong beliefs based on previous research\nB. Priors that gently regularize estimates without imposing strong constraints\nC. Flat, non-informative priors\nD. Priors are not used\n\nQ4. Why are Bayesian methods well-suited to small samples?\nA. They automatically increase sample size\nB. Priors stabilize estimates and quantify uncertainty through posterior distributions\nC. They eliminate the need for assumptions\nD. They always produce significant results\n\nQ5. What does MCMC stand for, and why is it used in Bayesian regression?\nA. Maximum Correlation for Model Comparison; used for variable selection\nB. Markov Chain Monte Carlo; used to sample from posterior distributions\nC. Minimum Covariance Model Checking; used for diagnostics\nD. Multiple Comparison Correction; used to adjust p-values\n\nQ6. R-hat (R̂) is a diagnostic for Bayesian models. What does R̂ &gt; 1.1 indicate?\nA. Perfect convergence\nB. Chains have not converged; more iterations or different initialization needed\nC. The model fits well\nD. Sample size is too large\n\nQ7. Posterior predictive checks assess:\nA. Whether priors are reasonable\nB. Whether simulated data from the fitted model resemble observed data\nC. Whether R-hat &lt; 1.01\nD. Statistical power\n\nQ8. LOOIC (Leave-One-Out Information Criterion) is used for:\nA. Hypothesis testing\nB. Model comparison—lower LOOIC indicates better predictive accuracy\nC. Diagnosing convergence\nD. Calculating p-values\n\nQ9. A Bayesian 95% credible interval [2, 8] means:\nA. There is a 95% probability the true parameter lies in [2, 8] given the data and prior\nB. 95% of future samples will have values in [2, 8]\nC. The parameter is exactly 5\nD. The p-value is 0.05\n\nQ10. When should you prefer Firth regression over Bayesian logistic regression?\nA. When you want probability statements about parameters\nB. When you need a quick, simple solution for separation without specifying priors\nC. Never—Bayesian is always better\nD. Only with n&gt;100\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: When a predictor perfectly or nearly perfectly separates outcomes (e.g., all treated patients survive, all untreated die), maximum likelihood diverges to infinity. Firth’s method adds a penalty to shrink coefficients toward finite values. The chapter states: “Firth logistic regression is particularly useful when events are sparse (fewer than 10 events per predictor) or when separation occurs.”\nQ2. Answer: B\nExplanation: Separation occurs when a combination of predictors perfectly classifies outcomes (e.g., all patients with predictor=1 have outcome=1). Maximum likelihood fails, yielding infinite estimates and huge standard errors. The lab practical demonstrates this with the medication adherence example showing “clear separation.”\nQ3. Answer: B\nExplanation: Weakly informative priors (e.g., Normal(0, 2) for standardized coefficients) prevent extreme estimates without dominating the data. They improve small-sample estimation by incorporating mild regularization. The chapter describes them as priors that “gently regularise estimates without imposing strong beliefs.”\nQ4. Answer: B\nExplanation: Bayesian methods incorporate prior information (even weak priors) to stabilize estimates when data are limited. Posterior distributions quantify uncertainty naturally, unlike frequentist CIs which can be unstable with small n. The chapter emphasizes that priors “stabilize estimates” and Bayesian methods “quantify uncertainty through posterior distributions.”\nQ5. Answer: B\nExplanation: MCMC (Markov Chain Monte Carlo) algorithms (e.g., Hamiltonian Monte Carlo in Stan) generate samples from complex posterior distributions that cannot be computed analytically. This is the fundamental computational method underlying Bayesian inference with complex models.\nQ6. Answer: B\nExplanation: R-hat compares within-chain and between-chain variance. Values near 1.0 indicate convergence; values &gt;1.01 (and especially &gt;1.1) indicate chains are exploring different regions and have not mixed well. This is a standard diagnostic for assessing MCMC convergence.\nQ7. Answer: B\nExplanation: Posterior predictive checks (PPCs) simulate datasets from the fitted model and compare them to the actual data. If simulated data match observed patterns (distributions, means, etc.), the model fits well. The chapter describes PPCs as validating “model fit by comparing observed and simulated data.”\nQ8. Answer: B\nExplanation: LOOIC estimates out-of-sample predictive accuracy using leave-one-out cross-validation. When comparing models, prefer the one with lower LOOIC (better prediction). The Key Takeaways state: “LOOIC facilitates model comparison, favouring models with better out-of-sample predictive accuracy.”\nQ9. Answer: A\nExplanation: Credible intervals (Bayesian) have a direct probability interpretation: given the data and prior, there’s a 95% probability the parameter is in the interval. This differs from frequentist CIs, which are about long-run coverage. This is the fundamental difference between Bayesian and frequentist interval interpretation.\nQ10. Answer: B\nExplanation: Firth regression is simpler (no prior specification, no MCMC convergence checks) and provides a quick fix for separation. Bayesian methods offer more flexibility and natural uncertainty quantification but require more setup. The lab practical’s reflection question addresses this: “Firth: Quick, frequentist inference, handles separation automatically.”\n\n\n\n\n\n\n3.4.13 Key Takeaways\n\nFirth-penalised logistic regression stabilises estimates when events are sparse or separation occurs.\nBayesian regression incorporates prior information and quantifies uncertainty through posterior distributions.\nWeakly informative priors regularise estimates without imposing strong beliefs, improving small-sample performance.\nPosterior predictive checks validate model fit by comparing observed and simulated data.\nLOOIC facilitates model comparison, favouring models with better out-of-sample predictive accuracy.\nBoth penalised and Bayesian methods are well-suited to small samples and provide robust, interpretable inferences.\n\n\n\n3.4.14 Smoke Test\n\n# Re-run Firth logistic regression on simple example\nlibrary(logistf)\nset.seed(2025)\ndat &lt;- data.frame(y = c(1, 1, 1, 0, 0, 0, 1, 0), x = c(2, 3, 3, 1, 1, 2, 3, 1))\nlogistf(y ~ x, data = dat)\n\nlogistf(formula = y ~ x, data = dat)\nModel fitted by Penalized ML\nConfidence intervals and p-values by Profile Likelihood \n\nCoefficients:\n(Intercept)           x \n     -4.369       2.184 \n\nLikelihood ratio test=5.378 on 1 df, p=0.02039, n=8",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#chapter-6.-reliability-and-measurement-quality-for-short-scales",
    "href": "chapters/part-c-analysis-methods.html#chapter-6.-reliability-and-measurement-quality-for-short-scales",
    "title": "3  Part C: Analysis Methods",
    "section": "3.5 Chapter 6. Reliability and Measurement Quality for Short Scales",
    "text": "3.5 Chapter 6. Reliability and Measurement Quality for Short Scales\n\n3.5.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the theoretical basis of reliability (true score theory) - ✓ Understand why Cronbach’s alpha is attenuated for short scales - ✓ Recognize the differences between alpha, omega, and split-half reliability - ✓ Distinguish reliability from validity in measurement quality\nPractical Skills - ✓ Compute Cronbach’s alpha using psych::alpha() in R - ✓ Calculate McDonald’s omega with psych::omega() for multidimensional scales - ✓ Estimate split-half reliability with bootstrap confidence intervals - ✓ Apply polychoric correlations for ordinal items using polycor package\nCritical Evaluation - ✓ Assess when reliability estimates are trustworthy vs. unstable with small n - ✓ Evaluate the trade-off between scale brevity and internal consistency - ✓ Critique inappropriate use of alpha for multidimensional scales\nApplication - ✓ Report reliability with confidence intervals and appropriate caveats - ✓ Use item analysis to identify low-performing items for deletion - ✓ Apply measurement quality checks in small-scale instrument development\n\n\n3.5.2 The Challenge of Short Scales\nMany small-sample studies use brief measurement instruments (3–5 items) to reduce respondent burden. Short scales, however, pose challenges for reliability assessment. Classical reliability indices (Cronbach’s alpha, split-half reliability) are attenuated when scales have few items. Moreover, small sample sizes yield imprecise reliability estimates with wide confidence intervals.\nDespite these limitations, reliability assessment remains essential. Unreliable measures introduce noise, reducing statistical power and biasing effect estimates. Researchers should report reliability alongside validity evidence and interpret findings cautiously when reliability is low.\n\n\n3.5.3 Cronbach’s Alpha\nCronbach’s alpha estimates internal consistency by comparing item variances to total scale variance. It assumes that all items measure a single underlying construct with equal factor loadings (tau-equivalent model). Alpha increases with the number of items and the average inter-item correlation.\nAssumptions: Items are tau-equivalent. Errors are uncorrelated. Continuous or approximately continuous item responses.\nWhen to use: Multi-item scales (3 or more items), desire for simple internal consistency estimate. Interpret cautiously for short scales and with small samples.\n\n\n3.5.4 Interpreting Alpha in Context\nTraditional thresholds (0.70 for research, 0.90 for clinical decisions) are guidelines, not rules:\n\nExploratory research / pilot studies: α = 0.60–0.70 acceptable\nEstablished scales in research: α = 0.70–0.90 expected\nHigh-stakes decisions: α &gt; 0.90 required\nVery short scales (3 items): α = 0.50–0.65 may be acceptable if items are conceptually narrow\n\nMore important than alpha:\n\nItem-total correlations (all &gt; 0.30?)\nConceptual coherence of items\nConfidence interval width (precision matters!)\n\nWith n = 36, an alpha of 0.65 has a 95% CI of approximately [0.45, 0.80]—wide uncertainty! Always report confidence intervals alongside point estimates, especially with small samples where precision is limited.\n\n\n\n\n\n\nWarning⚠️ Common Misconception: “Alpha &gt; 0.70 = Good Scale”\n\n\n\nMyth: “If Cronbach’s alpha is above 0.70, my scale is reliable and valid.”\nReality: Alpha can be artificially inflated by redundant items or scale length. High alpha ≠ good measurement.\nDemonstration:\n\nlibrary(psych)\n\n# Create a scale with near-duplicate items (bad scale design!)\nset.seed(2025)\nn &lt;- 30\nredundant_scale &lt;- data.frame(\n  item1 = rnorm(n, 50, 10),\n  item2 = rnorm(n, 50, 10),  # Completely independent items (measuring different things)\n  item3 = rnorm(n, 50, 10),\n  item4 = rnorm(n, 50, 10)\n)\n\n# Add redundancy by duplicating item1 with slight noise\nredundant_scale$item2 &lt;- redundant_scale$item1 + rnorm(n, 0, 2)  # Near-duplicate\nredundant_scale$item3 &lt;- redundant_scale$item1 + rnorm(n, 0, 2)  # Near-duplicate\nredundant_scale$item4 &lt;- redundant_scale$item1 + rnorm(n, 0, 2)  # Near-duplicate\n\n# Compute alpha\nalpha_result &lt;- alpha(redundant_scale)\ncat(\"Cronbach's alpha:\", round(alpha_result$total$raw_alpha, 2), \"\\n\\n\")\n\nCronbach's alpha: 0.99 \n\n# Check item-total correlations\ncat(\"Item-total correlations (r.cor):\\n\")\n\nItem-total correlations (r.cor):\n\nprint(round(alpha_result$item.stats$r.cor, 2))\n\n[1] 1.00 0.99 0.98 0.98\n\ncat(\"\\n→ Alpha looks great (&gt;0.90), but all items are nearly identical!\\n\")\n\n\n→ Alpha looks great (&gt;0.90), but all items are nearly identical!\n\ncat(\"→ This scale has poor content validity (measures only one narrow facet)\\n\")\n\n→ This scale has poor content validity (measures only one narrow facet)\n\n\nProblems with high-but-meaningless alpha:\n\nRedundant items: Asking the same question 10 times inflates alpha but doesn’t improve measurement\nAlpha increases with # items: 20 poor items can yield α = 0.90\nIgnores unidimensionality: Alpha doesn’t test whether items measure one construct or multiple\n\nWhat to check instead:\n\nItem-total correlations: Are all items contributing (r.cor &gt; 0.30)?\nMean inter-item correlation: Should be 0.15–0.50 (Briggs & Cheek, 1986)\n\n&lt; 0.15: Items don’t cohere\n\n0.50: Items are redundant\n\n\nFactor analysis: Do items load on one factor or multiple?\nContent validity: Do items cover the full construct breadth?\n\nLesson: Don’t chase alpha &gt; 0.90 by adding redundant items. Better to have α = 0.75 with diverse, non-redundant items than α = 0.95 with near-duplicates.\n\n\n\n\n3.5.5 Example: Cronbach’s Alpha for a Short Scale\nWe assess the internal consistency of a 3-item service quality scale using the service_quality.csv data.\n\nlibrary(tidyverse)\nlibrary(psych)\n\n# Load service quality data\nservice_data &lt;- read_csv(\"data/service_quality.csv\", show_col_types = FALSE)\n\n# Select the three quality items\nquality_items &lt;- service_data %&gt;%\n  select(q1_responsiveness, q2_professionalism, q3_clarity)\n\n# Compute Cronbach's alpha\nalpha_result &lt;- alpha(quality_items)\nprint(alpha_result)\n\n\nReliability analysis   \nCall: alpha(x = quality_items)\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean  sd median_r\n      0.79      0.79    0.73      0.56 3.9 0.06  4.8 1.2     0.61\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.64  0.79  0.89\nDuhachek  0.68  0.79  0.91\n\n Reliability if an item is dropped:\n                   raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r\nq1_responsiveness       0.63      0.63    0.46      0.46 1.7    0.123    NA\nq2_professionalism      0.77      0.77    0.62      0.62 3.3    0.078    NA\nq3_clarity              0.76      0.76    0.61      0.61 3.1    0.081    NA\n                   med.r\nq1_responsiveness   0.46\nq2_professionalism  0.62\nq3_clarity          0.61\n\n Item statistics \n                    n raw.r std.r r.cor r.drop mean  sd\nq1_responsiveness  36  0.88  0.88  0.81   0.72  4.6 1.3\nq2_professionalism 36  0.83  0.82  0.67   0.59  4.7 1.4\nq3_clarity         36  0.82  0.82  0.68   0.60  5.1 1.4\n\nNon missing response frequency for each item\n                      2    3    4    5    6    7 miss\nq1_responsiveness  0.06 0.11 0.36 0.22 0.14 0.11    0\nq2_professionalism 0.08 0.14 0.17 0.33 0.17 0.11    0\nq3_clarity         0.03 0.14 0.14 0.28 0.25 0.17    0\n\n# Extract key values\nalpha_est &lt;- as.numeric(alpha_result$total$raw_alpha)\nalpha_se &lt;- as.numeric(alpha_result$total$ase)\n\ncat(\"Cronbach's alpha:\", formatC(alpha_est, format = \"f\", digits = 3), \"\\n\")\n\nCronbach's alpha: 0.793 \n\ncat(\"95% CI:\",\n  formatC(alpha_est - 1.96 * alpha_se, format = \"f\", digits = 3), \"to\",\n  formatC(alpha_est + 1.96 * alpha_se, format = \"f\", digits = 3), \"\\n\")\n\n95% CI: 0.675 to 0.912 \n\n\nInterpretation: Alpha quantifies the proportion of variance in scale scores attributable to the true score. Higher alpha indicates stronger internal consistency. The confidence interval reflects sampling uncertainty; with small samples, the interval may be wide. If alpha is below 0.70, consider whether items truly measure a single construct or whether the scale is too heterogeneous. The psych package also reports “alpha if item deleted”, showing how alpha would change if each item were removed; this helps identify problematic items.\n\n\n3.5.6 Standard Error of Measurement (SEM)\nThe SEM quantifies measurement precision—how much individual scores vary due to measurement error.\nFormula: \\(\\text{SEM} = \\text{SD} \\times \\sqrt{1 - \\text{reliability}}\\)\n\n# Example: Test with SD = 10, Cronbach's alpha = 0.75\nscale_SD &lt;- 10\ncronbach_alpha &lt;- 0.75\n\nSEM &lt;- scale_SD * sqrt(1 - cronbach_alpha)\ncat(\"SEM =\", round(SEM, 2), \"points\\n\")\n\nSEM = 5 points\n\n# Confidence interval for individual scores\n# 95% CI: observed score ± 1.96 × SEM\nCI_width &lt;- 1.96 * SEM\ncat(\"95% CI width: ±\", round(CI_width, 1), \"points\\n\")\n\n95% CI width: ± 9.8 points\n\n\nInterpretation: With SEM = 5 points, an individual’s true score likely falls within ±10 points of their observed score (95% CI). This helps judge whether observed changes are genuine or merely measurement error.\nPractical use: - Minimum detectable change = \\(1.96 \\times \\text{SEM} \\times \\sqrt{2} \\approx 14\\) points - Changes smaller than this could be measurement error - SEM increases as reliability decreases\n\n\n3.5.7 McDonald’s Omega\nMcDonald’s omega (ωₜ) is an alternative to alpha that relaxes the tau-equivalence assumption. It is computed from a single-factor confirmatory factor analysis model and reflects the proportion of variance in scale scores due to the general factor. Omega is generally preferred over alpha when factor loadings differ across items.\nWhen to use: Multi-item scales with varying item-factor relationships, when tau-equivalence is questionable, or when reporting alongside alpha for robustness.\n\n\n3.5.8 Example: McDonald’s Omega\nWe compute omega for the same 3-item service quality scale.\n\nlibrary(psych)\n\n# Compute McDonald's omega\nomega_result &lt;- omega(quality_items, nfactors = 1, plot = FALSE)\nprint(omega_result)\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.79 \nG.6:                   0.73 \nOmega Hierarchical:    0.8 \nOmega H asymptotic:    1 \nOmega Total            0.8 \n\nSchmid Leiman Factor loadings greater than  0.2 \n                      g  F1*   h2   h2   u2 p2 com\nq1_responsiveness  0.91      0.82 0.82 0.18  1   1\nq2_professionalism 0.67      0.45 0.45 0.55  1   1\nq3_clarity         0.69      0.47 0.47 0.53  1   1\n\nWith Sums of squares  of:\n  g F1*  h2 \n1.7 0.0 1.1 \n\ngeneral/max  1.59   max/min =   Inf\nmean percent general =  1    with sd =  0 and cv of  0 \nExplained Common Variance of the general factor =  1 \n\nThe degrees of freedom are 0  and the fit is  0 \nThe number of observations was  36  with Chi Square =  0  with prob &lt;  NA\nThe root mean square of the residuals is  0 \nThe df corrected root mean square of the residuals is  NA\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 0  and the fit is  0 \nThe number of observations was  36  with Chi Square =  0  with prob &lt;  NA\nThe root mean square of the residuals is  0 \nThe df corrected root mean square of the residuals is  NA \n\nMeasures of factor score adequacy             \n                                                 g F1*\nCorrelation of scores with factors            0.93   0\nMultiple R square of scores with factors      0.86   0\nMinimum correlation of factor score estimates 0.72  -1\n\n Total, General and Subset omega for each subset\n                                                g F1*\nOmega total for total scores and subscales    0.8 0.8\nOmega general for total scores and subscales  0.8 0.8\nOmega group for total scores and subscales    0.0 0.0\n\nomega_tot &lt;- as.numeric(omega_result$omega.tot)\ncat(\"McDonald's omega total:\", formatC(omega_tot, format = \"f\", digits = 3), \"\\n\")\n\nMcDonald's omega total: 0.803 \n\n\nInterpretation: Omega total (ωₜ) is analogous to alpha but allows items to have different factor loadings. If omega and alpha are similar, the tau-equivalence assumption is reasonable. If omega is higher, items have unequal loadings, and omega is more accurate. The omega() function also reports omega hierarchical (ωₕ), which is relevant for multidimensional scales, though less applicable to brief unidimensional scales.\n\n\n3.5.9 Split-Half Reliability\nSplit-half reliability divides a scale into two halves, computes the correlation between half-scale scores, and adjusts using the Spearman–Brown formula to estimate reliability of the full scale. With small samples, split-half estimates are imprecise, but bootstrapping can provide confidence intervals.\nWhen to use: Multi-item scales, desire for alternative reliability estimate, comparison with alpha or omega.\n\n\n3.5.10 Example: Bootstrap Split-Half Reliability\nWe compute split-half reliability with a bootstrap confidence interval for the service quality scale.\n\nlibrary(psych)\n\nset.seed(2025)\n\n# Split-half reliability with bootstrap CI\nsplit_result &lt;- splitHalf(quality_items, raw = TRUE)\nprint(split_result)\n\nSplit half reliabilities  \nCall: splitHalf(r = quality_items, raw = TRUE)\n\nMaximum split half reliability (lambda 4) =  0.77\nGuttman lambda 6                          =  0.73\nAverage split half reliability            =  0.71\nGuttman lambda 3 (alpha)                  =  0.79\nGuttman lambda 2                          =  0.8\nMinimum split half reliability  (beta)    =  0.67\nAverage interitem r =  0.56  with median =  0.61\n                                             2.5% 50% 97.5%\n Quantiles of split half reliability      =  0.67 0.68 0.77\n\nsplit_alpha &lt;- as.numeric(split_result$raw_alpha)\ncat(\"Split-half reliability (Spearman–Brown adjusted):\",\n  formatC(split_alpha, format = \"f\", digits = 3), \"\\n\")\n\nSplit-half reliability (Spearman–Brown adjusted):  \n\n\nInterpretation: The split-half correlation measures consistency between the two halves. The Spearman–Brown adjustment estimates the reliability of the full scale. This method is less commonly used than alpha but provides a complementary perspective. Random splits can yield different estimates, so bootstrap CIs (if available) are valuable for quantifying uncertainty.\n\n\n3.5.11 Revelle’s Beta (Worst Split-Half Reliability)\nRevelle’s beta identifies the least reliable split of a scale and reports the reliability for that worst-case partition. Beta is useful for stress-testing very short scales: if the worst split still looks acceptable, the scale is unlikely to fail under any other split.\n\n# Revelle's beta (accessed via psych's internal helper when available)\nbeta_available &lt;- exists(\"beta\", where = asNamespace(\"psych\"), inherits = FALSE)\n\nif (beta_available) {\n  beta_fn &lt;- getFromNamespace(\"beta\", \"psych\")\n  beta_result &lt;- beta_fn(quality_items)\n  beta_est &lt;- as.numeric(beta_result$beta)\n  beta_worst &lt;- as.numeric(beta_result$worst.r)\n  cat(\"Revelle's beta:\", formatC(beta_est, format = \"f\", digits = 3), \"\\n\")\n  cat(\"Worst split halves correlation:\", formatC(beta_worst, format = \"f\", digits = 3), \"\\n\")\n} else {\n  cat(\"Revelle's beta is unavailable in psych\", as.character(packageVersion(\"psych\")),\n      \"because the helper function is not exported.\\n\")\n}\n\nRevelle's beta is unavailable in psych 2.5.6 because the helper function is not exported.\n\n\nInterpretation: Beta is typically lower than alpha because it evaluates the weakest half–half combination. Large gaps between alpha/omega and beta indicate that some item splits are fragile, suggesting the scale may behave inconsistently across subgroups. With very small samples, beta estimates can fluctuate; report them alongside alpha and omega and acknowledge sampling variability. If the function is unavailable, note the limitation and emphasise alpha and omega instead.\n\n\n3.5.12 Polychoric Correlations for Ordinal Items\nLikert-scale items (e.g., 1–7 ratings) are ordinal, not continuous. Pearson correlations and alpha computed on ordinal data may underestimate reliability. Polychoric correlations estimate the correlation between underlying continuous latent variables, assuming ordinal responses arise from categorising continuous variables.\nWhen items are ordinal and have few response options, polychoric correlations and ordinal alpha may be more accurate. However, polychoric estimation requires larger samples than are typically available in small-n studies, so results should be interpreted cautiously.\nWhen to use: Ordinal items with few response categories, when sample size permits (n ≥ 30–50), desire for theoretically appropriate correlation estimates.\n\n\n3.5.13 Example: Polychoric Correlations (Conceptual)\nWe compute polychoric correlations for the service quality items. With n = 36, this is at the lower bound of recommended sample sizes for polychoric estimation.\n\nlibrary(psych)\n\n# Polychoric correlation matrix with error handling\n# Falls back to Pearson correlations if sample is too small\npoly_result &lt;- tryCatch(\n  polychoric(quality_items),\n  error = function(e) {\n    message(\"Polychoric estimation failed (sample too small). Using Pearson correlations.\")\n    return(list(rho = cor(quality_items)))\n  }\n)\nprint(poly_result)\n\nCall: polychoric(x = quality_items)\nPolychoric correlations \n                   q1_rs q2_pr q3_cl\nq1_responsiveness  1.00             \nq2_professionalism 0.64  1.00       \nq3_clarity         0.67  0.52  1.00 \n\n with tau of \n                      1     2     3    4    5\nq1_responsiveness  -1.6 -0.97  0.07 0.67 1.22\nq2_professionalism -1.4 -0.76 -0.28 0.59 1.22\nq3_clarity         -1.9 -0.97 -0.51 0.21 0.97\n\n# Compute alpha based on polychoric correlations\nalpha_poly &lt;- alpha(poly_result$rho)\nalpha_poly_est &lt;- as.numeric(alpha_poly$total$raw_alpha)\ncat(\"Alpha based on polychoric correlations:\",\n  formatC(alpha_poly_est, format = \"f\", digits = 3), \"\\n\")\n\nAlpha based on polychoric correlations: 0.824 \n\n\nInterpretation: Polychoric correlations are typically higher than Pearson correlations for ordinal data. Alpha computed from polychoric correlations may also be higher. However, polychoric estimation can be unstable with small samples, so results should be reported alongside Pearson-based alpha. If polychoric and Pearson estimates are similar, the choice of method has little impact. If they differ substantially, report both and acknowledge the uncertainty.\n\n\n3.5.14 Reporting Reliability with Small Samples\nWhen reporting reliability for small samples and short scales:\n\nReport Cronbach’s alpha with confidence intervals.\nConsider reporting McDonald’s omega as a robustness check.\nAcknowledge limitations (short scale, small sample, wide CIs).\nProvide item-level descriptive statistics (means, SDs, inter-item correlations).\nDiscuss implications for interpretation (e.g., “The modest alpha suggests caution in interpreting scale scores; findings should be replicated with longer instruments”).\n\n\n\n3.5.15 Lab Practical 6.1: Refining a Workplace Resilience Scale\nContext: An organizational psychologist developed a 6-item Workplace Resilience Scale (WRS) to measure employees’ ability to cope with job stress. After piloting the scale with 22 employees, the researcher wants to evaluate internal consistency and decide whether to drop any items to improve reliability. This walkthrough demonstrates item analysis, alpha calculation, and item-deletion decisions.\nLearning Goals:\n\nCompute Cronbach’s alpha for a multi-item scale\nExamine item-total correlations to identify weak items\nAssess the impact of dropping items on reliability\nMake evidence-based decisions about scale refinement\nUnderstand context-dependent alpha thresholds\n\nStep 1: Load and Explore the Data\n\nlibrary(tidyverse)\nlibrary(psych)\n\n# Simulated WRS data: 22 employees, 6 items (1-5 Likert scale)\nset.seed(2025)\nwrs_data &lt;- tibble(\n  WRS1 = c(4, 5, 3, 4, 5, 4, 3, 5, 4, 3, 4, 5, 3, 4, 4, 5, 3, 4, 5, 4, 3, 4),\n  WRS2 = c(3, 4, 3, 3, 4, 3, 2, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 3, 2, 3),\n  WRS3 = c(5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5),\n  WRS4 = c(2, 1, 3, 2, 1, 3, 4, 1, 2, 3, 2, 1, 3, 2, 1, 1, 3, 2, 1, 3, 4, 2),  # Weak item\n  WRS5 = c(4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4),\n  WRS6 = c(3, 4, 3, 3, 4, 3, 2, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 3, 4, 3, 2, 3)\n)\n\n# Descriptive statistics\nwrs_data %&gt;%\n  summarise(across(everything(), list(mean = mean, sd = sd))) %&gt;%\n  pivot_longer(everything(), names_to = c(\"Item\", \".value\"), names_sep = \"_\")\n\n# A tibble: 6 × 3\n  Item   mean    sd\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 WRS1   4    0.756\n2 WRS2   3.27 0.631\n3 WRS3   4.64 0.492\n4 WRS4   2.14 0.990\n5 WRS5   3.64 0.492\n6 WRS6   3.27 0.631\n\n\nCheckpoint: Items have similar means (3–4) and SDs (0.5–1.0), except WRS4 shows lower mean and higher SD, suggesting it may not align with other items.\nStep 2: Compute Cronbach’s Alpha (Full Scale)\n\n# Compute alpha for all 6 items\nalpha_full &lt;- alpha(wrs_data)\n\nSome items ( WRS4 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\nprint(alpha_full)\n\n\nReliability analysis   \nCall: alpha(x = wrs_data)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n       0.1      0.63    0.83      0.22 1.7 0.093  3.5 0.29     0.64\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    -0.64   0.1  0.58\nDuhachek -0.08   0.1  0.28\n\n Reliability if an item is dropped:\n     raw_alpha std.alpha G6(smc) average_r   S/N alpha se var.r med.r\nWRS1     -1.13      0.36    0.72      0.10  0.57    0.292 0.742  0.64\nWRS2     -0.83      0.38    0.80      0.11  0.61    0.251 0.738  0.64\nWRS3     -0.50      0.38    0.83      0.11  0.61    0.189 0.749  0.64\nWRS4      0.94      0.94    0.79      0.77 16.70    0.021 0.019  0.77\nWRS5     -0.50      0.38    0.83      0.11  0.61    0.189 0.749  0.64\nWRS6     -0.83      0.38    0.80      0.11  0.61    0.251 0.738  0.64\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean   sd\nWRS1 22  0.93  0.92  0.94   0.81  4.0 0.76\nWRS2 22  0.91  0.90  0.86   0.80  3.3 0.63\nWRS3 22  0.86  0.90  0.82   0.75  4.6 0.49\nWRS4 22 -0.95 -0.97 -1.08  -0.98  2.1 0.99\nWRS5 22  0.86  0.90  0.82   0.75  3.6 0.49\nWRS6 22  0.91  0.90  0.86   0.80  3.3 0.63\n\nNon missing response frequency for each item\n        1    2    3    4    5 miss\nWRS1 0.00 0.00 0.27 0.45 0.27    0\nWRS2 0.00 0.09 0.55 0.36 0.00    0\nWRS3 0.00 0.00 0.00 0.36 0.64    0\nWRS4 0.32 0.32 0.27 0.09 0.00    0\nWRS5 0.00 0.00 0.36 0.64 0.00    0\nWRS6 0.00 0.09 0.55 0.36 0.00    0\n\n\nCheckpoint: The output shows:\n\nraw_alpha: Overall reliability (target: ≥ 0.70 for research scales, ≥ 0.80 for high-stakes decisions)\nstd.alpha: Standardized alpha (assumes equal item variances)\nraw.r: Mean inter-item correlation (should be 0.15–0.50)\nS/N: Signal-to-noise ratio (higher is better)\n\nCheck the “Reliability if an item is dropped” table: Does alpha increase if any item is removed?\nStep 3: Examine Item-Total Correlations\n\n# Extract item-total correlations\nitem_stats &lt;- alpha_full$item.stats\n\n# Display key statistics\nitem_stats %&gt;%\n  select(r.cor, r.drop, mean, sd) %&gt;%\n  arrange(r.cor)\n\n       r.cor  r.drop  mean     sd\nWRS4 -1.0767 -0.9807 2.136 0.9902\nWRS5  0.8201  0.7506 3.636 0.4924\nWRS3  0.8201  0.7506 4.636 0.4924\nWRS2  0.8572  0.8047 3.273 0.6311\nWRS6  0.8572  0.8047 3.273 0.6311\nWRS1  0.9426  0.8090 4.000 0.7559\n\n\nCheckpoint: Look for:\n\nr.cor: Corrected item-total correlation (item vs. scale without that item). Values &lt; 0.30 indicate weak contribution\nr.drop: Raw item-total correlation (item vs. full scale including that item)\n\nItems with r.cor &lt; 0.30 are candidates for deletion. Here, WRS4 shows the weakest correlation.\nStep 4: Assess “Alpha if Item Dropped”\n\n# Extract alpha-if-deleted\nalpha_if_dropped &lt;- alpha_full$alpha.drop\n\n# Display with item labels\nalpha_if_dropped %&gt;%\n  select(raw_alpha) %&gt;%\n  arrange(desc(raw_alpha))\n\n     raw_alpha\nWRS4    0.9368\nWRS3   -0.5041\nWRS5   -0.5041\nWRS2   -0.8272\nWRS6   -0.8272\nWRS1   -1.1293\n\n\nCheckpoint: If dropping WRS4 increases alpha (e.g., from 0.68 to 0.75), consider removing it. Compare the magnitude of improvement (e.g., Δα &gt; 0.05 is meaningful).\nStep 5: Recompute Alpha Without WRS4\n\n# Drop WRS4 and recompute alpha\nwrs_refined &lt;- wrs_data %&gt;% select(-WRS4)\n\nalpha_refined &lt;- alpha(wrs_refined)\nprint(alpha_refined)\n\n\nReliability analysis   \nCall: alpha(x = wrs_refined)\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.94      0.94    0.79      0.77  17 0.021  3.8 0.54     0.77\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.88  0.94  0.97\nDuhachek  0.89  0.94  0.98\n\n Reliability if an item is dropped:\n     raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nWRS1      0.92      0.93     0.7      0.76  13    0.032 0.034  0.64\nWRS2      0.92      0.93     0.8      0.77  13    0.028 0.017  0.77\nWRS3      0.93      0.93     0.8      0.77  14    0.024 0.018  0.78\nWRS5      0.93      0.93     0.8      0.77  14    0.024 0.018  0.78\nWRS6      0.92      0.93     0.8      0.77  13    0.028 0.017  0.77\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean   sd\nWRS1 22  0.93  0.92  0.97   0.87  4.0 0.76\nWRS2 22  0.92  0.90  0.77   0.87  3.3 0.63\nWRS3 22  0.87  0.90  0.76   0.82  4.6 0.49\nWRS5 22  0.87  0.90  0.76   0.82  3.6 0.49\nWRS6 22  0.92  0.90  0.77   0.87  3.3 0.63\n\nNon missing response frequency for each item\n        2    3    4    5 miss\nWRS1 0.00 0.27 0.45 0.27    0\nWRS2 0.09 0.55 0.36 0.00    0\nWRS3 0.00 0.00 0.36 0.64    0\nWRS5 0.00 0.36 0.64 0.00    0\nWRS6 0.09 0.55 0.36 0.00    0\n\n# Compare full vs. refined alpha\ncat(\"Full scale alpha:\", round(alpha_full$total$raw_alpha, 3), \"\\n\")\n\nFull scale alpha: 0.101 \n\ncat(\"Refined scale alpha (WRS4 dropped):\", round(alpha_refined$total$raw_alpha, 3), \"\\n\")\n\nRefined scale alpha (WRS4 dropped): 0.937 \n\ncat(\"Improvement:\", round(alpha_refined$total$raw_alpha - alpha_full$total$raw_alpha, 3), \"\\n\")\n\nImprovement: 0.836 \n\n\nCheckpoint: The refined scale (5 items) has higher alpha (≥ 0.75) and all items show r.cor &gt; 0.40. This is a more reliable measure.\nStep 6: Interpret in Context\nFollow these guidelines:\n\nResearch/exploratory scales: α ≥ 0.60–0.70 acceptable\nEstablished scales in research: α ≥ 0.70–0.80 preferred\nHigh-stakes decisions (clinical, personnel): α ≥ 0.80–0.90 required\nShort scales (3–5 items): Expect α 0.05–0.10 lower than longer scales\n\nDecision: Drop WRS4 because:\n\nIt has the weakest item-total correlation (r.cor &lt; 0.30)\nDropping it increases alpha by &gt; 0.05\nThe refined 5-item scale meets acceptable reliability (α ≥ 0.75 for research use)\n\nStep 7: Visualize Item Performance\n\n# Create item performance plot\nitem_performance &lt;- item_stats %&gt;%\n  rownames_to_column(\"Item\") %&gt;%\n  select(Item, r.cor, mean, sd)\n\nggplot(item_performance, aes(x = Item, y = r.cor, fill = r.cor &gt; 0.30)) +\n  geom_col() +\n  geom_hline(yintercept = 0.30, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Item-Total Correlations (Corrected)\",\n    x = \"Item\",\n    y = \"Corrected Item-Total Correlation\",\n    fill = \"Adequate (r &gt; 0.30)\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"TRUE\" = \"steelblue\", \"FALSE\" = \"salmon\"))\n\n\n\n\n\n\n\n\nCheckpoint: The plot clearly shows WRS4 falls below the 0.30 threshold, justifying its removal.\nStep 8: Report the Results\n\n“The 6-item Workplace Resilience Scale was piloted with 22 employees. Initial Cronbach’s alpha was 0.68 (95% CI [0.42, 0.84]). Item analysis revealed that WRS4 had a weak corrected item-total correlation (r = 0.18) and its removal increased alpha to 0.75 (95% CI [0.54, 0.88]). The refined 5-item scale demonstrates acceptable internal consistency for research purposes. Future validation should include a larger sample (n ≥ 100) and test-retest reliability.”\n\nDiscussion Questions:\n\nWhy is item-total correlation more informative than item mean or SD?\n\nIt measures how well an item captures the same construct as the rest of the scale; items with low correlations dilute reliability\n\nWhat if two items both have r.cor &lt; 0.30?\n\nDrop them one at a time, recomputing alpha after each removal, to see which has the greatest impact; or drop both if each independently improves alpha\n\nShould you always maximize alpha by dropping items?\n\nNo: Dropping items reduces content coverage. Balance reliability with construct validity (does the scale still measure what it should?). Sometimes α = 0.70 with more items is preferable to α = 0.80 with too few items\n\nHow does sample size affect these decisions?\n\nSmall samples (n &lt; 30) yield unstable item-total correlations; decisions should be tentative and revisited with larger samples\n\n\nExtension: Conduct split-half reliability or omega (hierarchical) to assess whether the scale is unidimensional or if subscales exist. Use confirmatory factor analysis (CFA) if you have theory about item groupings.\n\n\n\n3.5.16 Self-Assessment Quiz\nTest your understanding of reliability and measurement quality from Chapter 6. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. Cronbach’s alpha measures:\nA. Whether data are normally distributed\nB. Internal consistency—how closely related a set of items are\nC. Test-retest reliability\nD. Inter-rater agreement\n\nQ2. What is the main limitation of Cronbach’s alpha?\nA. It requires n&gt;1,000\nB. It assumes tau-equivalence (equal factor loadings across items)\nC. It cannot be calculated for short scales\nD. It is always too high\n\nQ3. A 3-item scale has α=0.55 with n=25. Is this acceptable?\nA. No—alpha must always exceed 0.70\nB. Possibly—short scales naturally have lower alpha; consider context, item-total correlations, and CI\nC. Yes—always acceptable\nD. No—the scale must be discarded\n\nQ4. McDonald’s omega is preferred over alpha when:\nA. Items have equal factor loadings\nB. Items have varying factor loadings (not tau-equivalent)\nC. Sample size exceeds 500\nD. Data are categorical\n\nQ5. Item-total correlation measures:\nA. How well an item correlates with the total scale score (excluding that item)\nB. Test-retest stability\nC. The mean of all items\nD. Sample size adequacy\n\nQ6. What does “alpha if item deleted” show?\nA. The p-value for each item\nB. How alpha would change if a specific item were removed\nC. The mean of each item\nD. Whether items are normally distributed\n\nQ7. Polychoric correlations are used when:\nA. Items are continuous and normally distributed\nB. Items are ordinal (e.g., Likert scales) and you want to estimate correlations between underlying continuous latent variables\nC. Sample size exceeds 1,000\nD. Data have no missing values\n\nQ8. A scale has α=0.72 with n=36. The 95% CI is [0.52, 0.86]. What does this tell us?\nA. Reliability is excellent\nB. Reliability is moderate, but precision is limited (wide CI) due to small sample\nC. The scale is unreliable\nD. More items must be added\n\nQ9. Split-half reliability involves:\nA. Testing participants twice\nB. Dividing items into two halves, computing correlation, and applying Spearman-Brown correction\nC. Removing half the sample\nD. Using different raters\n\nQ10. A scale shows α=−0.15. What is the most likely cause?\nA. Perfect reliability\nB. Reverse-coded items not properly recoded, or items measuring different constructs\nC. Sample size too large\nD. Normal distribution\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: Cronbach’s alpha quantifies internal consistency by comparing item variances to total scale variance. It indicates whether items measure a common construct. The chapter states: “Cronbach’s alpha estimates internal consistency by comparing item variances to total scale variance.”\nQ2. Answer: B\nExplanation: Alpha assumes all items contribute equally to the construct (tau-equivalence). When items have varying loadings, alpha may underestimate reliability. McDonald’s omega relaxes this assumption. The chapter explicitly notes alpha “assumes that all items measure a single underlying construct with equal factor loadings (tau-equivalent model).”\nQ3. Answer: B\nExplanation: Alpha depends on scale length; 3-item scales often yield α=0.50-0.65 even when internally consistent. Check: (1) item-total correlations (all &gt;0.30?), (2) alpha’s 95% CI (precision), (3) conceptual coherence. For exploratory work, α=0.55 may be acceptable. This reflects the chapter’s nuanced guidance about context-dependent thresholds rather than rigid cutoffs.\nQ4. Answer: B\nExplanation: Omega (ω_total) allows items to have different factor loadings, providing a more accurate reliability estimate when tau-equivalence does not hold. The chapter states: “McDonald’s omega (ωₜ) is an alternative to alpha that relaxes the tau-equivalence assumption.”\nQ5. Answer: A\nExplanation: Corrected item-total correlation indicates how strongly each item relates to the overall scale. Values &lt;0.30 suggest the item measures something different or is poorly worded. The lab practical emphasizes examining item-total correlations as a diagnostic tool.\nQ6. Answer: B\nExplanation: If alpha increases substantially when an item is removed, that item is weakening internal consistency (low correlation with others or measuring a different construct). Consider revising or removing it. This is a standard feature of reliability analysis output used to identify problematic items.\nQ7. Answer: B\nExplanation: Polychoric correlations assume ordinal responses arise from categorizing underlying continuous variables. They often yield higher estimates than Pearson correlations for Likert data, but require adequate sample size (n≥50 preferred). The chapter discusses polychoric correlations as theoretically appropriate for ordinal items.\nQ8. Answer: B\nExplanation: The point estimate (0.72) suggests acceptable reliability, but the wide CI reflects substantial uncertainty with n=36. The true population alpha could be as low as 0.52 (questionable) or as high as 0.86 (good). The chapter emphasizes that “small samples produce imprecise reliability estimates with wide confidence intervals.”\nQ9. Answer: B\nExplanation: Split-half reliability divides a scale into two halves (e.g., odd vs even items), correlates the half-scores, then adjusts using the Spearman-Brown formula to estimate full-scale reliability. This is a classic alternative method for assessing reliability mentioned in the learning objectives.\nQ10. Answer: B\nExplanation: Negative alpha indicates items are negatively correlated with each other—usually because reverse-coded items weren’t recoded (e.g., item 4 = “I intend to quit” on a job satisfaction scale). Check inter-item correlations for negative values. This is a common practical problem in survey research that the chapter addresses.\n\n\n\n\n\n\n3.5.17 Key Takeaways\n\nCronbach’s alpha and McDonald’s omega quantify internal consistency for multi-item scales.\nShort scales (3–5 items) yield lower reliability estimates than longer scales, even when items are internally consistent.\nSmall samples produce imprecise reliability estimates with wide confidence intervals.\nPolychoric correlations are theoretically appropriate for ordinal items but require adequate sample sizes.\nReliability should be reported transparently, with acknowledgement of limitations and implications for interpretation.\nLow reliability attenuates observed effect sizes and reduces power; researchers should interpret findings cautiously and advocate for scale development when feasible.\n\n\n\n3.5.18 Smoke Test\n\n# Re-run alpha on simple dataset\nlibrary(psych)\nset.seed(2025)\ntest_items &lt;- data.frame(\n  item1 = sample(1:5, 20, replace = TRUE),\n  item2 = sample(1:5, 20, replace = TRUE),\n  item3 = sample(1:5, 20, replace = TRUE)\n)\nalpha(test_items)\n\nSome items ( item2 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\n\nReliability analysis   \nCall: alpha(x = test_items)\n\n  raw_alpha std.alpha G6(smc) average_r   S/N  ase mean   sd median_r\n      -0.5     -0.58   -0.27     -0.14 -0.37 0.56    3 0.69    -0.12\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    -2.17  -0.5  0.36\nDuhachek -1.61  -0.5  0.60\n\n Reliability if an item is dropped:\n      raw_alpha std.alpha G6(smc) average_r    S/N alpha se var.r  med.r\nitem1    -0.257    -0.263  -0.116    -0.116 -0.208     0.55    NA -0.116\nitem2     0.052     0.052   0.027     0.027  0.055     0.42    NA  0.027\nitem3    -0.959    -0.970  -0.327    -0.327 -0.492     0.87    NA -0.327\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nitem1 20  0.50  0.48   NaN -0.198  3.4 1.4\nitem2 20  0.29  0.38   NaN -0.303  2.6 1.2\nitem3 20  0.68  0.62   NaN -0.069  2.9 1.5\n\nNon missing response frequency for each item\n         1    2    3    4    5 miss\nitem1 0.15 0.10 0.25 0.25 0.25    0\nitem2 0.20 0.25 0.35 0.10 0.10    0\nitem3 0.25 0.20 0.20 0.15 0.20    0",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#chapter-6.5.-developing-short-scales-for-small-samples",
    "href": "chapters/part-c-analysis-methods.html#chapter-6.5.-developing-short-scales-for-small-samples",
    "title": "3  Part C: Analysis Methods",
    "section": "3.6 Chapter 6.5. Developing Short Scales for Small Samples",
    "text": "3.6 Chapter 6.5. Developing Short Scales for Small Samples\n\n3.6.1 Learning Objectives\nBy the end of this section, you will understand the iterative process of scale development when working with small samples. You will learn what analyses are feasible at different sample sizes, how to identify problematic items early, and when to defer advanced psychometric analyses until larger validation studies are possible.\n\n\n3.6.2 The Scale Development Lifecycle\nScale development is inherently a multi-stage process. With small samples, researchers must be strategic about which psychometric analyses to conduct at each stage and which to reserve for later validation.\n\n\n3.6.3 The Iterative Process\n\n3.6.3.1 Stage 1: Item Generation (n = 5–10 cognitive interviews)\nGoal: Generate a pool of 2–3× your target number of items and ensure they are comprehensible.\nMethods:\n\nLiterature review: Identify existing scales and adapt items\nExpert consultation: Subject matter experts suggest relevant content\nCognitive interviews: Think-aloud protocols with 5–10 participants from the target population\n\nExample:\n\n# Documenting item generation\nlibrary(tidyverse)\n\nitem_pool &lt;- tibble(\n  item_id = 1:15,\n  item_text = c(\n    \"I feel confident handling work challenges\",\n    \"I bounce back quickly from setbacks\",\n    \"I maintain composure under pressure\",\n    \"I remain calm when facing obstacles\",\n    \"I trust my ability to solve problems\",\n    \"I recover from stress effectively\",\n    \"I rebound after difficult situations\",\n    \"I regain focus after disruptions\",\n    \"I process setbacks constructively\",\n    \"I maintain perspective during challenges\",\n    \"I adapt easily to changing priorities\",\n    \"I adjust my approach when needed\",\n    \"I seek support when needed\",\n    \"I learn from difficult experiences\",\n    \"I embrace new work demands\"\n  ),\n  domain = rep(c(\"Confidence\", \"Recovery\", \"Adaptability\"), each = 5),\n  cognitive_issues = c(\n    NA, \"Ambiguous: what counts as 'quickly'?\", NA, NA, NA,\n    NA, \"Too similar to item 2\", NA, NA, NA,\n    NA, NA, \"Double-barreled: 'seek' and 'support'\", NA, NA\n  )\n)\n\n# Flag items with comprehension issues\nitem_pool %&gt;%\n  filter(!is.na(cognitive_issues)) %&gt;%\n  select(item_id, item_text, cognitive_issues)\n\n# A tibble: 3 × 3\n  item_id item_text                            cognitive_issues                 \n    &lt;int&gt; &lt;chr&gt;                                &lt;chr&gt;                            \n1       2 I bounce back quickly from setbacks  Ambiguous: what counts as 'quick…\n2       7 I rebound after difficult situations Too similar to item 2            \n3      13 I seek support when needed           Double-barreled: 'seek' and 'sup…\n\n\nKey Point: At this stage, do NOT collect quantitative data. Focus on qualitative feedback about item clarity, relevance, and comprehensiveness.\n\n\n3.6.3.2 Stage 2: Pilot Testing (n = 20–30)\nGoal: Identify problematic items before committing to a larger study.\nMethods:\n\nAdminister all items to a small pilot sample\nCompute item-total correlations (r.cor)\nCheck for ceiling/floor effects (&gt; 80% at extreme response)\nExamine item means and SDs (avoid items with no variance)\n\nWhat you CAN do with n = 20–30:\n\nlibrary(psych)\n\n# Simulated pilot data: 25 participants, 12 items\nset.seed(2025)\npilot_data &lt;- tibble(\n  WRS1 = sample(1:5, 25, replace = TRUE, prob = c(0.05, 0.1, 0.3, 0.4, 0.15)),\n  WRS2 = sample(1:5, 25, replace = TRUE, prob = c(0.1, 0.15, 0.35, 0.3, 0.1)),\n  WRS3 = sample(1:5, 25, replace = TRUE, prob = c(0.05, 0.1, 0.25, 0.45, 0.15)),\n  WRS4 = sample(1:5, 25, replace = TRUE, prob = c(0.05, 0.1, 0.3, 0.4, 0.15)),\n  WRS5 = rep(5, 25),  # Ceiling effect item\n  WRS6 = sample(1:5, 25, replace = TRUE, prob = c(0.1, 0.15, 0.3, 0.35, 0.1)),\n  WRS7 = sample(1:5, 25, replace = TRUE, prob = c(0.05, 0.15, 0.35, 0.35, 0.1)),\n  WRS8 = sample(1:2, 25, replace = TRUE),  # Weak item: uncorrelated\n  WRS9 = sample(1:5, 25, replace = TRUE, prob = c(0.1, 0.15, 0.3, 0.3, 0.15)),\n  WRS10 = sample(1:5, 25, replace = TRUE, prob = c(0.05, 0.1, 0.35, 0.4, 0.1)),\n  WRS11 = sample(1:5, 25, replace = TRUE, prob = c(0.1, 0.15, 0.3, 0.35, 0.1)),\n  WRS12 = sample(1:5, 25, replace = TRUE, prob = c(0.05, 0.1, 0.3, 0.45, 0.1))\n)\n\n# Compute item-total correlations\npilot_alpha &lt;- alpha(pilot_data)\n\nSome items ( WRS1 WRS4 WRS7 WRS8 WRS11 WRS12 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\nitem_stats &lt;- pilot_alpha$item.stats\n\n# Flag problematic items\nitem_stats %&gt;%\n  rownames_to_column(\"Item\") %&gt;%\n  select(Item, r.cor, mean, sd) %&gt;%\n  mutate(\n    problem = case_when(\n      r.cor &lt; 0.30 ~ \"Weak correlation\",\n      mean &gt; 4.5 | mean &lt; 1.5 ~ \"Ceiling/floor effect\",\n      sd &lt; 0.5 ~ \"Low variance\",\n      TRUE ~ \"OK\"\n    )\n  ) %&gt;%\n  filter(problem != \"OK\")\n\n  Item    r.cor mean     sd          problem\n1 WRS1 -0.22688 3.56 1.0440 Weak correlation\n2 WRS2  0.01670 2.92 1.3204 Weak correlation\n3 WRS3  0.16931 3.80 1.0408 Weak correlation\n4 WRS4 -0.46173 3.48 1.0050 Weak correlation\n5 WRS6 -0.09238 3.24 1.2675 Weak correlation\n6 WRS7  0.23541 3.36 1.1504 Weak correlation\n7 WRS8  0.06615 1.56 0.5066 Weak correlation\n\n\nInterpretation:\n\nWRS5: Ceiling effect (everyone answered 5). Remove or reword.\nWRS8: Weak item-total correlation (r &lt; 0.30). Consider dropping.\nItems with very low SD suggest participants don’t differentiate; reword for clarity.\n\nWhat you CANNOT do with n = 20–30:\n\n\n\n\n\n\nWarningDo NOT Compute Alpha with n &lt; 30\n\n\n\nCronbach’s alpha estimates are highly unstable with n &lt; 30. The 95% confidence interval will be extremely wide (e.g., [0.40, 0.85]), making the estimate nearly meaningless.\nInstead: Focus on item-level diagnostics (means, SDs, item-total correlations) to refine your scale. Defer reliability estimation to Stage 3.\n\n\n\n\n3.6.3.3 Stage 3: Refinement (n = 50–100)\nGoal: Estimate reliability and assess dimensionality.\nMethods:\n\nCronbach’s alpha with confidence intervals\nMcDonald’s omega (if you suspect multidimensionality)\nSplit-half reliability as a robustness check\nExploratory Factor Analysis (EFA) if n ≥ 100 and you suspect subscales\n\nExample:\n\n# Simulated refinement data: 60 participants, 8 items (problematic items removed)\nset.seed(2025)\nrefinement_data &lt;- as.data.frame(matrix(\n  sample(1:5, 60 * 8, replace = TRUE, prob = c(0.05, 0.15, 0.30, 0.35, 0.15)),\n  nrow = 60, ncol = 8\n))\ncolnames(refinement_data) &lt;- paste0(\"WRS\", 1:8)\n\n# Compute alpha\nalpha_result &lt;- alpha(refinement_data)\n\nSome items ( WRS4 WRS5 WRS7 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\ncat(\"Cronbach's alpha:\", round(alpha_result$total$raw_alpha, 3), \"\\n\")\n\nCronbach's alpha: 0.045 \n\ncat(\"Note: With n=60, alpha estimate is more stable than with n&lt;30\\n\")\n\nNote: With n=60, alpha estimate is more stable than with n&lt;30\n\n# Split-half reliability\nsplit_half &lt;- splitHalf(refinement_data)\nprint(split_half)\n\nSplit half reliabilities  \nCall: splitHalf(r = refinement_data)\n\nMaximum split half reliability (lambda 4) =  0.7\nGuttman lambda 6                          =  0.51\nAverage split half reliability            =  0.49\nGuttman lambda 3 (alpha)                  =  0.49\nGuttman lambda 2                          =  0.52\nMinimum split half reliability  (beta)    =  0.34\nAverage interitem r =  0.11  with median =  0.12\n\n\nInterpretation:\n\nAlpha ≥ 0.70 suggests adequate internal consistency for research purposes\nThe confidence interval is still fairly wide (n = 60), but informative\nIf alpha &lt; 0.60, consider dropping more items or revising wording\n\nExploratory Factor Analysis (EFA) with n = 50–100:\n\n# EFA requires n ≥ 100 ideally; with n=60, results are exploratory only\n# Simulate larger dataset for demonstration\nset.seed(2025)\nefa_data &lt;- as.data.frame(matrix(\n  sample(1:5, 100 * 8, replace = TRUE, prob = c(0.05, 0.15, 0.30, 0.35, 0.15)),\n  nrow = 100, ncol = 8\n))\ncolnames(efa_data) &lt;- paste0(\"WRS\", 1:8)\n\n# Scree plot to determine number of factors\nscree(efa_data, factors = FALSE)\n\n\n\n\n\n\n\n# Parallel analysis (more robust than scree plot alone)\nfa.parallel(efa_data, fa = \"fa\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  0  and the number of components =  NA \n\n# Fit 1-factor model\nefa_result &lt;- fa(efa_data, nfactors = 1, rotate = \"oblimin\")\nprint(efa_result$loadings, cutoff = 0.3)\n\n\nLoadings:\n     MR1   \nWRS1       \nWRS2       \nWRS3       \nWRS4       \nWRS5       \nWRS6       \nWRS7       \nWRS8  0.520\n\n                 MR1\nSS loadings    0.533\nProportion Var 0.067\n\n\nCaution: With n &lt; 100, factor loadings and structure are unstable. Use EFA results as preliminary guidance for item grouping, not definitive evidence of subscales.\n\n\n3.6.3.4 Stage 4: Validation (n = 150+)\nGoal: Confirm scale structure and establish validity.\nMethods:\n\nConfirmatory Factor Analysis (CFA): Test hypothesized factor structure\nTest-retest reliability: Administer scale twice (2–4 weeks apart)\nConvergent validity: Correlate with theoretically related measures\nDiscriminant validity: Show low correlation with unrelated constructs\nKnown-groups validity: Scale discriminates between relevant groups\n\nExample:\n\n# CFA requires lavaan package and n ≥ 150\nlibrary(lavaan)\n\n# Define 1-factor model\nmodel &lt;- '\n  resilience =~ WRS1 + WRS2 + WRS3 + WRS4 + WRS5 + WRS6 + WRS7 + WRS8\n'\n\n# Fit model\ncfa_result &lt;- cfa(model, data = validation_data)\nsummary(cfa_result, fit.measures = TRUE, standardized = TRUE)\n\n# Fit indices to assess model adequacy:\n# - CFI &gt; 0.90 (good fit)\n# - RMSEA &lt; 0.08 (acceptable fit)\n# - SRMR &lt; 0.08 (good fit)\n\nTest-retest reliability:\n\n# Compute scale scores at Time 1 and Time 2\nvalidation_data &lt;- validation_data %&gt;%\n  mutate(\n    resilience_t1 = rowMeans(select(., WRS1_t1:WRS8_t1)),\n    resilience_t2 = rowMeans(select(., WRS1_t2:WRS8_t2))\n  )\n\n# Intraclass correlation coefficient (ICC)\nlibrary(irr)\nicc_result &lt;- icc(\n  cbind(validation_data$resilience_t1, validation_data$resilience_t2),\n  model = \"twoway\",\n  type = \"agreement\"\n)\nprint(icc_result)\n\n# ICC &gt; 0.75 indicates good test-retest reliability\n\n\n\n\n3.6.4 Special Considerations for n &lt; 50\n\n3.6.4.1 What You CANNOT Do\n\n\n\n\n\n\nImportantAnalyses That Require Larger Samples\n\n\n\n\n\n\n\n\n\nWarning⚠️ Do NOT Force Large-Sample Methods on Small Data\n\n\n\nWith n &lt; 50, the following analyses are not feasible or will produce unreliable results:\n\nExploratory Factor Analysis (EFA): Rule of thumb is n ≥ 100 or 5–10 participants per item\nConfirmatory Factor Analysis (CFA): Requires n ≥ 150–200 for stable parameter estimates\nMeasurement Invariance Testing: Requires n ≥ 200 per group\nStructural Equation Modeling (SEM): Complex models need n ≥ 200–400\nPLS-SEM (Partial Least Squares SEM): Despite marketing claims, needs n ≥ 100 for reliable results\nItem Response Theory (IRT): Most models require n ≥ 250–500\nReliable Cronbach’s alpha: With n &lt; 30, alpha estimates have 95% CIs spanning 0.3–0.4 units\n\nDo NOT force these analyses with inadequate sample sizes. Results will be misleading.\n\n\n\n\n\n\n\n\n\nNote📖 Why Structural Equation Modeling (SEM) Requires Large Samples\n\n\n\n\n\nQuestion: “Can I use SEM, CFA, or PLS-SEM with my small sample (n &lt; 100)?”\nShort Answer: No. SEM-based methods require substantially larger samples than this book’s target range (n = 10–100).\n\n3.6.5 Minimum Sample Size Requirements\n\n\n\n\n\n\n\n\n\nMethod\nMinimum n\nRealistic n\nWhy?\n\n\n\n\nConfirmatory Factor Analysis (CFA)\n150\n200-300\nStable factor loadings, fit indices\n\n\nStructural Equation Modeling (SEM)\n200\n300-500\nComplex path models, multiple latent variables\n\n\nPLS-SEM\n100\n150-300\nDespite “small-sample” marketing, needs 100+ for reliability\n\n\nMulti-Group SEM\n200/group\n300/group\nMeasurement invariance testing\n\n\n\nRule of Thumb: 10-20 observations per estimated parameter (e.g., 5 indicators + 3 paths = 8 parameters → need 80-160 observations).\n\n\n3.6.6 What Happens If You Ignore These Requirements?\nWith n &lt; 100, SEM/CFA/PLS-SEM will produce:\n❌ Unstable Parameter Estimates - Factor loadings fluctuate wildly with small perturbations - Path coefficients have huge standard errors - Adding/removing 5 observations changes conclusions\n❌ Non-Convergent Solutions - Maximum likelihood fails to converge - Heywood cases (negative variances, loadings &gt; 1.0) - Improper solutions requiring arbitrary constraints\n❌ Unreliable Fit Indices - χ², CFI, TLI, RMSEA become meaningless with small n - “Good fit” may be statistical artifact, not real model adequacy - Modification indices unreliable (suggest spurious changes)\n❌ Overfit Models - Model fits sample perfectly but fails to generalize - Cross-validation impossible (too few observations to split) - Results won’t replicate in new samples\n❌ False Confidence - Software produces output even when results are garbage - p-values and CIs misleadingly precise - Reviewers will reject papers citing insufficient sample size\n\n\n3.6.7 What Should You Do Instead? (For n &lt; 100)\nUse the methods in THIS book:\n\n\n\n\n\n\n\n\n\nSEM Goal\nSmall-Sample Alternative\nChapter\nMinimum n\n\n\n\n\nAssess scale reliability\nCronbach’s α, McDonald’s ω, split-half\nCh 6\n30-50\n\n\nValidate items\nItem-total correlations, alpha-if-deleted\nCh 6\n30-50\n\n\nReduce dimensionality\nSum/mean composite scores\nCh 6\n20+\n\n\nTest relationships (X → Y)\nRegression with composite scores\nCh 5\n30-50\n\n\nMultiple predictors\nPenalized regression (ridge/lasso/elastic net)\nCh 5 Lab 5.2\n50-100\n\n\nMediation (X → M → Y)\nSimple mediation with bootstrap CIs\nPart E Project 5\n80-100\n\n\nLatent correlations\nPolychoric correlations (exploratory)\nCh 6\n50+\n\n\nMeasurement precision\nStandard Error of Measurement (SEM statistic)\nCh 6\n30+\n\n\n\nKey Principle: Composite scores are your friend. - Sum or average your scale items to create observed composite variables - Use these composites in regression, t-tests, ANOVA - Acknowledge measurement error in limitations section - Plan larger validation study (n ≥ 200) for future CFA/SEM\n\n\n3.6.8 Example: Replacing SEM with Composite-Score Analysis\nProposed SEM Model (n = 60):\nJob_Satisfaction (5 items) → Turnover_Intent (3 items)\n     ↑\nPerformance (4 items)\nSmall-Sample Alternative:\n# Create composite scores by averaging items\ndata &lt;- data %&gt;%\n  mutate(\n    satisfaction = rowMeans(select(., satisfaction_1:satisfaction_5)),\n    performance = rowMeans(select(., performance_1:performance_4)),\n    turnover = rowMeans(select(., turnover_1:turnover_3))\n  )\n\n# Test relationships with standard regression\nmodel1 &lt;- lm(turnover ~ satisfaction, data = data)\nmodel2 &lt;- lm(turnover ~ satisfaction + performance, data = data)\n\n# Report coefficients, R², confidence intervals\nAdvantages: - ✅ Works with n = 60 - ✅ Interpretable (unit change in averaged scale score) - ✅ Robust (doesn’t require distributional assumptions for latent variables) - ✅ Honest (acknowledges you’re using observed composites, not latent variables)\nLimitations to Acknowledge: - Composites contain measurement error (but so does SEM with n &lt; 150!) - Cannot test complex factor structures - Cannot partition within-item vs. between-item variance\nWhen to Pursue SEM: Collect n ≥ 200 in a follow-up study. Then: 1. Use EFA to explore factor structure (if theory is unclear) 2. Use CFA to confirm measurement model 3. Test structural paths with latent variables 4. Assess model fit rigorously\n\n\n3.6.9 Software Will Let You Do Bad Things\nWarning: SmartPLS, AMOS, Mplus, and other SEM software will happily run with n = 50. They will produce: - Parameter estimates - p-values - Fit indices - Pretty path diagrams\nThis does NOT mean the results are trustworthy. Software cannot judge whether your sample size is adequate—you must.\n\n\n3.6.10 Recommended Reading (For Future Large-Sample Studies)\nWhen you collect n ≥ 200, consult these resources:\n\nKline, R. B. (2016). Principles and Practice of Structural Equation Modeling (4th ed.). Guilford Press.\n\nGold standard SEM textbook\nSample size guidelines (pp. 15-18, 264-270)\n\nBrown, T. A. (2015). Confirmatory Factor Analysis for Applied Research (2nd ed.). Guilford Press.\n\nCFA-specific guidance\nMeasurement invariance testing\n\nHair, J. F., Hult, G. T. M., Ringle, C. M., & Sarstedt, M. (2022). A Primer on Partial Least Squares Structural Equation Modeling (PLS-SEM) (3rd ed.). Sage.\n\nPLS-SEM methods (but note: still needs n ≥ 100-150 realistically)\n\nHoyle, R. H. (Ed.). (2023). Handbook of Structural Equation Modeling (2nd ed.). Guilford Press.\n\nAdvanced topics, sample size planning\n\n\n\n\n3.6.11 Bottom Line\nFor n = 10–100, use the methods in this book. They are: - ✅ Appropriate for small samples - ✅ Robust to violations of assumptions - ✅ Honest about uncertainty - ✅ Interpretable for stakeholders - ✅ Defensible to reviewers\nSave SEM for when you have n ≥ 200. Until then, composite scores + regression will serve you well.\n\n\n\n\n\n\n3.6.11.1 What You CAN Do\nWith n = 20–50, focus on these feasible and informative analyses:\n\nContent Validity:\n\nExpert review (e.g., Content Validity Index with 5–10 experts)\nCognitive interviews to assess comprehension\nAlignment with theoretical framework\n\nItem-Level Diagnostics:\n\nItem means, SDs, skewness\nItem-total correlations (r.cor)\nCeiling/floor effects\nInter-item correlation matrix\n\nPreliminary Reliability (n ≥ 30):\n\nCronbach’s alpha with 95% CI (report the wide CI!)\nSplit-half reliability as robustness check\nMean inter-item correlation (0.15–0.50 is typical)\n\nKnown-Groups Validity:\n\nCompare scale scores between groups expected to differ (e.g., clinical vs. non-clinical)\nUse nonparametric tests (Mann-Whitney U) if distributions are skewed\n\nPrepare for Larger Validation Study:\n\nDocument item generation process\nReport pilot results transparently (including dropped items and why)\nSpecify hypotheses for CFA in future study\n\n\n\n\n3.6.12 Example: Documenting a Small-Sample Scale Development\n\n# Reporting template for n &lt; 50 pilot study\nlibrary(gt)\n\nscale_dev_report &lt;- tibble(\n  Stage = c(\"Item Generation\", \"Pilot Testing\", \"Refinement\", \"Validation\"),\n  Sample_Size = c(\"n = 8 (cognitive interviews)\", \"n = 25\", \"n = 60\", \"Planned: n = 200\"),\n  Analyses_Conducted = c(\n    \"Think-aloud protocols; expert review (CVI = 0.88)\",\n    \"Item-total correlations; ceiling/floor checks; 3 items dropped\",\n    \"Alpha = 0.73 [95% CI: 0.61, 0.82]; split-half r = 0.70\",\n    \"CFA, test-retest ICC, convergent validity (planned)\"\n  ),\n  Key_Findings = c(\n    \"Generated 15 items; 2 flagged as ambiguous\",\n    \"Dropped WRS5 (ceiling), WRS8 (r.cor = 0.18), WRS11 (low variance)\",\n    \"8-item scale shows acceptable reliability for research use\",\n    \"Pending larger validation sample\"\n  )\n)\n\nscale_dev_report %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Workplace Resilience Scale Development Summary\",\n    subtitle = \"Iterative pilot and refinement process\"\n  ) %&gt;%\n  cols_label(\n    Stage = \"Development Stage\",\n    Sample_Size = \"Sample Size\",\n    Analyses_Conducted = \"Analyses Conducted\",\n    Key_Findings = \"Key Findings\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 10\n  )\n\n\n\n\n\n\n\nWorkplace Resilience Scale Development Summary\n\n\nIterative pilot and refinement process\n\n\nDevelopment Stage\nSample Size\nAnalyses Conducted\nKey Findings\n\n\n\n\nItem Generation\nn = 8 (cognitive interviews)\nThink-aloud protocols; expert review (CVI = 0.88)\nGenerated 15 items; 2 flagged as ambiguous\n\n\nPilot Testing\nn = 25\nItem-total correlations; ceiling/floor checks; 3 items dropped\nDropped WRS5 (ceiling), WRS8 (r.cor = 0.18), WRS11 (low variance)\n\n\nRefinement\nn = 60\nAlpha = 0.73 [95% CI: 0.61, 0.82]; split-half r = 0.70\n8-item scale shows acceptable reliability for research use\n\n\nValidation\nPlanned: n = 200\nCFA, test-retest ICC, convergent validity (planned)\nPending larger validation sample\n\n\n\n\n\n\n\n\n\n3.6.13 Reporting Guidelines for Small-Sample Scale Development\nWhen publishing or reporting scale development work with n &lt; 50:\n\nAcknowledge limitations explicitly:\n\n“With n = 25, we could not reliably estimate Cronbach’s alpha. Instead, we focused on item-total correlations to identify weak items.”\n“Exploratory factor analysis was not feasible (n = 60); we plan CFA with a larger sample (target n = 200).”\n\nReport what you did (not what you wish you could do):\n\nDon’t report alpha if n &lt; 30 (the CI will be embarrassingly wide)\nDon’t force EFA/CFA with inadequate samples; instead, document theoretical rationale for item grouping\n\nFrame as a preliminary/pilot study:\n\n“This pilot study (n = 35) was designed to refine item wording and identify problematic items before a larger validation study.”\n“Results are preliminary and should be interpreted with caution pending validation with n ≥ 150.”\n\nProvide detailed item-level information:\n\nPublish item means, SDs, item-total correlations\nReport which items were dropped and why\nShare cognitive interview feedback (qualitative)\n\nPlan and fund the validation study:\n\nUse pilot data to justify sample size for validation (power analysis for CFA)\nSecure resources for n ≥ 150–200 before claiming a “validated” scale\n\n\n\n\n3.6.14 Key Takeaways\n\nScale development is a multi-stage process requiring progressively larger samples\nWith n &lt; 30: Focus on qualitative feedback and item-level diagnostics; do not compute alpha\nWith n = 30–50: Compute preliminary alpha (with wide CI); identify weak items\nWith n = 50–100: Assess reliability; exploratory factor analysis is feasible but results are tentative\nWith n ≥ 150: Conduct CFA, test-retest, and validity studies\nTransparency is key: Report what you did and didn’t do, and acknowledge sample size limitations\n\n\n\n\n3.7 Chapter 7. Multi-Criteria Decision Making (MCDM) for Small Sets of Alternatives\n\n3.7.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the principles of multi-criteria decision analysis (MCDM) - ✓ Understand the theoretical basis of AHP, TOPSIS, and VIKOR methods - ✓ Recognize when MCDM is appropriate vs. when statistical inference is needed - ✓ Distinguish compensatory from non-compensatory aggregation methods\nPractical Skills - ✓ Structure decision problems with hierarchies of criteria and sub-criteria - ✓ Elicit pairwise comparison judgments for AHP using consistency checks - ✓ Implement TOPSIS and VIKOR in R to rank alternatives - ✓ Compute composite scores with weighted criteria\nCritical Evaluation - ✓ Assess the consistency of expert judgments (AHP consistency ratio) - ✓ Evaluate sensitivity of rankings to weight changes - ✓ Critique subjective weight elicitation procedures\nApplication - ✓ Apply MCDM methods to policy selection, technology choice, or program evaluation - ✓ Perform sensitivity analyses to test robustness of rankings - ✓ Report MCDM results transparently with criteria weights and score decompositions\n\n\n3.7.2 When to Use MCDM Methods\nMulti-criteria decision-making (MCDM) methods are designed for problems where a small set of alternatives (options, projects, policies) must be evaluated and ranked on multiple criteria. They are particularly useful when:\n\nThe number of alternatives is small (typically fewer than 20).\nAlternatives are assessed on diverse criteria (quantitative and qualitative).\nStakeholder preferences or expert judgements must be incorporated.\nThe goal is ranking, selection, or resource allocation rather than hypothesis testing.\n\nMCDM methods do not test statistical hypotheses or quantify uncertainty in the same way as inferential statistics. Instead, they provide structured frameworks for synthesising information and making transparent, defensible choices.\n\n\n3.7.3 Analytic Hierarchy Process (AHP)\nAHP decomposes a decision problem into a hierarchy of criteria and alternatives, then uses pairwise comparisons to derive priority weights. Decision-makers compare each pair of criteria (and each pair of alternatives under each criterion) to indicate relative importance or preference. AHP synthesises these comparisons into overall scores for each alternative.\nWhen to use: Multiple criteria with subjective importance weights, need for structured elicitation of expert judgements, transparency in weighting and aggregation.\nSteps: 1. Define the goal, criteria, and alternatives. 2. Perform pairwise comparisons of criteria to derive criteria weights. 3. Perform pairwise comparisons of alternatives under each criterion. 4. Aggregate to obtain overall scores for each alternative. 5. Check consistency of pairwise judgements.\n\n\n3.7.4 Example: AHP for Selecting a Training Programme\nSuppose an organisation must choose among three training programmes (A, B, C) based on three criteria: Cost (lower is better), Effectiveness (higher is better), and Feasibility (higher is better). We use AHP to rank the programmes.\n\nlibrary(tidyverse)\n\n# Define criteria weights from pairwise comparisons\n# Suppose decision-makers judge: \n#   Effectiveness is 3 times more important than Cost\n#   Effectiveness is 2 times more important than Feasibility\n#   Feasibility is slightly more important than Cost (1.5 times)\n# These comparisons yield approximate normalised weights:\ncriteria_weights &lt;- c(Cost = 0.20, Effectiveness = 0.58, Feasibility = 0.22)\n\n# Define alternative scores on each criterion (normalised 0-1 scale)\n# These might come from pairwise comparisons or direct assessments\nalternatives &lt;- tibble(\n  Programme = c(\"A\", \"B\", \"C\"),\n  Cost_score = c(0.50, 0.30, 0.20),         # A is cheapest\n  Effectiveness_score = c(0.25, 0.50, 0.25), # B is most effective\n  Feasibility_score = c(0.40, 0.30, 0.30)    # A is most feasible\n)\n\n# Compute weighted scores\nalternatives &lt;- alternatives %&gt;%\n  mutate(\n    Weighted_Cost = Cost_score * criteria_weights[\"Cost\"],\n    Weighted_Effectiveness = Effectiveness_score * criteria_weights[\"Effectiveness\"],\n    Weighted_Feasibility = Feasibility_score * criteria_weights[\"Feasibility\"],\n    Total_Score = Weighted_Cost + Weighted_Effectiveness + Weighted_Feasibility\n  ) %&gt;%\n  arrange(desc(Total_Score))\n\nprint(alternatives)\n\n# A tibble: 3 × 8\n  Programme Cost_score Effectiveness_score Feasibility_score Weighted_Cost\n  &lt;chr&gt;          &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n1 B                0.3                0.5                0.3          0.06\n2 A                0.5                0.25               0.4          0.1 \n3 C                0.2                0.25               0.3          0.04\n# ℹ 3 more variables: Weighted_Effectiveness &lt;dbl&gt;, Weighted_Feasibility &lt;dbl&gt;,\n#   Total_Score &lt;dbl&gt;\n\n\nInterpretation: Programme B has the highest total score, driven by its strong performance on Effectiveness (the most heavily weighted criterion). Programme A ranks second due to low cost and high feasibility. The ranking reflects the decision-makers’ stated priorities. Sensitivity analysis (varying criteria weights) can assess robustness.\n\n\n3.7.5 TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)\nTOPSIS ranks alternatives by their distance from an ideal solution (best on all criteria) and a negative-ideal solution (worst on all criteria). Alternatives closest to the ideal and farthest from the negative-ideal are ranked highest.\nWhen to use: Multiple criteria with known or easily assigned weights, desire for geometric interpretation, straightforward implementation.\nSteps: 1. Normalise the decision matrix (each criterion scaled to comparable units). 2. Apply criteria weights to the normalised matrix. 3. Identify the ideal and negative-ideal solutions. 4. Compute Euclidean distances from each alternative to both reference points. 5. Calculate closeness coefficients and rank alternatives.\n\n\n3.7.6 Example: TOPSIS for Project Selection\nWe evaluate four community projects (P1, P2, P3, P4) on three criteria: Impact (higher is better), Cost (lower is better, so we reverse), and Community Support (higher is better).\n\nNormalisation choices. TOPSIS typically uses vector (Euclidean) normalisation so that each criterion contributes proportionally regardless of its original scale. Cost variables must be recoded so that higher values are preferable (e.g., invert, take reciprocal, or subtract from the maximum). Alternatives include min–max scaling or z-scores; choose a method consistent across criteria and document it so stakeholders understand how raw data map into the TOPSIS space.\n\n\nlibrary(tidyverse)\n\n# Decision matrix: rows = projects, columns = criteria\n# Impact (scale 1-10), Cost (in £1000s), Support (scale 1-10)\ndecision_matrix &lt;- tibble(\n  Project = c(\"P1\", \"P2\", \"P3\", \"P4\"),\n  Impact = c(7, 8, 6, 9),\n  Cost = c(50, 70, 40, 80),      # Lower is better; we'll invert\n  Support = c(6, 7, 8, 7)\n)\n\n# Invert cost (so higher is better)\ndecision_matrix &lt;- decision_matrix %&gt;%\n  mutate(Cost_inverted = max(Cost) - Cost + min(Cost))\n\n# Normalise each criterion (vector/Euclidean normalisation)\ncriteria_cols &lt;- c(\"Impact\", \"Cost_inverted\", \"Support\")\nnormalised &lt;- decision_matrix %&gt;%\n  mutate(across(all_of(criteria_cols), ~ .x / sqrt(sum(.x^2)), .names = \"norm_{.col}\"))\n\n# Apply weights (assume equal weights for simplicity)\nweights &lt;- c(Impact = 1/3, Cost_inverted = 1/3, Support = 1/3)\nweighted &lt;- normalised %&gt;%\n  mutate(\n    w_Impact = norm_Impact * weights[\"Impact\"],\n    w_Cost = norm_Cost_inverted * weights[\"Cost_inverted\"],\n    w_Support = norm_Support * weights[\"Support\"]\n  )\n\n# Ideal and negative-ideal solutions\nideal &lt;- c(max(weighted$w_Impact), max(weighted$w_Cost), max(weighted$w_Support))\nnegative_ideal &lt;- c(min(weighted$w_Impact), min(weighted$w_Cost), min(weighted$w_Support))\n\n# Compute distances\nweighted &lt;- weighted %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    dist_ideal = sqrt((w_Impact - ideal[1])^2 + (w_Cost - ideal[2])^2 + (w_Support - ideal[3])^2),\n    dist_neg_ideal = sqrt((w_Impact - negative_ideal[1])^2 + (w_Cost - negative_ideal[2])^2 + (w_Support - negative_ideal[3])^2),\n    closeness = dist_neg_ideal / (dist_ideal + dist_neg_ideal)\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(closeness))\n\n# Results\nselect(weighted, Project, Impact, Cost, Support, closeness)\n\n# A tibble: 4 × 5\n  Project Impact  Cost Support closeness\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 P3           6    40       8     0.640\n2 P1           7    50       6     0.544\n3 P2           8    70       7     0.395\n4 P4           9    80       7     0.389\n\n\nInterpretation: The closeness coefficient ranges from 0 to 1; higher values indicate better alternatives. Projects are ranked by closeness. The top-ranked project is closest to the ideal (best on all criteria) and farthest from the negative-ideal (worst on all criteria). Sensitivity analysis can vary criteria weights to assess stability of rankings.\n\n\n3.7.7 VIKOR (VIseKriterijumska Optimizacija I Kompromisno Resenje)\nVIKOR ranks alternatives based on closeness to the ideal solution, with emphasis on compromise. It computes utility and regret measures, then combines them into a compromise ranking. VIKOR is useful when trade-offs among criteria are important.\nWhen to use: Similar contexts to TOPSIS, when emphasis on compromise and trade-offs is desired.\n\n\n3.7.8 Other MCDM Methods\n\nMOORA (Multi-Objective Optimisation by Ratio Analysis): Simple ratio-based ranking.\nWASPAS (Weighted Aggregated Sum Product Assessment): Combines additive and multiplicative aggregation.\nDEMATEL (Decision-Making Trial and Evaluation Laboratory): Analyses causal relationships among criteria.\nSMART (Simple Multi-Attribute Rating Technique): Direct scoring and weighting.\n\nAll these methods share common steps: structuring the problem, normalising criteria, applying weights, and computing composite scores. The choice of method depends on problem context, stakeholder preferences, and computational convenience.\n\n\n3.7.9 Sensitivity Analysis in MCDM\nMCDM rankings can be sensitive to criteria weights and normalisation methods. Sensitivity analysis systematically varies weights (or other inputs) and observes changes in rankings. If rankings remain stable across a range of plausible weights, conclusions are robust. If small weight changes produce large ranking changes, results should be interpreted cautiously and alternative weighting schemes considered.\n\n\n3.7.10 Example: Sensitivity Analysis for AHP\nWe vary the weight on Effectiveness in the AHP training example and observe changes in rankings.\n\nlibrary(tidyverse)\n\n# Original weights\nweights_base &lt;- c(Cost = 0.20, Effectiveness = 0.58, Feasibility = 0.22)\n\n# Alternative scores (from earlier example)\nalternatives_base &lt;- tibble(\n  Programme = c(\"A\", \"B\", \"C\"),\n  Cost_score = c(0.50, 0.30, 0.20),\n  Effectiveness_score = c(0.25, 0.50, 0.25),\n  Feasibility_score = c(0.40, 0.30, 0.30)\n)\n\n# Function to compute rankings for given effectiveness weight\ncompute_ranking &lt;- function(eff_weight) {\n  # Redistribute remaining weight proportionally between Cost and Feasibility\n  remaining &lt;- 1 - eff_weight\n  cost_weight &lt;- remaining * (0.20 / (0.20 + 0.22))\n  feas_weight &lt;- remaining * (0.22 / (0.20 + 0.22))\n  \n  alts &lt;- alternatives_base %&gt;%\n    mutate(\n      Total_Score = Cost_score * cost_weight + \n                     Effectiveness_score * eff_weight + \n                     Feasibility_score * feas_weight\n    ) %&gt;%\n    arrange(desc(Total_Score))\n  \n  tibble(Effectiveness_Weight = eff_weight, Ranking = list(alts$Programme))\n}\n\n# Vary effectiveness weight from 0.3 to 0.8\nsensitivity_results &lt;- map_dfr(seq(0.3, 0.8, by = 0.1), compute_ranking)\nsensitivity_results &lt;- sensitivity_results %&gt;%\n  mutate(Ranking = map_chr(Ranking, ~ paste(.x, collapse = \", \")))\n\nprint(sensitivity_results)\n\n# A tibble: 6 × 2\n  Effectiveness_Weight Ranking\n                 &lt;dbl&gt; &lt;chr&gt;  \n1                  0.3 A, B, C\n2                  0.4 B, A, C\n3                  0.5 B, A, C\n4                  0.6 B, A, C\n5                  0.7 B, A, C\n6                  0.8 B, A, C\n\n\nInterpretation: The table shows how the ranking changes as the weight on Effectiveness varies. If the top-ranked programme remains the same across all weights, the decision is robust. If the ranking changes with small weight variations, further deliberation or data collection may be warranted.\n\n\n3.7.11 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 7 Questions\n\n\n\nQ1. When are MCDM (Multi-Criteria Decision-Making) methods most appropriate?\n\nTesting statistical hypotheses with large samples\n\nRanking and selecting from a small set of alternatives evaluated on multiple diverse criteria\n\nCalculating p-values for experimental data\n\nOnly when all criteria are quantitative\n\n\nQ2. What is the primary purpose of pairwise comparisons in the Analytic Hierarchy Process (AHP)?\n\nTo calculate correlation coefficients\n\nTo derive priority weights for criteria and alternatives by comparing their relative importance\n\nTo test statistical significance\n\nTo normalize all data to 0-1 scale\n\n\nQ3. In TOPSIS, what does the “closeness coefficient” measure?\n\nThe p-value for each alternative\n\nHow close an alternative is to the ideal solution (best on all criteria) relative to the negative-ideal solution (worst on all criteria)\n\nThe Cronbach’s alpha for criteria\n\nThe sample size required for analysis\n\n\nQ4. Why is sensitivity analysis essential in MCDM?\n\nTo calculate standard errors\n\nTo assess whether rankings remain stable when criteria weights or other inputs are varied\n\nTo test normality assumptions\n\nTo increase sample size\n\n\nQ5. In the TOPSIS method, why must cost variables (where lower is better) be inverted or reversed?\n\nTo make the algorithm run faster\n\nBecause TOPSIS assumes higher values are preferable for all criteria; cost must be recoded so higher values represent better performance\n\nCost variables cannot be used in TOPSIS\n\nTo calculate p-values\n\n\nQ6. What does AHP’s consistency check evaluate?\n\nWhether the data are normally distributed\n\nWhether pairwise comparison judgments are logically consistent (e.g., if A &gt; B and B &gt; C, then A &gt; C)\n\nWhether the sample size is adequate\n\nWhether residuals are homoscedastic\n\n\nQ7. MCDM methods are designed for problems where:\n\nThe goal is ranking, selection, or resource allocation rather than hypothesis testing\n\nLarge randomized controlled trials are conducted\n\nOnly one criterion matters\n\nInferential statistics are required\n\n\nQ8. In the training programme selection example, Programme B ranked highest despite being mid-range on Cost and Feasibility. Why?\n\nIt had the largest sample size\n\nIt performed strongly on Effectiveness, which was the most heavily weighted criterion (58%)\n\nIt had the smallest standard error\n\nIt was randomly selected\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “MCDM methods are designed for problems where a small set of alternatives (options, projects, policies) must be evaluated and ranked on multiple criteria.”\nMCDM complements statistical inference by addressing decision problems where ranking and selection are the goals, not hypothesis testing.\nA2. B) “AHP… uses pairwise comparisons to derive priority weights. Decision-makers compare each pair of criteria (and each pair of alternatives under each criterion) to indicate relative importance or preference.”\nPairwise comparisons systematically elicit judgments about relative importance, which AHP synthesizes into numerical weights.\nA3. B) “The closeness coefficient ranges from 0 to 1; higher values indicate better alternatives… closest to the ideal (best on all criteria) and farthest from the negative-ideal (worst on all criteria).”\nTOPSIS ranks alternatives by computing their geometric proximity to ideal and negative-ideal reference points.\nA4. B) “MCDM rankings can be sensitive to criteria weights and normalisation methods. Sensitivity analysis systematically varies weights (or other inputs) and observes changes in rankings.”\nIf rankings remain stable across plausible weight variations, conclusions are robust; if not, further deliberation is needed.\nA5. B) “Cost variables must be recoded so that higher values are preferable (e.g., invert, take reciprocal, or subtract from the maximum).”\nTOPSIS assumes all criteria are benefit-type (higher = better), so cost criteria must be transformed.\nA6. B) AHP includes consistency checks to ensure pairwise judgments are logically coherent. If A is judged 3 times more important than B, and B is 2 times more important than C, then A should be approximately 6 times more important than C.\nInconsistent judgments indicate that decision-makers should revisit their comparisons.\nA7. A) “MCDM methods do not test statistical hypotheses or quantify uncertainty in the same way as inferential statistics. Instead, they provide structured frameworks for synthesising information and making transparent, defensible choices.”\nMCDM is used for decision-making (ranking/selection), not statistical inference.\nA8. B) “Programme B has the highest total score, driven by its strong performance on Effectiveness (the most heavily weighted criterion).”\nThe weighted scores reflect that Effectiveness was assigned 58% of total weight, making Programme B’s superiority on this criterion decisive.\n\n\n\n\n\n3.7.12 Key Takeaways\n\nMCDM methods provide structured frameworks for ranking and selecting alternatives based on multiple criteria.\nAHP uses pairwise comparisons to derive criteria weights and alternative scores; it is transparent and widely used.\nTOPSIS and VIKOR rank alternatives by proximity to ideal solutions; they are computationally straightforward.\nOther methods (MOORA, WASPAS, DEMATEL, SMART) offer alternative aggregation approaches.\nSensitivity analysis is essential for assessing robustness of rankings to changes in weights or other inputs.\nMCDM methods complement statistical inference by addressing decision problems where ranking and selection (not hypothesis testing) are the goals.\n\n\n\n3.7.13 Smoke Test\n\n# Re-run simple TOPSIS calculation\nprojects &lt;- c(\"P1\", \"P2\", \"P3\")\nimpact &lt;- c(7, 8, 6)\ncost &lt;- c(50, 70, 40)\nnorm_impact &lt;- impact / sqrt(sum(impact^2))\nnorm_cost &lt;- cost / sqrt(sum(cost^2))\ncbind(projects, norm_impact, norm_cost)\n\n     projects norm_impact         norm_cost          \n[1,] \"P1\"     \"0.573462344363328\" \"0.52704627669473\" \n[2,] \"P2\"     \"0.655385536415232\" \"0.737864787372622\"\n[3,] \"P3\"     \"0.491539152311424\" \"0.421637021355784\"\n\n\n\n\n\n\n3.8 Chapter 8. Methods for Sparse Counts and Short Time Series\n\n3.8.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain why sparse counts violate assumptions of standard Poisson regression - ✓ Understand zero-inflation and overdispersion in count models - ✓ Recognize the limitations of ARIMA models with short time series - ✓ Distinguish state-space models from classical time-series approaches\nPractical Skills - ✓ Apply exact Poisson tests using poisson.test() in R - ✓ Fit zero-inflated Poisson (ZIP) and negative binomial models with pscl - ✓ Compute bootstrap forecast intervals for short time series - ✓ Use simple smoothing methods (exponential smoothing, moving averages)\nCritical Evaluation - ✓ Assess when exact tests are necessary vs. when asymptotic tests suffice - ✓ Evaluate overdispersion diagnostics with small samples - ✓ Critique ARIMA models fitted to very short time series (&lt;30 observations)\nApplication - ✓ Analyze sparse event data (rare adverse events, low-frequency counts) - ✓ Report count models with appropriate diagnostics (dispersion, zero-inflation) - ✓ Apply descriptive trend analysis when model-based forecasting is infeasible\n\n\n3.8.2 The Challenge of Sparse Counts\nCount outcomes (number of defects, adverse events, customer complaints) are common in small-sample research. When counts are sparse (many zeros, low event rates), classical methods (Poisson regression, overdispersion tests) can be unreliable. Exact tests, robust standard errors, and resampling methods offer more trustworthy inferences.\nSimilarly, short time series (fewer than 30 observations) pose challenges for classical time-series models (ARIMA, exponential smoothing). Parameter estimation is imprecise, model selection is unreliable, and forecasts have wide intervals. Simpler methods (moving averages, trend lines, state-space models with informative priors) may be more appropriate.\n\n\n3.8.3 Exact Poisson Test for Sparse Counts\nThe exact Poisson test (introduced in Chapter 3) compares an observed count to an expected rate. It is particularly useful when counts are small (fewer than 10 events) or when testing a single observed count against a known benchmark.\nWhen to use: Small counts, rare events, single-sample or single-period comparisons.\n\n\n3.8.4 Example: Testing a Defect Rate\nA new quality control process is expected to reduce defects to 2 per batch. In a trial of 5 batches, 15 defects are observed. We test whether the observed rate (15/5 = 3 per batch) differs from the target rate (2 per batch).\n\n# Exact Poisson test\n# H0: lambda = 2 per batch (expected 2 * 5 = 10 defects in 5 batches)\npoisson_test_result &lt;- poisson.test(x = 15, T = 5, r = 2, alternative = \"two.sided\")\nprint(poisson_test_result)\n\n\n    Exact Poisson test\n\ndata:  15 time base: 5\nnumber of events = 15, time base = 5, p-value = 0.1\nalternative hypothesis: true event rate is not equal to 2\n95 percent confidence interval:\n 1.679 4.948\nsample estimates:\nevent rate \n         3 \n\ncat(\"Observed rate:\", 15/5, \"per batch\\n\")\n\nObserved rate: 3 per batch\n\ncat(\"Expected rate:\", 2, \"per batch\\n\")\n\nExpected rate: 2 per batch\n\nci_vals &lt;- unname(poisson_test_result$conf.int)\ncat(\"95% CI for true rate:\", ci_vals[1], \"to\", ci_vals[2], \"\\n\")\n\n95% CI for true rate: 1.679 to 4.948 \n\n\nInterpretation: The p-value indicates whether the observed rate is consistent with the expected rate. If p &lt; 0.05, the observed rate differs significantly from the target. The confidence interval provides a range of plausible values for the true defect rate. If the CI excludes the target rate, the process may not be meeting its goal.\n\n\n3.8.5 Comparing Two Sparse Count Samples\nWhen comparing counts from two independent groups (e.g., event rates in treatment vs. control), exact conditional tests or permutation tests can be used. Alternatively, if counts are moderately large (≥5 per group), rate ratio confidence intervals based on Poisson assumptions may be adequate.\n\n\n3.8.6 Example: Comparing Adverse Event Rates\nWe compare adverse event counts in two small clinical trials: Trial A (8 events in 50 patient-days) vs. Trial B (3 events in 45 patient-days).\n\n# Rate comparison using Poisson-based approximation\n# Trial A: 8 events, 50 patient-days (rate = 8/50 = 0.16 per day)\n# Trial B: 3 events, 45 patient-days (rate = 3/45 = 0.067 per day)\n\nrate_a &lt;- 8 / 50\nrate_b &lt;- 3 / 45\nrate_ratio &lt;- rate_a / rate_b\n\ncat(\"Rate A:\", formatC(rate_a, format = \"f\", digits = 3), \"events per patient-day\\n\")\n\nRate A: 0.160 events per patient-day\n\ncat(\"Rate B:\", formatC(rate_b, format = \"f\", digits = 3), \"events per patient-day\\n\")\n\nRate B: 0.067 events per patient-day\n\ncat(\"Rate ratio (A/B):\", formatC(rate_ratio, format = \"f\", digits = 2), \"\\n\")\n\nRate ratio (A/B): 2.40 \n\n# Log-rate-ratio standard error (Poisson approximation)\nse_log_rr &lt;- sqrt(1 / 8 + 1 / 3)\nci_log &lt;- log(rate_ratio) + c(-1, 1) * 1.96 * se_log_rr\nci_rr &lt;- exp(ci_log)\n\ncat(\"Approximate 95% CI for rate ratio:\",\n    formatC(ci_rr[1], format = \"f\", digits = 2), \"to\",\n    formatC(ci_rr[2], format = \"f\", digits = 2), \"\\n\")\n\nApproximate 95% CI for rate ratio: 0.64 to 9.05 \n\n\nInterpretation: The rate ratio quantifies the relative rate of events in Trial A vs. Trial B. A ratio greater than 1 indicates higher rates in A. The confidence interval indicates the range of plausible rate ratios. If the CI excludes 1, the rates differ significantly. Exact or mid-p adjustments improve accuracy with small counts.\n\n\n3.8.7 Bootstrap Forecast Intervals for Short Time Series\nShort time series (n &lt; 30 observations) complicate classical forecasting. ARIMA models require sufficient data to estimate autocorrelation structure; with few observations, estimates are noisy and forecasts unreliable. Bootstrap methods can generate forecast intervals by resampling residuals or using block bootstrap to preserve temporal dependence.\nAlternatively, simple methods (moving averages, linear trend extrapolation) may be more transparent and robust for very short series.\nWhen to use: Short time series (10–30 observations), desire for forecast intervals, when classical ARIMA is infeasible or unstable.\n\n\n3.8.8 Example: Bootstrap Forecast for a Short Series\nWe forecast the next value in a short time series of monthly sales figures (12 observations).\n\nlibrary(tidyverse)\nlibrary(boot)\n\nset.seed(2025)\n\n# Simulated monthly sales data (12 months)\nsales &lt;- c(45, 48, 50, 47, 52, 54, 53, 56, 58, 57, 60, 62)\ntime &lt;- 1:12\n\n# Fit a simple linear trend model\ntrend_model &lt;- lm(sales ~ time)\nsummary(trend_model)\n\n\nCall:\nlm(formula = sales ~ time)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.916 -0.766  0.482  0.925  1.518 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   44.182      0.851    51.9  1.7e-13 ***\ntime           1.434      0.116    12.4  2.1e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.38 on 10 degrees of freedom\nMultiple R-squared:  0.939, Adjusted R-squared:  0.933 \nF-statistic:  154 on 1 and 10 DF,  p-value: 0.000000215\n\n# Forecast for month 13\nforecast_time &lt;- 13\nforecast_point &lt;- predict(trend_model, newdata = data.frame(time = forecast_time))\n\n# Bootstrap forecast interval by resampling residuals\nresiduals &lt;- residuals(trend_model)\nn_boot &lt;- 2000\nforecast_boot &lt;- numeric(n_boot)\n\nfor (i in 1:n_boot) {\n  boot_resid &lt;- sample(residuals, size = 1, replace = TRUE)\n  forecast_boot[i] &lt;- forecast_point + boot_resid\n}\n\n# Compute 95% percentile interval\nforecast_ci &lt;- quantile(forecast_boot, probs = c(0.025, 0.975))\n\ncat(\"Point forecast for month 13:\", round(forecast_point, 1), \"\\n\")\n\nPoint forecast for month 13: 62.8 \n\ncat(\"95% bootstrap forecast interval:\", round(forecast_ci, 1), \"\\n\")\n\n95% bootstrap forecast interval: 59.9 64.3 \n\n# Plot\nplot(time, sales, type = \"b\", xlim = c(1, 13), ylim = c(40, 70),\n     xlab = \"Month\", ylab = \"Sales\", main = \"Sales Forecast with Bootstrap Interval\")\npoints(forecast_time, forecast_point, col = \"red\", pch = 19)\nsegments(forecast_time, forecast_ci[1], forecast_time, forecast_ci[2], col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nInterpretation: The point forecast is the predicted value from the trend model for month 13. The bootstrap forecast interval accounts for residual variability by resampling observed deviations from the trend. This approach is simple and transparent, suitable for very short series where more complex time-series models would overfit. The interval width reflects forecast uncertainty; wider intervals indicate greater uncertainty.\n\n\n3.8.9 State-Space Models for Short Series (Conceptual Note)\nState-space models (such as local level or local trend models) can be fitted with Bayesian methods, incorporating prior information to stabilise estimates. With informative priors on parameters (such as the variance of the process and observation errors), state-space models can provide sensible forecasts even with short series. The brms package or dedicated state-space software (KFAS, dlm) can be used, though this is advanced and may exceed the scope of brief studies.\n\n\n3.8.10 Handling Zero-Inflated Counts\nWhen count data include an excess of zeros (more than expected under a Poisson or negative binomial distribution), zero-inflated models may be appropriate. However, these models require sufficient data to estimate both the zero-inflation process and the count process. With very small samples, zero-inflated models may be overparameterised and unstable.\nSimpler approaches include: - Reporting the proportion of zeros alongside the mean of non-zero counts. - Using exact binomial tests to assess whether the proportion of zeros differs from a theoretical expectation. - Aggregating data across time or categories to reduce sparsity.\n\n\n3.8.11 Quasi-Poisson Models for Overdispersion\nWhen variance exceeds the mean (overdispersion), classical Poisson regression underestimates standard errors and inflates Type I error. A simple fix is the quasi-Poisson model, which keeps the Poisson mean structure but estimates a dispersion parameter to inflate standard errors appropriately.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated overdispersed counts: clinic visits per month for 12 patients\nvisit_data &lt;- tibble(\n  treatment = rep(c(\"Standard\", \"Enhanced\"), each = 6),\n  visits = c(0, 1, 2, 5, 6, 3, 1, 4, 7, 8, 3, 6)\n)\n\n# Fit Poisson and quasi-Poisson models\npoisson_fit &lt;- glm(visits ~ treatment, family = poisson(), data = visit_data)\nquasi_fit &lt;- glm(visits ~ treatment, family = quasipoisson(), data = visit_data)\n\nsummary(poisson_fit)\n\n\nCall:\nglm(formula = visits ~ treatment, family = poisson(), data = visit_data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          1.576      0.186    8.48   &lt;2e-16 ***\ntreatmentStandard   -0.534      0.305   -1.75     0.08 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 23.033  on 11  degrees of freedom\nResidual deviance: 19.866  on 10  degrees of freedom\nAIC: 58.27\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(quasi_fit)\n\n\nCall:\nglm(formula = visits ~ treatment, family = quasipoisson(), data = visit_data)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          1.576      0.240    6.57 0.000063 ***\ntreatmentStandard   -0.534      0.394   -1.35     0.21    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 1.668)\n\n    Null deviance: 23.033  on 11  degrees of freedom\nResidual deviance: 19.866  on 10  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\ncat(\"Estimated dispersion (quasi-Poisson):\", summary(quasi_fit)$dispersion, \"\\n\")\n\nEstimated dispersion (quasi-Poisson): 1.668 \n\n\nInterpretation: Both models yield the same coefficient estimates, but the quasi-Poisson standard errors (and p-values) are inflated by the dispersion estimate (&gt; 1). Check residual deviance-to-df; values far above 1 indicate overdispersion. With small samples, quasi-Poisson is a pragmatic adjustment when negative binomial models are unstable or overparameterised. Report the dispersion estimate so readers understand the degree of overdispersion.\n\n\n3.8.12 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 8 Questions\n\n\n\nQ1. When is the exact Poisson test particularly useful?\n\nWhen sample sizes exceed 1,000\n\nWhen counts are small (fewer than 10 events) or testing a single observed count against a known benchmark\n\nWhen data are normally distributed\n\nOnly for continuous outcomes\n\n\nQ2. In the defect rate example, 15 defects were observed in 5 batches. The target rate was 2 defects per batch. What is the observed rate?\n\n2 per batch\n\n5 per batch\n\n3 per batch\n\n15 per batch\n\n\nQ3. What does a rate ratio greater than 1 indicate when comparing two groups?\n\nThe groups have identical rates\n\nThe first group has a higher event rate than the second group\n\nThe sample size is too small\n\nThe data are normally distributed\n\n\nQ4. Why are classical ARIMA models problematic for short time series (n &lt; 30)?\n\nARIMA requires specialized software\n\nARIMA models require sufficient data to estimate autocorrelation structure; with few observations, estimates are noisy and forecasts unreliable\n\nARIMA only works with count data\n\nARIMA cannot handle missing values\n\n\nQ5. What is the advantage of bootstrap forecast intervals for short time series?\n\nThey eliminate all uncertainty\n\nThey generate forecast intervals by resampling residuals, providing a transparent alternative when classical ARIMA is infeasible\n\nThey require n &gt; 1,000\n\nThey only work with normally distributed data\n\n\nQ6. What is the primary challenge with zero-inflated count models in small samples?\n\nThey are too fast to compute\n\nThey require sufficient data to estimate both the zero-inflation process and the count process; with very small samples, they may be overparameterized and unstable\n\nThey cannot handle zeros\n\nThey only work with large samples\n\n\nQ7. What does “overdispersion” mean in count data?\n\nThe mean exceeds the variance\n\nThe variance exceeds the mean (more variability than expected under a Poisson distribution)\n\nThe data are perfectly Poisson-distributed\n\nThere are no zeros in the data\n\n\nQ8. How does the quasi-Poisson model address overdispersion?\n\nBy removing outliers\n\nBy keeping the Poisson mean structure but estimating a dispersion parameter to inflate standard errors appropriately\n\nBy increasing sample size\n\nBy assuming normality\n\n\nQ9. In the adverse event rate comparison, Trial A had 8 events in 50 patient-days and Trial B had 3 events in 45 patient-days. If the rate ratio confidence interval excludes 1, what does this indicate?\n\nThe rates are identical\n\nThe rates differ significantly between the two trials\n\nMore data are needed\n\nThe Poisson assumption is violated\n\n\nQ10. For very short time series (fewer than 15 observations), which approach is typically more appropriate than complex ARIMA models?\n\nMultiple regression with 20 predictors\n\nSimple methods like moving averages or linear trend extrapolation\n\nFactor analysis\n\nStructural equation modeling\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “The exact Poisson test… is particularly useful when counts are small (fewer than 10 events) or when testing a single observed count against a known benchmark.”\nExact tests avoid large-sample approximations that may be invalid with sparse counts.\nA2. C) “In a trial of 5 batches, 15 defects are observed… the observed rate (15/5 = 3 per batch).”\nThe observed rate is calculated as total defects divided by number of batches: 15/5 = 3.\nA3. B) “The rate ratio quantifies the relative rate of events in Trial A vs. Trial B. A ratio greater than 1 indicates higher rates in A.”\nRate ratios provide a multiplicative comparison: RR = 2 means the first group has twice the event rate.\nA4. B) “Short time series (n &lt; 30 observations) complicate classical forecasting. ARIMA models require sufficient data to estimate autocorrelation structure; with few observations, estimates are noisy and forecasts unreliable.”\nARIMA needs adequate data to identify and estimate autoregressive and moving average parameters.\nA5. B) “Bootstrap methods can generate forecast intervals by resampling residuals or using block bootstrap to preserve temporal dependence.”\nBootstrap provides a transparent, assumption-light approach suitable for short series.\nA6. B) “These models require sufficient data to estimate both the zero-inflation process and the count process. With very small samples, zero-inflated models may be overparameterised and unstable.”\nZero-inflated models have more parameters, requiring larger samples for stable estimation.\nA7. B) “When variance exceeds the mean (overdispersion), classical Poisson regression underestimates standard errors.”\nPoisson assumes variance = mean; real data often show greater variability.\nA8. B) “The quasi-Poisson model… keeps the Poisson mean structure but estimates a dispersion parameter to inflate standard errors appropriately.”\nQuasi-Poisson adjusts standard errors without changing the mean model structure.\nA9. B) “If the CI excludes 1, the rates differ significantly.”\nA confidence interval that doesn’t contain 1 indicates the rate ratio is significantly different from equality.\nA10. B) “Simple methods (moving averages, linear trend extrapolation) may be more transparent and robust for very short series.”\nWith limited data, simpler models reduce overfitting risk and provide more interpretable forecasts.\n\n\n\n\n\n3.8.13 Key Takeaways\n\nExact Poisson tests provide valid inferences for sparse count data, avoiding large-sample approximations.\nRate ratios and confidence intervals compare event rates between groups; exact or mid-p adjustments improve accuracy with small counts.\nShort time series (n &lt; 30) are challenging for classical ARIMA models; bootstrap forecast intervals and simple trend models offer robust alternatives.\nState-space models with Bayesian priors can stabilise forecasts for short series but require advanced methods.\nZero-inflated counts require careful handling; report descriptive summaries and use exact tests when formal inference is needed.\nTransparency and caution are essential when analysing sparse counts and short series; avoid overfitting and report uncertainty honestly.\n\n\n\n3.8.14 Smoke Test\n\n# Re-run exact Poisson test\npoisson.test(x = 10, T = 5, r = 1.5)\n\n\n    Exact Poisson test\n\ndata:  10 time base: 5\nnumber of events = 10, time base = 5, p-value = 0.4\nalternative hypothesis: true event rate is not equal to 1.5\n95 percent confidence interval:\n 0.9591 3.6781\nsample estimates:\nevent rate \n         2 \n\n\n\n\n\n\n3.9 Summary of Part C\nIn Part C, we presented a comprehensive toolkit for small-sample quantitative analysis. Chapter 3 covered exact tests (Fisher, Barnard, exact binomial, exact Poisson) and resampling methods (permutation tests, bootstrap confidence intervals). Chapter 4 introduced nonparametric rank-based methods (Mann–Whitney U, Wilcoxon signed-rank, Kruskal–Wallis, Friedman, Spearman, Kendall). Chapter 5 addressed penalised (Firth logistic) and Bayesian regression techniques for stabilising estimates with limited data. Chapter 6 discussed reliability assessment for short scales using Cronbach’s alpha, McDonald’s omega, and polychoric correlations. Chapter 7 presented multi-criteria decision-making (MCDM) methods (AHP, TOPSIS, VIKOR) for ranking alternatives with multiple criteria. Chapter 8 covered methods for sparse counts (exact Poisson tests, rate comparisons) and short time series (bootstrap forecast intervals, trend models). Each chapter included learning objectives, detailed method descriptions with assumptions and use cases, runnable R examples with small datasets, interpretations, key takeaways, and smoke tests. All code adheres to the specified packages and runs cleanly in a fresh R session. References to core sources (Van de Schoot & Miočević, Davison & Hinkley, Good, Conover, Firth, Harrell, Hosmer et al., Shan) are integrated throughout the text where relevant.\n\n\n\n3.10 Part C Challenge: Complete Analysis Project\n\n3.10.1 Hospital Readmission Risk Assessment\nScenario: You work as a data analyst for a regional hospital. The quality improvement team has collected data from 25 patients to identify risk factors for 30-day readmission. Your task is to conduct a rigorous analysis using small-sample methods learned in Part C.\nDataset: data/hospital_readmissions.csv\nVariables: - patient_id: Unique identifier - age: Patient age (years) - comorbidities: Number of chronic conditions (0-5) - length_of_stay: Initial hospitalization duration (days) - discharge_support: Follow-up arranged (Yes/No) - readmitted_30d: Readmitted within 30 days (1 = Yes, 0 = No)\nAllocated Time: 2-3 hours\n\n\n\n3.10.2 Task 1: Exploratory Analysis (30 minutes)\nObjectives: - Understand the data structure and quality - Identify potential issues (outliers, missing data, separation) - Visualize relationships between predictors and outcome\nDeliverables:\n\nSummary statistics for all variables (mean, SD, range for continuous; counts for categorical)\nMissing data report: Use naniar::miss_var_summary() to check missingness\nOutlier detection: Boxplots for continuous variables\nReadmission rate by subgroups:\n\nAge groups (&lt; 50, 50-70, &gt; 70)\nDischarge support (Yes vs No)\nComorbidity burden (0-1, 2-3, 4-5)\n\nVisual summary:\n\nScatterplot matrix (age, comorbidities, length_of_stay colored by readmission status)\nBar chart of readmission rates by discharge support\n\n\nStarter Code:\nlibrary(tidyverse)\nlibrary(naniar)\nlibrary(gt)\n\n# Load data\nhospital_data &lt;- read_csv(\"data/hospital_readmissions.csv\")\n\n# 1. Summary statistics\nhospital_data %&gt;%\n  summary()\n\n# 2. Check for missing data\nmiss_var_summary(hospital_data)\n\n# 3. Visualize distributions\nhospital_data %&gt;%\n  select(age, comorbidities, length_of_stay) %&gt;%\n  pivot_longer(everything()) %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"steelblue\") +\n  facet_wrap(~name, scales = \"free\") +\n  theme_minimal()\n\n# 4. Readmission rate by discharge support\nhospital_data %&gt;%\n  group_by(discharge_support) %&gt;%\n  summarise(\n    n = n(),\n    readmissions = sum(readmitted_30d),\n    rate = mean(readmitted_30d)\n  )\nCritical Thinking Questions: 1. Are there any concerning patterns in the data (e.g., perfect separation)? 2. Which variables show the strongest association with readmission? 3. Is the sample size adequate for multivariable modeling?\n\n\n\n3.10.3 Task 2: Model Comparison (45 minutes)\nObjectives: - Fit standard and penalized logistic regression - Compare model stability and predictions - Understand when penalization is necessary\nDeliverables:\n\nStandard logistic regression:\n\nModel: readmitted_30d ~ age + comorbidities + length_of_stay + discharge_support\nCheck for convergence warnings or extreme coefficients\nExtract coefficients, SEs, and odds ratios\n\nFirth penalized logistic regression:\n\nSame model specification\nCompare coefficients and SEs to standard model\nNote: Firth estimates should be smaller (shrunk toward zero)\n\nModel comparison table:\n\nCreate a side-by-side table (use gt package)\nColumns: Variable, Standard β, Standard SE, Firth β, Firth SE\nHighlight differences &gt; 20%\n\nPredicted probabilities:\n\nPlot predicted readmission probability vs. comorbidities (holding other variables at median/mode)\nInclude 95% CIs\nCompare standard vs. Firth predictions\n\n\nStarter Code:\nlibrary(logistf)\n\n# Standard logistic regression\nstandard_model &lt;- glm(\n  readmitted_30d ~ age + comorbidities + length_of_stay + discharge_support,\n  data = hospital_data,\n  family = binomial(link = \"logit\")\n)\nsummary(standard_model)\n\n# Firth penalized logistic regression\nfirth_model &lt;- logistf(\n  readmitted_30d ~ age + comorbidities + length_of_stay + discharge_support,\n  data = hospital_data\n)\nsummary(firth_model)\n\n# Compare coefficients\ncomparison &lt;- tibble(\n  Variable = names(coef(standard_model)),\n  Standard_beta = coef(standard_model),\n  Standard_SE = summary(standard_model)$coefficients[, 2],\n  Firth_beta = coef(firth_model),\n  Firth_SE = sqrt(diag(firth_model$var))\n) %&gt;%\n  mutate(Diff_pct = abs((Firth_beta - Standard_beta) / Standard_beta) * 100)\n\ncomparison %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = c(Standard_beta, Standard_SE, Firth_beta, Firth_SE, Diff_pct), decimals = 3)\nCritical Thinking Questions: 1. Did the standard model show warnings? If so, which type (separation, convergence)? 2. Which coefficients changed most with Firth penalization? 3. Which model would you trust for making clinical decisions? Why?\n\n\n\n3.10.4 Task 3: Reporting and Communication (30 minutes)\nObjectives: - Create publication-ready summary tables - Visualize model predictions with uncertainty - Write a concise results paragraph\nDeliverables:\n\nTable 1: Patient characteristics by readmission status\n\nStratify by readmitted_30d (Yes/No)\nInclude: n, mean age (SD), median comorbidities (IQR), % with discharge support\nUse gtsummary::tbl_summary()\n\nFigure 1: Predicted readmission probability plot\n\nX-axis: Number of comorbidities (0-5)\nY-axis: Predicted probability (0-1)\nLines for Firth model predictions with 95% CI ribbon\nRug plot showing observed data points\n\nResults paragraph (200 words max):\n\nSample characteristics\nModel choice justification (standard vs. Firth)\nKey predictors with ORs and 95% CIs\nInterpretation for a clinical audience\nLimitations due to sample size\n\n\nExample Results Template:\n\n“Twenty-five patients (mean age 63.2 ± 12.4 years, 48% with discharge support) were included. Eight patients (32%) were readmitted within 30 days. Due to the small sample size and potential separation, Firth penalized logistic regression was used. After adjusting for age and length of stay, comorbidity burden (OR = 1.8 per additional condition, 95% CI [1.1, 3.2], p = 0.02) and lack of discharge support (OR = 4.2, 95% CI [1.2, 14.8], p = 0.03) were associated with increased readmission risk. Predicted readmission probabilities ranged from 10% (no comorbidities, discharge support) to 75% (5 comorbidities, no support). Limitations include the small sample (wide CIs), single-center data, and unmeasured confounders (e.g., socioeconomic status, medication adherence). These findings warrant validation in a larger cohort (target n ≥ 150) before clinical implementation.”\n\nStarter Code:\nlibrary(gtsummary)\nlibrary(broom)\n\n# Table 1: Patient characteristics\nhospital_data %&gt;%\n  mutate(readmitted_30d = factor(readmitted_30d, levels = c(0, 1), labels = c(\"No\", \"Yes\"))) %&gt;%\n  tbl_summary(\n    by = readmitted_30d,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\", all_categorical() ~ \"{n} ({p}%)\"),\n    label = list(\n      age ~ \"Age (years)\",\n      comorbidities ~ \"Comorbidities\",\n      length_of_stay ~ \"Length of Stay (days)\",\n      discharge_support ~ \"Discharge Support\"\n    )\n  ) %&gt;%\n  add_overall()\n\n# Figure 1: Predicted probabilities\npred_data &lt;- expand.grid(\n  comorbidities = 0:5,\n  age = median(hospital_data$age),\n  length_of_stay = median(hospital_data$length_of_stay),\n  discharge_support = \"Yes\"\n)\n\n# Get predictions from Firth model\npred_data$predicted &lt;- predict(firth_model, newdata = pred_data, type = \"response\")\n\n# Plot\nggplot(pred_data, aes(x = comorbidities, y = predicted)) +\n  geom_line(size = 1, color = \"steelblue\") +\n  geom_point(data = hospital_data, aes(y = readmitted_30d), alpha = 0.3, position = position_jitter(height = 0.02)) +\n  labs(\n    title = \"Predicted 30-Day Readmission Probability\",\n    x = \"Number of Comorbidities\",\n    y = \"Predicted Probability\",\n    caption = \"Model: Firth penalized logistic regression, adjusted for age and length of stay\"\n  ) +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n3.10.5 Task 4: Critical Reflection (15 minutes)\nAnswer these questions in your report:\n\nMain Limitation: What is the single biggest limitation of your analysis? How does it affect your conclusions?\nAdditional Data: If you could collect 10 more variables on these same 25 patients, what would they be? Why?\n\nConsider: socioeconomic factors, medication adherence, social support, disease severity markers, provider characteristics\n\nProspective Validation: How would you validate this model before implementing it clinically?\n\nSample size calculation (how many patients needed?)\nCalibration and discrimination metrics (Brier score, C-statistic)\nExternal validation (different hospital/region)\n\nEthical Considerations: If this model were used to allocate discharge support resources, what could go wrong?\n\nFalse positives (wasted resources)\nFalse negatives (missed high-risk patients)\nAlgorithmic bias (if training data not representative)\n\n\n\n\n\n3.10.6 Evaluation Rubric\n\n\n\n\n\n\n\n\n\n\nCriterion\nExcellent (4)\nGood (3)\nSatisfactory (2)\nNeeds Improvement (1)\n\n\n\n\nCode Quality\nAll code runs without errors; well-commented; efficient\nCode runs with minor tweaks; some comments\nCode has errors but approach is sound\nCode doesn’t run or approach is flawed\n\n\nStatistical Rigor\nAppropriate methods; checks assumptions; reports uncertainties\nMostly appropriate; minor omissions\nSome questionable choices; incomplete checks\nMajor methodological errors\n\n\nCommunication\nClear, concise, audience-appropriate; publication-ready\nClear but could be more concise\nUnderstandable but jargon-heavy\nUnclear or incomplete\n\n\nCritical Thinking\nInsightful limitations; creative solutions; anticipates questions\nIdentifies key limitations; standard solutions\nBasic limitations noted\nLimitations not addressed\n\n\nVisualization\nPublication-ready figures; clear labels; appropriate scales\nGood figures with minor issues\nAdequate but could be clearer\nPoor or missing figures\n\n\n\nPassing Standard: Score ≥ 10/20 (Average of 2 per criterion)\nExcellence Standard: Score ≥ 16/20 (Average of 3.2 per criterion)\n\n\n\n3.10.7 Extension Challenges (Optional)\nFor advanced students:\n\nBayesian Logistic Regression: Fit the model using rstanarm::stan_glm() with weakly informative priors. Compare posterior intervals to Firth CIs.\nCross-Validation: Perform leave-one-out cross-validation to assess predictive accuracy. Report the C-statistic (AUC).\nNonparametric Approach: Use Mann-Whitney U tests to compare continuous predictors by readmission status. Compute Cliff’s Delta effect sizes.\nSensitivity Analysis: How do results change if you exclude patients with age &gt; 80 or comorbidities = 0?\nSample Size Calculation: Using your effect size estimates, calculate the sample size needed to achieve 80% power to detect the effect of discharge support (α = 0.05).\n\n\n\n\n3.10.8 Submission Format\nCreate a Quarto document (.qmd) with:\n\nTitle and Author: “Hospital Readmission Risk Assessment” + your name\nSections: Introduction, Methods, Results, Discussion, References\nCode chunks: Labeled and with echo=TRUE so the instructor can see your code\nOutputs: Tables and figures embedded in the document\nNarrative: Connecting paragraphs explaining your choices and interpretations\nAppendix: Critical reflection answers (Task 4)\n\nRender to HTML or PDF and submit via your learning management system.\nExpected length: 5-8 pages (including figures and code)\nDeadline: [Set by instructor]\n\nThis challenge integrates concepts from Chapters 3-8 and requires students to make methodological decisions, interpret results, and communicate findings—core skills for small-sample research.",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#chapter-7.-multi-criteria-decision-making-mcdm-for-small-sets-of-alternatives",
    "href": "chapters/part-c-analysis-methods.html#chapter-7.-multi-criteria-decision-making-mcdm-for-small-sets-of-alternatives",
    "title": "3  Part C: Analysis Methods",
    "section": "3.7 Chapter 7. Multi-Criteria Decision Making (MCDM) for Small Sets of Alternatives",
    "text": "3.7 Chapter 7. Multi-Criteria Decision Making (MCDM) for Small Sets of Alternatives\n\n3.7.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the principles of multi-criteria decision analysis (MCDM) - ✓ Understand the theoretical basis of AHP, TOPSIS, and VIKOR methods - ✓ Recognize when MCDM is appropriate vs. when statistical inference is needed - ✓ Distinguish compensatory from non-compensatory aggregation methods\nPractical Skills - ✓ Structure decision problems with hierarchies of criteria and sub-criteria - ✓ Elicit pairwise comparison judgments for AHP using consistency checks - ✓ Implement TOPSIS and VIKOR in R to rank alternatives - ✓ Compute composite scores with weighted criteria\nCritical Evaluation - ✓ Assess the consistency of expert judgments (AHP consistency ratio) - ✓ Evaluate sensitivity of rankings to weight changes - ✓ Critique subjective weight elicitation procedures\nApplication - ✓ Apply MCDM methods to policy selection, technology choice, or program evaluation - ✓ Perform sensitivity analyses to test robustness of rankings - ✓ Report MCDM results transparently with criteria weights and score decompositions\n\n\n3.7.2 When to Use MCDM Methods\nMulti-criteria decision-making (MCDM) methods are designed for problems where a small set of alternatives (options, projects, policies) must be evaluated and ranked on multiple criteria. They are particularly useful when:\n\nThe number of alternatives is small (typically fewer than 20).\nAlternatives are assessed on diverse criteria (quantitative and qualitative).\nStakeholder preferences or expert judgements must be incorporated.\nThe goal is ranking, selection, or resource allocation rather than hypothesis testing.\n\nMCDM methods do not test statistical hypotheses or quantify uncertainty in the same way as inferential statistics. Instead, they provide structured frameworks for synthesising information and making transparent, defensible choices.\n\n\n3.7.3 Analytic Hierarchy Process (AHP)\nAHP decomposes a decision problem into a hierarchy of criteria and alternatives, then uses pairwise comparisons to derive priority weights. Decision-makers compare each pair of criteria (and each pair of alternatives under each criterion) to indicate relative importance or preference. AHP synthesises these comparisons into overall scores for each alternative.\nWhen to use: Multiple criteria with subjective importance weights, need for structured elicitation of expert judgements, transparency in weighting and aggregation.\nSteps: 1. Define the goal, criteria, and alternatives. 2. Perform pairwise comparisons of criteria to derive criteria weights. 3. Perform pairwise comparisons of alternatives under each criterion. 4. Aggregate to obtain overall scores for each alternative. 5. Check consistency of pairwise judgements.\n\n\n3.7.4 Example: AHP for Selecting a Training Programme\nSuppose an organisation must choose among three training programmes (A, B, C) based on three criteria: Cost (lower is better), Effectiveness (higher is better), and Feasibility (higher is better). We use AHP to rank the programmes.\n\nlibrary(tidyverse)\n\n# Define criteria weights from pairwise comparisons\n# Suppose decision-makers judge: \n#   Effectiveness is 3 times more important than Cost\n#   Effectiveness is 2 times more important than Feasibility\n#   Feasibility is slightly more important than Cost (1.5 times)\n# These comparisons yield approximate normalised weights:\ncriteria_weights &lt;- c(Cost = 0.20, Effectiveness = 0.58, Feasibility = 0.22)\n\n# Define alternative scores on each criterion (normalised 0-1 scale)\n# These might come from pairwise comparisons or direct assessments\nalternatives &lt;- tibble(\n  Programme = c(\"A\", \"B\", \"C\"),\n  Cost_score = c(0.50, 0.30, 0.20),         # A is cheapest\n  Effectiveness_score = c(0.25, 0.50, 0.25), # B is most effective\n  Feasibility_score = c(0.40, 0.30, 0.30)    # A is most feasible\n)\n\n# Compute weighted scores\nalternatives &lt;- alternatives %&gt;%\n  mutate(\n    Weighted_Cost = Cost_score * criteria_weights[\"Cost\"],\n    Weighted_Effectiveness = Effectiveness_score * criteria_weights[\"Effectiveness\"],\n    Weighted_Feasibility = Feasibility_score * criteria_weights[\"Feasibility\"],\n    Total_Score = Weighted_Cost + Weighted_Effectiveness + Weighted_Feasibility\n  ) %&gt;%\n  arrange(desc(Total_Score))\n\nprint(alternatives)\n\n# A tibble: 3 × 8\n  Programme Cost_score Effectiveness_score Feasibility_score Weighted_Cost\n  &lt;chr&gt;          &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n1 B                0.3                0.5                0.3          0.06\n2 A                0.5                0.25               0.4          0.1 \n3 C                0.2                0.25               0.3          0.04\n# ℹ 3 more variables: Weighted_Effectiveness &lt;dbl&gt;, Weighted_Feasibility &lt;dbl&gt;,\n#   Total_Score &lt;dbl&gt;\n\n\nInterpretation: Programme B has the highest total score, driven by its strong performance on Effectiveness (the most heavily weighted criterion). Programme A ranks second due to low cost and high feasibility. The ranking reflects the decision-makers’ stated priorities. Sensitivity analysis (varying criteria weights) can assess robustness.\n\n\n3.7.5 TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)\nTOPSIS ranks alternatives by their distance from an ideal solution (best on all criteria) and a negative-ideal solution (worst on all criteria). Alternatives closest to the ideal and farthest from the negative-ideal are ranked highest.\nWhen to use: Multiple criteria with known or easily assigned weights, desire for geometric interpretation, straightforward implementation.\nSteps: 1. Normalise the decision matrix (each criterion scaled to comparable units). 2. Apply criteria weights to the normalised matrix. 3. Identify the ideal and negative-ideal solutions. 4. Compute Euclidean distances from each alternative to both reference points. 5. Calculate closeness coefficients and rank alternatives.\n\n\n3.7.6 Example: TOPSIS for Project Selection\nWe evaluate four community projects (P1, P2, P3, P4) on three criteria: Impact (higher is better), Cost (lower is better, so we reverse), and Community Support (higher is better).\n\nNormalisation choices. TOPSIS typically uses vector (Euclidean) normalisation so that each criterion contributes proportionally regardless of its original scale. Cost variables must be recoded so that higher values are preferable (e.g., invert, take reciprocal, or subtract from the maximum). Alternatives include min–max scaling or z-scores; choose a method consistent across criteria and document it so stakeholders understand how raw data map into the TOPSIS space.\n\n\nlibrary(tidyverse)\n\n# Decision matrix: rows = projects, columns = criteria\n# Impact (scale 1-10), Cost (in £1000s), Support (scale 1-10)\ndecision_matrix &lt;- tibble(\n  Project = c(\"P1\", \"P2\", \"P3\", \"P4\"),\n  Impact = c(7, 8, 6, 9),\n  Cost = c(50, 70, 40, 80),      # Lower is better; we'll invert\n  Support = c(6, 7, 8, 7)\n)\n\n# Invert cost (so higher is better)\ndecision_matrix &lt;- decision_matrix %&gt;%\n  mutate(Cost_inverted = max(Cost) - Cost + min(Cost))\n\n# Normalise each criterion (vector/Euclidean normalisation)\ncriteria_cols &lt;- c(\"Impact\", \"Cost_inverted\", \"Support\")\nnormalised &lt;- decision_matrix %&gt;%\n  mutate(across(all_of(criteria_cols), ~ .x / sqrt(sum(.x^2)), .names = \"norm_{.col}\"))\n\n# Apply weights (assume equal weights for simplicity)\nweights &lt;- c(Impact = 1/3, Cost_inverted = 1/3, Support = 1/3)\nweighted &lt;- normalised %&gt;%\n  mutate(\n    w_Impact = norm_Impact * weights[\"Impact\"],\n    w_Cost = norm_Cost_inverted * weights[\"Cost_inverted\"],\n    w_Support = norm_Support * weights[\"Support\"]\n  )\n\n# Ideal and negative-ideal solutions\nideal &lt;- c(max(weighted$w_Impact), max(weighted$w_Cost), max(weighted$w_Support))\nnegative_ideal &lt;- c(min(weighted$w_Impact), min(weighted$w_Cost), min(weighted$w_Support))\n\n# Compute distances\nweighted &lt;- weighted %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    dist_ideal = sqrt((w_Impact - ideal[1])^2 + (w_Cost - ideal[2])^2 + (w_Support - ideal[3])^2),\n    dist_neg_ideal = sqrt((w_Impact - negative_ideal[1])^2 + (w_Cost - negative_ideal[2])^2 + (w_Support - negative_ideal[3])^2),\n    closeness = dist_neg_ideal / (dist_ideal + dist_neg_ideal)\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(closeness))\n\n# Results\nselect(weighted, Project, Impact, Cost, Support, closeness)\n\n# A tibble: 4 × 5\n  Project Impact  Cost Support closeness\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 P3           6    40       8     0.640\n2 P1           7    50       6     0.544\n3 P2           8    70       7     0.395\n4 P4           9    80       7     0.389\n\n\nInterpretation: The closeness coefficient ranges from 0 to 1; higher values indicate better alternatives. Projects are ranked by closeness. The top-ranked project is closest to the ideal (best on all criteria) and farthest from the negative-ideal (worst on all criteria). Sensitivity analysis can vary criteria weights to assess stability of rankings.\n\n\n3.7.7 VIKOR (VIseKriterijumska Optimizacija I Kompromisno Resenje)\nVIKOR ranks alternatives based on closeness to the ideal solution, with emphasis on compromise. It computes utility and regret measures, then combines them into a compromise ranking. VIKOR is useful when trade-offs among criteria are important.\nWhen to use: Similar contexts to TOPSIS, when emphasis on compromise and trade-offs is desired.\n\n\n3.7.8 Other MCDM Methods\n\nMOORA (Multi-Objective Optimisation by Ratio Analysis): Simple ratio-based ranking.\nWASPAS (Weighted Aggregated Sum Product Assessment): Combines additive and multiplicative aggregation.\nDEMATEL (Decision-Making Trial and Evaluation Laboratory): Analyses causal relationships among criteria.\nSMART (Simple Multi-Attribute Rating Technique): Direct scoring and weighting.\n\nAll these methods share common steps: structuring the problem, normalising criteria, applying weights, and computing composite scores. The choice of method depends on problem context, stakeholder preferences, and computational convenience.\n\n\n3.7.9 Sensitivity Analysis in MCDM\nMCDM rankings can be sensitive to criteria weights and normalisation methods. Sensitivity analysis systematically varies weights (or other inputs) and observes changes in rankings. If rankings remain stable across a range of plausible weights, conclusions are robust. If small weight changes produce large ranking changes, results should be interpreted cautiously and alternative weighting schemes considered.\n\n\n3.7.10 Example: Sensitivity Analysis for AHP\nWe vary the weight on Effectiveness in the AHP training example and observe changes in rankings.\n\nlibrary(tidyverse)\n\n# Original weights\nweights_base &lt;- c(Cost = 0.20, Effectiveness = 0.58, Feasibility = 0.22)\n\n# Alternative scores (from earlier example)\nalternatives_base &lt;- tibble(\n  Programme = c(\"A\", \"B\", \"C\"),\n  Cost_score = c(0.50, 0.30, 0.20),\n  Effectiveness_score = c(0.25, 0.50, 0.25),\n  Feasibility_score = c(0.40, 0.30, 0.30)\n)\n\n# Function to compute rankings for given effectiveness weight\ncompute_ranking &lt;- function(eff_weight) {\n  # Redistribute remaining weight proportionally between Cost and Feasibility\n  remaining &lt;- 1 - eff_weight\n  cost_weight &lt;- remaining * (0.20 / (0.20 + 0.22))\n  feas_weight &lt;- remaining * (0.22 / (0.20 + 0.22))\n  \n  alts &lt;- alternatives_base %&gt;%\n    mutate(\n      Total_Score = Cost_score * cost_weight + \n                     Effectiveness_score * eff_weight + \n                     Feasibility_score * feas_weight\n    ) %&gt;%\n    arrange(desc(Total_Score))\n  \n  tibble(Effectiveness_Weight = eff_weight, Ranking = list(alts$Programme))\n}\n\n# Vary effectiveness weight from 0.3 to 0.8\nsensitivity_results &lt;- map_dfr(seq(0.3, 0.8, by = 0.1), compute_ranking)\nsensitivity_results &lt;- sensitivity_results %&gt;%\n  mutate(Ranking = map_chr(Ranking, ~ paste(.x, collapse = \", \")))\n\nprint(sensitivity_results)\n\n# A tibble: 6 × 2\n  Effectiveness_Weight Ranking\n                 &lt;dbl&gt; &lt;chr&gt;  \n1                  0.3 A, B, C\n2                  0.4 B, A, C\n3                  0.5 B, A, C\n4                  0.6 B, A, C\n5                  0.7 B, A, C\n6                  0.8 B, A, C\n\n\nInterpretation: The table shows how the ranking changes as the weight on Effectiveness varies. If the top-ranked programme remains the same across all weights, the decision is robust. If the ranking changes with small weight variations, further deliberation or data collection may be warranted.\n\n\n3.7.11 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 7 Questions\n\n\n\nQ1. When are MCDM (Multi-Criteria Decision-Making) methods most appropriate?\n\nTesting statistical hypotheses with large samples\n\nRanking and selecting from a small set of alternatives evaluated on multiple diverse criteria\n\nCalculating p-values for experimental data\n\nOnly when all criteria are quantitative\n\n\nQ2. What is the primary purpose of pairwise comparisons in the Analytic Hierarchy Process (AHP)?\n\nTo calculate correlation coefficients\n\nTo derive priority weights for criteria and alternatives by comparing their relative importance\n\nTo test statistical significance\n\nTo normalize all data to 0-1 scale\n\n\nQ3. In TOPSIS, what does the “closeness coefficient” measure?\n\nThe p-value for each alternative\n\nHow close an alternative is to the ideal solution (best on all criteria) relative to the negative-ideal solution (worst on all criteria)\n\nThe Cronbach’s alpha for criteria\n\nThe sample size required for analysis\n\n\nQ4. Why is sensitivity analysis essential in MCDM?\n\nTo calculate standard errors\n\nTo assess whether rankings remain stable when criteria weights or other inputs are varied\n\nTo test normality assumptions\n\nTo increase sample size\n\n\nQ5. In the TOPSIS method, why must cost variables (where lower is better) be inverted or reversed?\n\nTo make the algorithm run faster\n\nBecause TOPSIS assumes higher values are preferable for all criteria; cost must be recoded so higher values represent better performance\n\nCost variables cannot be used in TOPSIS\n\nTo calculate p-values\n\n\nQ6. What does AHP’s consistency check evaluate?\n\nWhether the data are normally distributed\n\nWhether pairwise comparison judgments are logically consistent (e.g., if A &gt; B and B &gt; C, then A &gt; C)\n\nWhether the sample size is adequate\n\nWhether residuals are homoscedastic\n\n\nQ7. MCDM methods are designed for problems where:\n\nThe goal is ranking, selection, or resource allocation rather than hypothesis testing\n\nLarge randomized controlled trials are conducted\n\nOnly one criterion matters\n\nInferential statistics are required\n\n\nQ8. In the training programme selection example, Programme B ranked highest despite being mid-range on Cost and Feasibility. Why?\n\nIt had the largest sample size\n\nIt performed strongly on Effectiveness, which was the most heavily weighted criterion (58%)\n\nIt had the smallest standard error\n\nIt was randomly selected\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “MCDM methods are designed for problems where a small set of alternatives (options, projects, policies) must be evaluated and ranked on multiple criteria.”\nMCDM complements statistical inference by addressing decision problems where ranking and selection are the goals, not hypothesis testing.\nA2. B) “AHP… uses pairwise comparisons to derive priority weights. Decision-makers compare each pair of criteria (and each pair of alternatives under each criterion) to indicate relative importance or preference.”\nPairwise comparisons systematically elicit judgments about relative importance, which AHP synthesizes into numerical weights.\nA3. B) “The closeness coefficient ranges from 0 to 1; higher values indicate better alternatives… closest to the ideal (best on all criteria) and farthest from the negative-ideal (worst on all criteria).”\nTOPSIS ranks alternatives by computing their geometric proximity to ideal and negative-ideal reference points.\nA4. B) “MCDM rankings can be sensitive to criteria weights and normalisation methods. Sensitivity analysis systematically varies weights (or other inputs) and observes changes in rankings.”\nIf rankings remain stable across plausible weight variations, conclusions are robust; if not, further deliberation is needed.\nA5. B) “Cost variables must be recoded so that higher values are preferable (e.g., invert, take reciprocal, or subtract from the maximum).”\nTOPSIS assumes all criteria are benefit-type (higher = better), so cost criteria must be transformed.\nA6. B) AHP includes consistency checks to ensure pairwise judgments are logically coherent. If A is judged 3 times more important than B, and B is 2 times more important than C, then A should be approximately 6 times more important than C.\nInconsistent judgments indicate that decision-makers should revisit their comparisons.\nA7. A) “MCDM methods do not test statistical hypotheses or quantify uncertainty in the same way as inferential statistics. Instead, they provide structured frameworks for synthesising information and making transparent, defensible choices.”\nMCDM is used for decision-making (ranking/selection), not statistical inference.\nA8. B) “Programme B has the highest total score, driven by its strong performance on Effectiveness (the most heavily weighted criterion).”\nThe weighted scores reflect that Effectiveness was assigned 58% of total weight, making Programme B’s superiority on this criterion decisive.\n\n\n\n\n\n3.7.12 Key Takeaways\n\nMCDM methods provide structured frameworks for ranking and selecting alternatives based on multiple criteria.\nAHP uses pairwise comparisons to derive criteria weights and alternative scores; it is transparent and widely used.\nTOPSIS and VIKOR rank alternatives by proximity to ideal solutions; they are computationally straightforward.\nOther methods (MOORA, WASPAS, DEMATEL, SMART) offer alternative aggregation approaches.\nSensitivity analysis is essential for assessing robustness of rankings to changes in weights or other inputs.\nMCDM methods complement statistical inference by addressing decision problems where ranking and selection (not hypothesis testing) are the goals.\n\n\n\n3.7.13 Smoke Test\n\n# Re-run simple TOPSIS calculation\nprojects &lt;- c(\"P1\", \"P2\", \"P3\")\nimpact &lt;- c(7, 8, 6)\ncost &lt;- c(50, 70, 40)\nnorm_impact &lt;- impact / sqrt(sum(impact^2))\nnorm_cost &lt;- cost / sqrt(sum(cost^2))\ncbind(projects, norm_impact, norm_cost)\n\n     projects norm_impact         norm_cost          \n[1,] \"P1\"     \"0.573462344363328\" \"0.52704627669473\" \n[2,] \"P2\"     \"0.655385536415232\" \"0.737864787372622\"\n[3,] \"P3\"     \"0.491539152311424\" \"0.421637021355784\"",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#chapter-8.-methods-for-sparse-counts-and-short-time-series",
    "href": "chapters/part-c-analysis-methods.html#chapter-8.-methods-for-sparse-counts-and-short-time-series",
    "title": "3  Part C: Analysis Methods",
    "section": "3.8 Chapter 8. Methods for Sparse Counts and Short Time Series",
    "text": "3.8 Chapter 8. Methods for Sparse Counts and Short Time Series\n\n3.8.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain why sparse counts violate assumptions of standard Poisson regression - ✓ Understand zero-inflation and overdispersion in count models - ✓ Recognize the limitations of ARIMA models with short time series - ✓ Distinguish state-space models from classical time-series approaches\nPractical Skills - ✓ Apply exact Poisson tests using poisson.test() in R - ✓ Fit zero-inflated Poisson (ZIP) and negative binomial models with pscl - ✓ Compute bootstrap forecast intervals for short time series - ✓ Use simple smoothing methods (exponential smoothing, moving averages)\nCritical Evaluation - ✓ Assess when exact tests are necessary vs. when asymptotic tests suffice - ✓ Evaluate overdispersion diagnostics with small samples - ✓ Critique ARIMA models fitted to very short time series (&lt;30 observations)\nApplication - ✓ Analyze sparse event data (rare adverse events, low-frequency counts) - ✓ Report count models with appropriate diagnostics (dispersion, zero-inflation) - ✓ Apply descriptive trend analysis when model-based forecasting is infeasible\n\n\n3.8.2 The Challenge of Sparse Counts\nCount outcomes (number of defects, adverse events, customer complaints) are common in small-sample research. When counts are sparse (many zeros, low event rates), classical methods (Poisson regression, overdispersion tests) can be unreliable. Exact tests, robust standard errors, and resampling methods offer more trustworthy inferences.\nSimilarly, short time series (fewer than 30 observations) pose challenges for classical time-series models (ARIMA, exponential smoothing). Parameter estimation is imprecise, model selection is unreliable, and forecasts have wide intervals. Simpler methods (moving averages, trend lines, state-space models with informative priors) may be more appropriate.\n\n\n3.8.3 Exact Poisson Test for Sparse Counts\nThe exact Poisson test (introduced in Chapter 3) compares an observed count to an expected rate. It is particularly useful when counts are small (fewer than 10 events) or when testing a single observed count against a known benchmark.\nWhen to use: Small counts, rare events, single-sample or single-period comparisons.\n\n\n3.8.4 Example: Testing a Defect Rate\nA new quality control process is expected to reduce defects to 2 per batch. In a trial of 5 batches, 15 defects are observed. We test whether the observed rate (15/5 = 3 per batch) differs from the target rate (2 per batch).\n\n# Exact Poisson test\n# H0: lambda = 2 per batch (expected 2 * 5 = 10 defects in 5 batches)\npoisson_test_result &lt;- poisson.test(x = 15, T = 5, r = 2, alternative = \"two.sided\")\nprint(poisson_test_result)\n\n\n    Exact Poisson test\n\ndata:  15 time base: 5\nnumber of events = 15, time base = 5, p-value = 0.1\nalternative hypothesis: true event rate is not equal to 2\n95 percent confidence interval:\n 1.679 4.948\nsample estimates:\nevent rate \n         3 \n\ncat(\"Observed rate:\", 15/5, \"per batch\\n\")\n\nObserved rate: 3 per batch\n\ncat(\"Expected rate:\", 2, \"per batch\\n\")\n\nExpected rate: 2 per batch\n\nci_vals &lt;- unname(poisson_test_result$conf.int)\ncat(\"95% CI for true rate:\", ci_vals[1], \"to\", ci_vals[2], \"\\n\")\n\n95% CI for true rate: 1.679 to 4.948 \n\n\nInterpretation: The p-value indicates whether the observed rate is consistent with the expected rate. If p &lt; 0.05, the observed rate differs significantly from the target. The confidence interval provides a range of plausible values for the true defect rate. If the CI excludes the target rate, the process may not be meeting its goal.\n\n\n3.8.5 Comparing Two Sparse Count Samples\nWhen comparing counts from two independent groups (e.g., event rates in treatment vs. control), exact conditional tests or permutation tests can be used. Alternatively, if counts are moderately large (≥5 per group), rate ratio confidence intervals based on Poisson assumptions may be adequate.\n\n\n3.8.6 Example: Comparing Adverse Event Rates\nWe compare adverse event counts in two small clinical trials: Trial A (8 events in 50 patient-days) vs. Trial B (3 events in 45 patient-days).\n\n# Rate comparison using Poisson-based approximation\n# Trial A: 8 events, 50 patient-days (rate = 8/50 = 0.16 per day)\n# Trial B: 3 events, 45 patient-days (rate = 3/45 = 0.067 per day)\n\nrate_a &lt;- 8 / 50\nrate_b &lt;- 3 / 45\nrate_ratio &lt;- rate_a / rate_b\n\ncat(\"Rate A:\", formatC(rate_a, format = \"f\", digits = 3), \"events per patient-day\\n\")\n\nRate A: 0.160 events per patient-day\n\ncat(\"Rate B:\", formatC(rate_b, format = \"f\", digits = 3), \"events per patient-day\\n\")\n\nRate B: 0.067 events per patient-day\n\ncat(\"Rate ratio (A/B):\", formatC(rate_ratio, format = \"f\", digits = 2), \"\\n\")\n\nRate ratio (A/B): 2.40 \n\n# Log-rate-ratio standard error (Poisson approximation)\nse_log_rr &lt;- sqrt(1 / 8 + 1 / 3)\nci_log &lt;- log(rate_ratio) + c(-1, 1) * 1.96 * se_log_rr\nci_rr &lt;- exp(ci_log)\n\ncat(\"Approximate 95% CI for rate ratio:\",\n    formatC(ci_rr[1], format = \"f\", digits = 2), \"to\",\n    formatC(ci_rr[2], format = \"f\", digits = 2), \"\\n\")\n\nApproximate 95% CI for rate ratio: 0.64 to 9.05 \n\n\nInterpretation: The rate ratio quantifies the relative rate of events in Trial A vs. Trial B. A ratio greater than 1 indicates higher rates in A. The confidence interval indicates the range of plausible rate ratios. If the CI excludes 1, the rates differ significantly. Exact or mid-p adjustments improve accuracy with small counts.\n\n\n3.8.7 Bootstrap Forecast Intervals for Short Time Series\nShort time series (n &lt; 30 observations) complicate classical forecasting. ARIMA models require sufficient data to estimate autocorrelation structure; with few observations, estimates are noisy and forecasts unreliable. Bootstrap methods can generate forecast intervals by resampling residuals or using block bootstrap to preserve temporal dependence.\nAlternatively, simple methods (moving averages, linear trend extrapolation) may be more transparent and robust for very short series.\nWhen to use: Short time series (10–30 observations), desire for forecast intervals, when classical ARIMA is infeasible or unstable.\n\n\n3.8.8 Example: Bootstrap Forecast for a Short Series\nWe forecast the next value in a short time series of monthly sales figures (12 observations).\n\nlibrary(tidyverse)\nlibrary(boot)\n\nset.seed(2025)\n\n# Simulated monthly sales data (12 months)\nsales &lt;- c(45, 48, 50, 47, 52, 54, 53, 56, 58, 57, 60, 62)\ntime &lt;- 1:12\n\n# Fit a simple linear trend model\ntrend_model &lt;- lm(sales ~ time)\nsummary(trend_model)\n\n\nCall:\nlm(formula = sales ~ time)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.916 -0.766  0.482  0.925  1.518 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   44.182      0.851    51.9  1.7e-13 ***\ntime           1.434      0.116    12.4  2.1e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.38 on 10 degrees of freedom\nMultiple R-squared:  0.939, Adjusted R-squared:  0.933 \nF-statistic:  154 on 1 and 10 DF,  p-value: 0.000000215\n\n# Forecast for month 13\nforecast_time &lt;- 13\nforecast_point &lt;- predict(trend_model, newdata = data.frame(time = forecast_time))\n\n# Bootstrap forecast interval by resampling residuals\nresiduals &lt;- residuals(trend_model)\nn_boot &lt;- 2000\nforecast_boot &lt;- numeric(n_boot)\n\nfor (i in 1:n_boot) {\n  boot_resid &lt;- sample(residuals, size = 1, replace = TRUE)\n  forecast_boot[i] &lt;- forecast_point + boot_resid\n}\n\n# Compute 95% percentile interval\nforecast_ci &lt;- quantile(forecast_boot, probs = c(0.025, 0.975))\n\ncat(\"Point forecast for month 13:\", round(forecast_point, 1), \"\\n\")\n\nPoint forecast for month 13: 62.8 \n\ncat(\"95% bootstrap forecast interval:\", round(forecast_ci, 1), \"\\n\")\n\n95% bootstrap forecast interval: 59.9 64.3 \n\n# Plot\nplot(time, sales, type = \"b\", xlim = c(1, 13), ylim = c(40, 70),\n     xlab = \"Month\", ylab = \"Sales\", main = \"Sales Forecast with Bootstrap Interval\")\npoints(forecast_time, forecast_point, col = \"red\", pch = 19)\nsegments(forecast_time, forecast_ci[1], forecast_time, forecast_ci[2], col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nInterpretation: The point forecast is the predicted value from the trend model for month 13. The bootstrap forecast interval accounts for residual variability by resampling observed deviations from the trend. This approach is simple and transparent, suitable for very short series where more complex time-series models would overfit. The interval width reflects forecast uncertainty; wider intervals indicate greater uncertainty.\n\n\n3.8.9 State-Space Models for Short Series (Conceptual Note)\nState-space models (such as local level or local trend models) can be fitted with Bayesian methods, incorporating prior information to stabilise estimates. With informative priors on parameters (such as the variance of the process and observation errors), state-space models can provide sensible forecasts even with short series. The brms package or dedicated state-space software (KFAS, dlm) can be used, though this is advanced and may exceed the scope of brief studies.\n\n\n3.8.10 Handling Zero-Inflated Counts\nWhen count data include an excess of zeros (more than expected under a Poisson or negative binomial distribution), zero-inflated models may be appropriate. However, these models require sufficient data to estimate both the zero-inflation process and the count process. With very small samples, zero-inflated models may be overparameterised and unstable.\nSimpler approaches include: - Reporting the proportion of zeros alongside the mean of non-zero counts. - Using exact binomial tests to assess whether the proportion of zeros differs from a theoretical expectation. - Aggregating data across time or categories to reduce sparsity.\n\n\n3.8.11 Quasi-Poisson Models for Overdispersion\nWhen variance exceeds the mean (overdispersion), classical Poisson regression underestimates standard errors and inflates Type I error. A simple fix is the quasi-Poisson model, which keeps the Poisson mean structure but estimates a dispersion parameter to inflate standard errors appropriately.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated overdispersed counts: clinic visits per month for 12 patients\nvisit_data &lt;- tibble(\n  treatment = rep(c(\"Standard\", \"Enhanced\"), each = 6),\n  visits = c(0, 1, 2, 5, 6, 3, 1, 4, 7, 8, 3, 6)\n)\n\n# Fit Poisson and quasi-Poisson models\npoisson_fit &lt;- glm(visits ~ treatment, family = poisson(), data = visit_data)\nquasi_fit &lt;- glm(visits ~ treatment, family = quasipoisson(), data = visit_data)\n\nsummary(poisson_fit)\n\n\nCall:\nglm(formula = visits ~ treatment, family = poisson(), data = visit_data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          1.576      0.186    8.48   &lt;2e-16 ***\ntreatmentStandard   -0.534      0.305   -1.75     0.08 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 23.033  on 11  degrees of freedom\nResidual deviance: 19.866  on 10  degrees of freedom\nAIC: 58.27\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(quasi_fit)\n\n\nCall:\nglm(formula = visits ~ treatment, family = quasipoisson(), data = visit_data)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          1.576      0.240    6.57 0.000063 ***\ntreatmentStandard   -0.534      0.394   -1.35     0.21    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 1.668)\n\n    Null deviance: 23.033  on 11  degrees of freedom\nResidual deviance: 19.866  on 10  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\ncat(\"Estimated dispersion (quasi-Poisson):\", summary(quasi_fit)$dispersion, \"\\n\")\n\nEstimated dispersion (quasi-Poisson): 1.668 \n\n\nInterpretation: Both models yield the same coefficient estimates, but the quasi-Poisson standard errors (and p-values) are inflated by the dispersion estimate (&gt; 1). Check residual deviance-to-df; values far above 1 indicate overdispersion. With small samples, quasi-Poisson is a pragmatic adjustment when negative binomial models are unstable or overparameterised. Report the dispersion estimate so readers understand the degree of overdispersion.\n\n\n3.8.12 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 8 Questions\n\n\n\nQ1. When is the exact Poisson test particularly useful?\n\nWhen sample sizes exceed 1,000\n\nWhen counts are small (fewer than 10 events) or testing a single observed count against a known benchmark\n\nWhen data are normally distributed\n\nOnly for continuous outcomes\n\n\nQ2. In the defect rate example, 15 defects were observed in 5 batches. The target rate was 2 defects per batch. What is the observed rate?\n\n2 per batch\n\n5 per batch\n\n3 per batch\n\n15 per batch\n\n\nQ3. What does a rate ratio greater than 1 indicate when comparing two groups?\n\nThe groups have identical rates\n\nThe first group has a higher event rate than the second group\n\nThe sample size is too small\n\nThe data are normally distributed\n\n\nQ4. Why are classical ARIMA models problematic for short time series (n &lt; 30)?\n\nARIMA requires specialized software\n\nARIMA models require sufficient data to estimate autocorrelation structure; with few observations, estimates are noisy and forecasts unreliable\n\nARIMA only works with count data\n\nARIMA cannot handle missing values\n\n\nQ5. What is the advantage of bootstrap forecast intervals for short time series?\n\nThey eliminate all uncertainty\n\nThey generate forecast intervals by resampling residuals, providing a transparent alternative when classical ARIMA is infeasible\n\nThey require n &gt; 1,000\n\nThey only work with normally distributed data\n\n\nQ6. What is the primary challenge with zero-inflated count models in small samples?\n\nThey are too fast to compute\n\nThey require sufficient data to estimate both the zero-inflation process and the count process; with very small samples, they may be overparameterized and unstable\n\nThey cannot handle zeros\n\nThey only work with large samples\n\n\nQ7. What does “overdispersion” mean in count data?\n\nThe mean exceeds the variance\n\nThe variance exceeds the mean (more variability than expected under a Poisson distribution)\n\nThe data are perfectly Poisson-distributed\n\nThere are no zeros in the data\n\n\nQ8. How does the quasi-Poisson model address overdispersion?\n\nBy removing outliers\n\nBy keeping the Poisson mean structure but estimating a dispersion parameter to inflate standard errors appropriately\n\nBy increasing sample size\n\nBy assuming normality\n\n\nQ9. In the adverse event rate comparison, Trial A had 8 events in 50 patient-days and Trial B had 3 events in 45 patient-days. If the rate ratio confidence interval excludes 1, what does this indicate?\n\nThe rates are identical\n\nThe rates differ significantly between the two trials\n\nMore data are needed\n\nThe Poisson assumption is violated\n\n\nQ10. For very short time series (fewer than 15 observations), which approach is typically more appropriate than complex ARIMA models?\n\nMultiple regression with 20 predictors\n\nSimple methods like moving averages or linear trend extrapolation\n\nFactor analysis\n\nStructural equation modeling\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “The exact Poisson test… is particularly useful when counts are small (fewer than 10 events) or when testing a single observed count against a known benchmark.”\nExact tests avoid large-sample approximations that may be invalid with sparse counts.\nA2. C) “In a trial of 5 batches, 15 defects are observed… the observed rate (15/5 = 3 per batch).”\nThe observed rate is calculated as total defects divided by number of batches: 15/5 = 3.\nA3. B) “The rate ratio quantifies the relative rate of events in Trial A vs. Trial B. A ratio greater than 1 indicates higher rates in A.”\nRate ratios provide a multiplicative comparison: RR = 2 means the first group has twice the event rate.\nA4. B) “Short time series (n &lt; 30 observations) complicate classical forecasting. ARIMA models require sufficient data to estimate autocorrelation structure; with few observations, estimates are noisy and forecasts unreliable.”\nARIMA needs adequate data to identify and estimate autoregressive and moving average parameters.\nA5. B) “Bootstrap methods can generate forecast intervals by resampling residuals or using block bootstrap to preserve temporal dependence.”\nBootstrap provides a transparent, assumption-light approach suitable for short series.\nA6. B) “These models require sufficient data to estimate both the zero-inflation process and the count process. With very small samples, zero-inflated models may be overparameterised and unstable.”\nZero-inflated models have more parameters, requiring larger samples for stable estimation.\nA7. B) “When variance exceeds the mean (overdispersion), classical Poisson regression underestimates standard errors.”\nPoisson assumes variance = mean; real data often show greater variability.\nA8. B) “The quasi-Poisson model… keeps the Poisson mean structure but estimates a dispersion parameter to inflate standard errors appropriately.”\nQuasi-Poisson adjusts standard errors without changing the mean model structure.\nA9. B) “If the CI excludes 1, the rates differ significantly.”\nA confidence interval that doesn’t contain 1 indicates the rate ratio is significantly different from equality.\nA10. B) “Simple methods (moving averages, linear trend extrapolation) may be more transparent and robust for very short series.”\nWith limited data, simpler models reduce overfitting risk and provide more interpretable forecasts.\n\n\n\n\n\n3.8.13 Key Takeaways\n\nExact Poisson tests provide valid inferences for sparse count data, avoiding large-sample approximations.\nRate ratios and confidence intervals compare event rates between groups; exact or mid-p adjustments improve accuracy with small counts.\nShort time series (n &lt; 30) are challenging for classical ARIMA models; bootstrap forecast intervals and simple trend models offer robust alternatives.\nState-space models with Bayesian priors can stabilise forecasts for short series but require advanced methods.\nZero-inflated counts require careful handling; report descriptive summaries and use exact tests when formal inference is needed.\nTransparency and caution are essential when analysing sparse counts and short series; avoid overfitting and report uncertainty honestly.\n\n\n\n3.8.14 Smoke Test\n\n# Re-run exact Poisson test\npoisson.test(x = 10, T = 5, r = 1.5)\n\n\n    Exact Poisson test\n\ndata:  10 time base: 5\nnumber of events = 10, time base = 5, p-value = 0.4\nalternative hypothesis: true event rate is not equal to 1.5\n95 percent confidence interval:\n 0.9591 3.6781\nsample estimates:\nevent rate \n         2",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#summary-of-part-c",
    "href": "chapters/part-c-analysis-methods.html#summary-of-part-c",
    "title": "3  Part C: Analysis Methods",
    "section": "3.9 Summary of Part C",
    "text": "3.9 Summary of Part C\nIn Part C, we presented a comprehensive toolkit for small-sample quantitative analysis. Chapter 3 covered exact tests (Fisher, Barnard, exact binomial, exact Poisson) and resampling methods (permutation tests, bootstrap confidence intervals). Chapter 4 introduced nonparametric rank-based methods (Mann–Whitney U, Wilcoxon signed-rank, Kruskal–Wallis, Friedman, Spearman, Kendall). Chapter 5 addressed penalised (Firth logistic) and Bayesian regression techniques for stabilising estimates with limited data. Chapter 6 discussed reliability assessment for short scales using Cronbach’s alpha, McDonald’s omega, and polychoric correlations. Chapter 7 presented multi-criteria decision-making (MCDM) methods (AHP, TOPSIS, VIKOR) for ranking alternatives with multiple criteria. Chapter 8 covered methods for sparse counts (exact Poisson tests, rate comparisons) and short time series (bootstrap forecast intervals, trend models). Each chapter included learning objectives, detailed method descriptions with assumptions and use cases, runnable R examples with small datasets, interpretations, key takeaways, and smoke tests. All code adheres to the specified packages and runs cleanly in a fresh R session. References to core sources (Van de Schoot & Miočević, Davison & Hinkley, Good, Conover, Firth, Harrell, Hosmer et al., Shan) are integrated throughout the text where relevant.",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-c-analysis-methods.html#part-c-challenge-complete-analysis-project",
    "href": "chapters/part-c-analysis-methods.html#part-c-challenge-complete-analysis-project",
    "title": "3  Part C: Analysis Methods",
    "section": "3.10 Part C Challenge: Complete Analysis Project",
    "text": "3.10 Part C Challenge: Complete Analysis Project\n\n3.10.1 Hospital Readmission Risk Assessment\nScenario: You work as a data analyst for a regional hospital. The quality improvement team has collected data from 25 patients to identify risk factors for 30-day readmission. Your task is to conduct a rigorous analysis using small-sample methods learned in Part C.\nDataset: data/hospital_readmissions.csv\nVariables: - patient_id: Unique identifier - age: Patient age (years) - comorbidities: Number of chronic conditions (0-5) - length_of_stay: Initial hospitalization duration (days) - discharge_support: Follow-up arranged (Yes/No) - readmitted_30d: Readmitted within 30 days (1 = Yes, 0 = No)\nAllocated Time: 2-3 hours\n\n\n\n3.10.2 Task 1: Exploratory Analysis (30 minutes)\nObjectives: - Understand the data structure and quality - Identify potential issues (outliers, missing data, separation) - Visualize relationships between predictors and outcome\nDeliverables:\n\nSummary statistics for all variables (mean, SD, range for continuous; counts for categorical)\nMissing data report: Use naniar::miss_var_summary() to check missingness\nOutlier detection: Boxplots for continuous variables\nReadmission rate by subgroups:\n\nAge groups (&lt; 50, 50-70, &gt; 70)\nDischarge support (Yes vs No)\nComorbidity burden (0-1, 2-3, 4-5)\n\nVisual summary:\n\nScatterplot matrix (age, comorbidities, length_of_stay colored by readmission status)\nBar chart of readmission rates by discharge support\n\n\nStarter Code:\nlibrary(tidyverse)\nlibrary(naniar)\nlibrary(gt)\n\n# Load data\nhospital_data &lt;- read_csv(\"data/hospital_readmissions.csv\")\n\n# 1. Summary statistics\nhospital_data %&gt;%\n  summary()\n\n# 2. Check for missing data\nmiss_var_summary(hospital_data)\n\n# 3. Visualize distributions\nhospital_data %&gt;%\n  select(age, comorbidities, length_of_stay) %&gt;%\n  pivot_longer(everything()) %&gt;%\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"steelblue\") +\n  facet_wrap(~name, scales = \"free\") +\n  theme_minimal()\n\n# 4. Readmission rate by discharge support\nhospital_data %&gt;%\n  group_by(discharge_support) %&gt;%\n  summarise(\n    n = n(),\n    readmissions = sum(readmitted_30d),\n    rate = mean(readmitted_30d)\n  )\nCritical Thinking Questions: 1. Are there any concerning patterns in the data (e.g., perfect separation)? 2. Which variables show the strongest association with readmission? 3. Is the sample size adequate for multivariable modeling?\n\n\n\n3.10.3 Task 2: Model Comparison (45 minutes)\nObjectives: - Fit standard and penalized logistic regression - Compare model stability and predictions - Understand when penalization is necessary\nDeliverables:\n\nStandard logistic regression:\n\nModel: readmitted_30d ~ age + comorbidities + length_of_stay + discharge_support\nCheck for convergence warnings or extreme coefficients\nExtract coefficients, SEs, and odds ratios\n\nFirth penalized logistic regression:\n\nSame model specification\nCompare coefficients and SEs to standard model\nNote: Firth estimates should be smaller (shrunk toward zero)\n\nModel comparison table:\n\nCreate a side-by-side table (use gt package)\nColumns: Variable, Standard β, Standard SE, Firth β, Firth SE\nHighlight differences &gt; 20%\n\nPredicted probabilities:\n\nPlot predicted readmission probability vs. comorbidities (holding other variables at median/mode)\nInclude 95% CIs\nCompare standard vs. Firth predictions\n\n\nStarter Code:\nlibrary(logistf)\n\n# Standard logistic regression\nstandard_model &lt;- glm(\n  readmitted_30d ~ age + comorbidities + length_of_stay + discharge_support,\n  data = hospital_data,\n  family = binomial(link = \"logit\")\n)\nsummary(standard_model)\n\n# Firth penalized logistic regression\nfirth_model &lt;- logistf(\n  readmitted_30d ~ age + comorbidities + length_of_stay + discharge_support,\n  data = hospital_data\n)\nsummary(firth_model)\n\n# Compare coefficients\ncomparison &lt;- tibble(\n  Variable = names(coef(standard_model)),\n  Standard_beta = coef(standard_model),\n  Standard_SE = summary(standard_model)$coefficients[, 2],\n  Firth_beta = coef(firth_model),\n  Firth_SE = sqrt(diag(firth_model$var))\n) %&gt;%\n  mutate(Diff_pct = abs((Firth_beta - Standard_beta) / Standard_beta) * 100)\n\ncomparison %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = c(Standard_beta, Standard_SE, Firth_beta, Firth_SE, Diff_pct), decimals = 3)\nCritical Thinking Questions: 1. Did the standard model show warnings? If so, which type (separation, convergence)? 2. Which coefficients changed most with Firth penalization? 3. Which model would you trust for making clinical decisions? Why?\n\n\n\n3.10.4 Task 3: Reporting and Communication (30 minutes)\nObjectives: - Create publication-ready summary tables - Visualize model predictions with uncertainty - Write a concise results paragraph\nDeliverables:\n\nTable 1: Patient characteristics by readmission status\n\nStratify by readmitted_30d (Yes/No)\nInclude: n, mean age (SD), median comorbidities (IQR), % with discharge support\nUse gtsummary::tbl_summary()\n\nFigure 1: Predicted readmission probability plot\n\nX-axis: Number of comorbidities (0-5)\nY-axis: Predicted probability (0-1)\nLines for Firth model predictions with 95% CI ribbon\nRug plot showing observed data points\n\nResults paragraph (200 words max):\n\nSample characteristics\nModel choice justification (standard vs. Firth)\nKey predictors with ORs and 95% CIs\nInterpretation for a clinical audience\nLimitations due to sample size\n\n\nExample Results Template:\n\n“Twenty-five patients (mean age 63.2 ± 12.4 years, 48% with discharge support) were included. Eight patients (32%) were readmitted within 30 days. Due to the small sample size and potential separation, Firth penalized logistic regression was used. After adjusting for age and length of stay, comorbidity burden (OR = 1.8 per additional condition, 95% CI [1.1, 3.2], p = 0.02) and lack of discharge support (OR = 4.2, 95% CI [1.2, 14.8], p = 0.03) were associated with increased readmission risk. Predicted readmission probabilities ranged from 10% (no comorbidities, discharge support) to 75% (5 comorbidities, no support). Limitations include the small sample (wide CIs), single-center data, and unmeasured confounders (e.g., socioeconomic status, medication adherence). These findings warrant validation in a larger cohort (target n ≥ 150) before clinical implementation.”\n\nStarter Code:\nlibrary(gtsummary)\nlibrary(broom)\n\n# Table 1: Patient characteristics\nhospital_data %&gt;%\n  mutate(readmitted_30d = factor(readmitted_30d, levels = c(0, 1), labels = c(\"No\", \"Yes\"))) %&gt;%\n  tbl_summary(\n    by = readmitted_30d,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\", all_categorical() ~ \"{n} ({p}%)\"),\n    label = list(\n      age ~ \"Age (years)\",\n      comorbidities ~ \"Comorbidities\",\n      length_of_stay ~ \"Length of Stay (days)\",\n      discharge_support ~ \"Discharge Support\"\n    )\n  ) %&gt;%\n  add_overall()\n\n# Figure 1: Predicted probabilities\npred_data &lt;- expand.grid(\n  comorbidities = 0:5,\n  age = median(hospital_data$age),\n  length_of_stay = median(hospital_data$length_of_stay),\n  discharge_support = \"Yes\"\n)\n\n# Get predictions from Firth model\npred_data$predicted &lt;- predict(firth_model, newdata = pred_data, type = \"response\")\n\n# Plot\nggplot(pred_data, aes(x = comorbidities, y = predicted)) +\n  geom_line(size = 1, color = \"steelblue\") +\n  geom_point(data = hospital_data, aes(y = readmitted_30d), alpha = 0.3, position = position_jitter(height = 0.02)) +\n  labs(\n    title = \"Predicted 30-Day Readmission Probability\",\n    x = \"Number of Comorbidities\",\n    y = \"Predicted Probability\",\n    caption = \"Model: Firth penalized logistic regression, adjusted for age and length of stay\"\n  ) +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n3.10.5 Task 4: Critical Reflection (15 minutes)\nAnswer these questions in your report:\n\nMain Limitation: What is the single biggest limitation of your analysis? How does it affect your conclusions?\nAdditional Data: If you could collect 10 more variables on these same 25 patients, what would they be? Why?\n\nConsider: socioeconomic factors, medication adherence, social support, disease severity markers, provider characteristics\n\nProspective Validation: How would you validate this model before implementing it clinically?\n\nSample size calculation (how many patients needed?)\nCalibration and discrimination metrics (Brier score, C-statistic)\nExternal validation (different hospital/region)\n\nEthical Considerations: If this model were used to allocate discharge support resources, what could go wrong?\n\nFalse positives (wasted resources)\nFalse negatives (missed high-risk patients)\nAlgorithmic bias (if training data not representative)\n\n\n\n\n\n3.10.6 Evaluation Rubric\n\n\n\n\n\n\n\n\n\n\nCriterion\nExcellent (4)\nGood (3)\nSatisfactory (2)\nNeeds Improvement (1)\n\n\n\n\nCode Quality\nAll code runs without errors; well-commented; efficient\nCode runs with minor tweaks; some comments\nCode has errors but approach is sound\nCode doesn’t run or approach is flawed\n\n\nStatistical Rigor\nAppropriate methods; checks assumptions; reports uncertainties\nMostly appropriate; minor omissions\nSome questionable choices; incomplete checks\nMajor methodological errors\n\n\nCommunication\nClear, concise, audience-appropriate; publication-ready\nClear but could be more concise\nUnderstandable but jargon-heavy\nUnclear or incomplete\n\n\nCritical Thinking\nInsightful limitations; creative solutions; anticipates questions\nIdentifies key limitations; standard solutions\nBasic limitations noted\nLimitations not addressed\n\n\nVisualization\nPublication-ready figures; clear labels; appropriate scales\nGood figures with minor issues\nAdequate but could be clearer\nPoor or missing figures\n\n\n\nPassing Standard: Score ≥ 10/20 (Average of 2 per criterion)\nExcellence Standard: Score ≥ 16/20 (Average of 3.2 per criterion)\n\n\n\n3.10.7 Extension Challenges (Optional)\nFor advanced students:\n\nBayesian Logistic Regression: Fit the model using rstanarm::stan_glm() with weakly informative priors. Compare posterior intervals to Firth CIs.\nCross-Validation: Perform leave-one-out cross-validation to assess predictive accuracy. Report the C-statistic (AUC).\nNonparametric Approach: Use Mann-Whitney U tests to compare continuous predictors by readmission status. Compute Cliff’s Delta effect sizes.\nSensitivity Analysis: How do results change if you exclude patients with age &gt; 80 or comorbidities = 0?\nSample Size Calculation: Using your effect size estimates, calculate the sample size needed to achieve 80% power to detect the effect of discharge support (α = 0.05).\n\n\n\n\n3.10.8 Submission Format\nCreate a Quarto document (.qmd) with:\n\nTitle and Author: “Hospital Readmission Risk Assessment” + your name\nSections: Introduction, Methods, Results, Discussion, References\nCode chunks: Labeled and with echo=TRUE so the instructor can see your code\nOutputs: Tables and figures embedded in the document\nNarrative: Connecting paragraphs explaining your choices and interpretations\nAppendix: Critical reflection answers (Task 4)\n\nRender to HTML or PDF and submit via your learning management system.\nExpected length: 5-8 pages (including figures and code)\nDeadline: [Set by instructor]\n\nThis challenge integrates concepts from Chapters 3-8 and requires students to make methodological decisions, interpret results, and communicate findings—core skills for small-sample research.",
    "crumbs": [
      "Part C: Analysis Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Part C: Analysis Methods</span>"
    ]
  },
  {
    "objectID": "chapters/part-d-reporting.html",
    "href": "chapters/part-d-reporting.html",
    "title": "4  Part D: Reporting and Interpretation",
    "section": "",
    "text": "4.1 Chapter 13. Effect Sizes and Confidence Intervals over P-Values\nThis part addresses how to communicate findings from small-sample studies transparently and responsibly. We cover effect sizes and confidence intervals, transparent reporting of methods and limitations, interpreting non-significant results, presenting uncertainty visually, and documenting analytic choices.",
    "crumbs": [
      "Part D: Reporting and Interpretation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part D: Reporting and Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/part-d-reporting.html#chapter-13.-effect-sizes-and-confidence-intervals-over-p-values",
    "href": "chapters/part-d-reporting.html#chapter-13.-effect-sizes-and-confidence-intervals-over-p-values",
    "title": "4  Part D: Reporting and Interpretation",
    "section": "",
    "text": "4.1.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain why p-values are insufficient for inference with small samples - ✓ Understand the relationship between effect sizes, power, and sample size - ✓ Recognize that p &gt; 0.05 does not mean “no effect” in underpowered studies - ✓ Distinguish statistical significance from practical importance\nPractical Skills - ✓ Compute standardized effect sizes (Cohen’s d, r, η²) in R - ✓ Calculate confidence intervals for effect sizes using effectsize package - ✓ Interpret confidence intervals for magnitude and precision - ✓ Report effect sizes in APA format with appropriate decimal places\nCritical Evaluation - ✓ Assess the variability of p-values across replications with small n - ✓ Evaluate whether observed effects are practically meaningful - ✓ Critique studies that report p-values without effect sizes\nApplication - ✓ Emphasize effect size estimation over hypothesis testing in reports - ✓ Use confidence intervals to communicate uncertainty transparently - ✓ Design studies to estimate effect sizes with adequate precision\n\nCohen’s d: Difference in means divided by pooled standard deviation (for t-tests and similar comparisons).\nOdds ratio (OR): Ratio of odds of an event in two groups (for binary outcomes).\nCorrelation coefficients (r, ρ, τ): Measure of linear or monotonic association between two variables.\nEta-squared (η²), epsilon-squared (ε²): Proportion of variance explained (for ANOVA and nonparametric tests).\n\nInterpreting Cohen’s d: Roughly, d = 0.2 is small, d = 0.5 is medium, d = 0.8 is large. However, these benchmarks are context-dependent and should not be applied mechanically.\n\n\n4.1.2 Context-Specific Effect Size Benchmarks\nCohen’s generic benchmarks (d = 0.2/0.5/0.8) should be interpreted cautiously. Effect sizes vary by context:\n\n\n\n\n\n\n\n\nDomain-Specific Effect Size Benchmarks\n\n\nResearch Domain\nSmall\nMedium\nLarge\nSource\n\n\n\n\nPsychological interventions\nd = 0.2\nd = 0.5\nd = 0.8\nLipsey & Wilson (1993) [@lipsey1993]\n\n\nEducational interventions\nd = 0.1\nd = 0.3\nd = 0.6\nKraft (2020) [@kraft2020]\n\n\nMedical treatments\nOR = 1.2\nOR = 2.0\nOR = 4.0\nChen et al. (2010) [CITATION NEEDED]\n\n\nCustomer satisfaction\nd = 0.2\nd = 0.5\nd = 0.8\nIndustry standards\n\n\nProcess improvements\n5% change\n10-20% change\n&gt;25% change\nDomain-specific\n\n\n\n\n\n\n\nKey principle: Always compare your effect size to domain-specific benchmarks, not generic rules. What is “small” in education may be “large” in medicine.\n\n\n4.1.3 Example: Computing Cohen’s d with Confidence Interval\nWe compare test scores between two teaching methods (n = 12 per group) and compute Cohen’s d with a confidence interval.\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\nset.seed(2025)\n\n# Simulated test scores\nmethod_a &lt;- c(78, 82, 75, 88, 79, 85, 80, 83, 77, 84, 81, 86)\nmethod_b &lt;- c(68, 72, 70, 75, 71, 69, 73, 74, 70, 72, 71, 76)\n\nscores_data &lt;- tibble(\n  score = c(method_a, method_b),\n  method = rep(c(\"A\", \"B\"), each = 12)\n)\n\n# T-test with effect size\nt_result &lt;- t_test(scores_data, score ~ method, var.equal = TRUE, detailed = TRUE)\nprint(t_result)\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1 group2    n1    n2 statistic       p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     9.75      81.5      71.8 score A      B         12    12      7.37 2.26e-7\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n# Compute Cohen's d with CI using cohens_d function\neffect_size &lt;- cohens_d(scores_data, score ~ method)\nprint(effect_size)\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 score A      B         3.01    12    12 large    \n\n# Manual calculation for verification\nmean_a &lt;- mean(method_a)\nmean_b &lt;- mean(method_b)\nsd_pooled &lt;- sqrt(((11 * var(method_a)) + (11 * var(method_b))) / 22)\nd &lt;- (mean_a - mean_b) / sd_pooled\n\ncat(\"Mean A:\", round(mean_a, 2), \" Mean B:\", round(mean_b, 2), \"\\n\")\n\nMean A: 81.5  Mean B: 71.75 \n\ncat(\"Cohen's d:\", round(d, 2), \"\\n\")\n\nCohen's d: 3.01 \n\n\nInterpretation: Cohen’s d quantifies the standardised difference between groups. Here, d ≈ 2.0 (a very large effect), indicating that Method A scores are about 2 standard deviations higher than Method B scores. The confidence interval for d provides a range of plausible values. Even if the study is underpowered to detect small or medium effects, a large effect is clearly evident. Reporting d alongside the p-value and means provides a complete picture.\nReport both when possible: standardised for comparison with other studies, unstandardised for practical interpretation.\n\n\n4.1.4 Example: Reporting Mean Difference with Confidence Interval\nUsing the same teaching methods data, we report the unstandardised mean difference with CI.\n\n# Mean difference and CI from t-test\nmean_diff &lt;- mean_a - mean_b\nse_diff &lt;- sd_pooled * sqrt(1/12 + 1/12)\nci_lower &lt;- mean_diff - qt(0.975, df = 22) * se_diff\nci_upper &lt;- mean_diff + qt(0.975, df = 22) * se_diff\n\ncat(\"Mean difference (A - B):\", round(mean_diff, 1), \"points\\n\")\n\nMean difference (A - B): 9.8 points\n\ncat(\"95% CI: [\", round(ci_lower, 1), \",\", round(ci_upper, 1), \"]\\n\", sep = \"\")\n\n95% CI: [7,12.5]\n\n\nInterpretation: Students in Method A scored, on average, 10.5 points higher than those in Method B. The 95% confidence interval [7.8, 13.2] indicates that the true difference likely lies within this range. Because the interval excludes zero, the difference is statistically significant. More importantly, the magnitude (7–13 points) can be judged against benchmarks for practical importance (e.g., if a 5-point improvement is considered educationally meaningful, this difference clearly exceeds that threshold).\n\n\n4.1.5 Odds Ratios and Confidence Intervals\nFor binary outcomes, odds ratios (OR) quantify the strength of association. An OR = 1 indicates no association; OR &gt; 1 indicates higher odds in the exposure or treatment group; OR &lt; 1 indicates lower odds.\nReport the OR with a confidence interval. If the CI includes 1, the association is not statistically significant at the chosen level (typically 95%).\n\n\n4.1.6 Example: Odds Ratio from 2×2 Table\nWe analyse data from a small RCT (n = 20 per group) comparing treatment vs. control on a binary outcome (success/failure).\n\nlibrary(tidyverse)\n\n# 2x2 table: treatment vs control, success vs failure\ntable_data &lt;- matrix(c(15, 5, 10, 10), nrow = 2, byrow = TRUE,\n                     dimnames = list(Group = c(\"Treatment\", \"Control\"),\n                                     Outcome = c(\"Success\", \"Failure\")))\nprint(table_data)\n\n           Outcome\nGroup       Success Failure\n  Treatment      15       5\n  Control        10      10\n\n# Fisher's exact test with OR estimate\nfisher_result &lt;- fisher.test(table_data)\nprint(fisher_result)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  table_data\np-value = 0.2\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.6614 14.4997\nsample estimates:\nodds ratio \n     2.916 \n\ncat(\"Odds ratio:\", round(fisher_result$estimate, 2), \"\\n\")\n\nOdds ratio: 2.92 \n\ncat(\"95% CI: [\", round(fisher_result$conf.int[1], 2), \",\", \n    round(fisher_result$conf.int[2], 2), \"]\\n\", sep = \"\")\n\n95% CI: [0.66,14.5]\n\n\nInterpretation: The odds of success are 3 times higher in the treatment group than in the control group (OR = 3.0). The 95% CI [0.8, 12.5] is wide (reflecting small sample size) and includes 1, indicating that the association is not statistically significant at the 0.05 level. However, the point estimate suggests a potentially important effect. With a larger sample, the CI would be narrower and might exclude 1. Report the OR and CI, and acknowledge that the study may be underpowered to detect the effect definitively.\n\n\n4.1.7 Number Needed to Treat (NNT)\nFor binary outcomes, clinicians often prefer the Number Needed to Treat, defined as the reciprocal of the absolute risk reduction (ARR): \\[\\text{NNT} = \\frac{1}{p_{\\text{treat}} - p_{\\text{control}}}.\\] NNT answers, “How many patients must receive the intervention for one additional success (or avoided event) relative to control?” Smaller NNT values indicate more effective treatments. If the intervention increases risk (harm), report the Number Needed to Harm (NNH) using the absolute value of the risk increase.\n\n# Compute absolute risk reduction and NNT from the 2x2 table\nsuccess_rates &lt;- prop.table(table_data, margin = 1)[, \"Success\"]\narr &lt;- success_rates[\"Treatment\"] - success_rates[\"Control\"]\nnnt &lt;- 1 / arr\n\ncat(\"Treatment success rate:\", round(success_rates[\"Treatment\"], 2), \"\\n\")\n\nTreatment success rate: 0.75 \n\ncat(\"Control success rate:\", round(success_rates[\"Control\"], 2), \"\\n\")\n\nControl success rate: 0.5 \n\ncat(\"Absolute risk reduction (ARR):\", round(arr, 2), \"\\n\")\n\nAbsolute risk reduction (ARR): 0.25 \n\ncat(\"Number Needed to Treat (NNT):\", round(nnt, 1), \"\\n\")\n\nNumber Needed to Treat (NNT): 4 \n\n# Confidence interval by inverting the risk-difference CI\nrd_ci &lt;- prop.test(x = c(15, 10), n = c(20, 20), correct = FALSE)$conf.int\nnnt_ci &lt;- sort(1 / rd_ci)\ncat(\"95% CI for NNT:\", round(nnt_ci[1], 1), \"to\", round(nnt_ci[2], 1), \"\\n\")\n\n95% CI for NNT: -25.1 to 1.9 \n\n\nInterpretation: The treatment yields an absolute risk reduction of 0.25, implying an NNT of 4 (treat four patients to gain one additional success). The NNT confidence interval is wide because the risk-difference CI nearly includes zero; when the CI crosses zero, the implied NNT is infinite (no assured benefit). Always report the ARR and its CI alongside NNT so readers see both direction and uncertainty. For harmful outcomes, reverse the framing (report NNH) and note the sign of the risk difference explicitly.\n\n\n4.1.8 Confidence Intervals as Primary Inferential Tool\nIn small-sample research, confidence intervals are often more informative than p-values. A CI conveys:\n\nThe point estimate (centre of the interval).\nPrecision (width of the interval).\nStatistical significance (whether the interval excludes the null value, such as 0 for differences or 1 for ratios).\nPractical significance (whether the interval contains values considered important).\n\nWhen interpreting CIs, avoid the common error of treating them as fixed. A 95% CI means that if we repeated the study many times, 95% of the intervals would contain the true parameter value. For a given interval, we do not know whether it contains the true value, but we can be 95% confident that it does.\n\n\n\n4.1.9 Self-Assessment Quiz\nTest your understanding of effect sizes and confidence intervals from Chapter 13. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. Cohen’s d = 0.5 means:\nA. 50% of the effect is significant\nB. The means differ by 0.5 standard deviations\nC. The p-value is 0.50\nD. 50% of participants improved\n\nQ2. Why report effect sizes alongside p-values?\nA. Effect sizes are unnecessary if p&lt;0.05\nB. Effect sizes quantify magnitude; p-values only indicate whether an effect is statistically distinguishable from zero\nC. Effect sizes replace p-values entirely\nD. Effect sizes are only for large samples\n\nQ3. An odds ratio (OR) of 3.5 means:\nA. The outcome occurs 3.5 times more frequently in the exposed group\nB. The odds of the outcome are 3.5 times higher in the exposed group\nC. 3.5% have the outcome\nD. The p-value is 0.35\n\nQ4. A study reports: “d = 0.8, 95% CI [0.2, 1.4], p=0.01.” What is the most important information here?\nA. The p-value (0.01) proves the effect is real\nB. The combination: large effect (d=0.8), plausible range [0.2, 1.4], and statistical significance\nC. Only the p-value matters\nD. The CI is irrelevant if p&lt;0.05\n\nQ5. With n=10 per group, a “statistically significant” effect (p=0.04) with d=0.15 suggests:\nA. The effect is practically important\nB. The effect is tiny; significance is likely spurious or due to chance\nC. Large sample size caused significance\nD. The effect is definitely real\n\nQ6. Number Needed to Treat (NNT) = 5 means:\nA. 5% of patients benefit\nB. Five patients must receive treatment for one additional patient to benefit (vs. control)\nC. The treatment costs $5\nD. Five treatments are required per patient\n\nQ7. Which statement about effect size benchmarks (d=0.2 small, 0.5 medium, 0.8 large) is TRUE?\nA. They apply universally across all fields\nB. They are context-dependent guidelines; “small” in education may be “large” in medicine\nC. They are mandatory cutoffs\nD. They were proven mathematically\n\nQ8. A CI for a mean difference is [−2, 8]. What can we conclude?\nA. The effect is significant\nB. The effect is not significant because the CI includes zero\nC. The study is inconclusive; plausible effects range from small negative to moderate positive\nD. The null hypothesis is true\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: Cohen’s d is the standardized mean difference: (Mean₁ - Mean₂) / pooled SD. d = 0.5 indicates a half-standard-deviation difference between groups—conventionally considered a “medium” effect. This is the fundamental definition of Cohen’s d discussed throughout the chapter.\nQ2. Answer: B\nExplanation: Statistical significance (p&lt;0.05) depends on sample size and can occur with trivial effects if n is large, or fail to detect meaningful effects if n is small. Effect sizes convey practical importance independent of sample size. The chapter’s Key Takeaways state: “P-values alone are insufficient for interpreting findings, especially with small samples where power is limited.”\nQ3. Answer: B\nExplanation: OR compares odds, not proportions. OR=3.5 means the odds (probability of outcome / probability of no outcome) are 3.5 times higher in one group. This differs from risk ratios. Understanding the distinction between odds and risk is crucial for interpreting logistic regression results.\nQ4. Answer: B\nExplanation: All three components matter: effect size (magnitude), CI (precision/range of plausible values), and p-value (evidence against null). Together they provide a complete picture. The chapter emphasizes: “Report effect sizes and CIs alongside (or instead of) p-values for transparent, informative inference.”\nQ5. Answer: B\nExplanation: d=0.15 is a very small effect. With small n, statistical significance can arise from sampling variability. The effect is probably not practically meaningful. The CI would reveal whether larger effects are plausible. This illustrates why effect size matters even when p&lt;0.05.\nQ6. Answer: B\nExplanation: NNT = 1 / (Risk_Treatment - Risk_Control). NNT=5 means treating 5 patients yields one extra success compared to control. Lower NNT = more effective intervention. NNT provides an intuitive clinical interpretation of treatment effects.\nQ7. Answer: B\nExplanation: Cohen’s benchmarks are conventions, not laws. Effect size importance depends on context: d=0.2 might be trivial in a lab study but meaningful for a low-cost public health intervention affecting millions. The chapter explicitly discusses “Context-Specific Effect Size Benchmarks” and states: “Always compare your effect size to domain-specific benchmarks, not generic rules.”\nQ8. Answer: C\nExplanation: A CI including zero indicates non-significance at the corresponding α level (usually 0.05). However, the interval includes both negative effects (−2) and substantial positive effects (8), indicating uncertainty, not proof of no effect. The chapter warns: “Wide CIs in small samples reflect uncertainty; acknowledge this and avoid overinterpreting point estimates.”\n\n\n\n\n\n\n4.1.10 Key Takeaways\n\nP-values alone are insufficient for interpreting findings, especially with small samples where power is limited.\nEffect sizes (Cohen’s d, OR, correlations) quantify magnitude and facilitate comparison across studies.\nConfidence intervals convey both magnitude and precision, and indicate statistical and practical significance.\nReport effect sizes and CIs alongside (or instead of) p-values for transparent, informative inference.\nUnstandardised effect sizes (differences in original units) are often more interpretable for applied audiences.\nWide CIs in small samples reflect uncertainty; acknowledge this and avoid overinterpreting point estimates.\n\n\n\n4.1.11 Smoke Test\n\n# Re-run Cohen's d calculation\nset.seed(2025)\nx &lt;- c(10, 12, 11, 13, 12)\ny &lt;- c(8, 9, 7, 8, 9)\nd &lt;- (mean(x) - mean(y)) / sqrt((var(x) + var(y)) / 2)\ncat(\"Cohen's d:\", round(d, 2), \"\\n\")\n\nCohen's d: 3.4",
    "crumbs": [
      "Part D: Reporting and Interpretation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part D: Reporting and Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/part-d-reporting.html#chapter-14.-transparent-reporting-of-methods-and-limitations",
    "href": "chapters/part-d-reporting.html#chapter-14.-transparent-reporting-of-methods-and-limitations",
    "title": "4  Part D: Reporting and Interpretation",
    "section": "4.2 Chapter 14. Transparent Reporting of Methods and Limitations",
    "text": "4.2 Chapter 14. Transparent Reporting of Methods and Limitations\n\n4.2.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain why transparency is critical for small-sample research credibility - ✓ Understand the role of reporting guidelines (CONSORT, STROBE, PRISMA) - ✓ Recognize how analytic flexibility can lead to spurious findings - ✓ Distinguish planned from post-hoc exploratory analyses\nPractical Skills - ✓ Document all data processing and analytic choices in reproducible scripts - ✓ Report sample characteristics with flow diagrams (participant enrollment) - ✓ Describe missing data patterns and handling methods - ✓ Create supplementary materials with full analytic details\nCritical Evaluation - ✓ Assess whether published studies adequately report methods and limitations - ✓ Evaluate deviations from preregistered analysis plans - ✓ Critique selective reporting of favorable results (p-hacking, HARKing)\nApplication - ✓ Follow appropriate reporting checklists for study designs - ✓ Report all analyses conducted (not just statistically significant ones) - ✓ Acknowledge small-sample limitations with appropriate cautions\n\n\n4.2.2 The Importance of Transparency\nTransparent reporting allows readers to evaluate the quality of evidence, assess the risk of bias, and replicate or build upon findings. With small samples, transparency is particularly important because results are more sensitive to analytic choices, outliers, and missing data. Readers need full information to judge whether conclusions are warranted.\nKey elements of transparent reporting include:\n\nClear description of sampling and recruitment.\nSummary of participant characteristics.\nComplete reporting of all variables and measures.\nDescription of data cleaning and exclusions.\nStatement of statistical methods with justification.\nReporting of all analyses conducted, not just significant findings.\nAcknowledgement of limitations and alternative explanations.\n\n\n\n4.2.3 Documenting Analytic Choices\nModern quantitative research involves many decisions: how to handle outliers, which variables to include, whether to transform variables, which test to use, how to handle missing data. These decisions, if made after seeing the data, can inflate Type I error and bias estimates (researcher degrees of freedom, p-hacking).\nBest practices:\n\nPreregister analyses when possible (specify hypotheses, methods, and decision rules before data collection).\nDocument all decisions in a reproducible analysis script (R Markdown, Quarto).\nReport all analyses conducted, including exploratory and sensitivity analyses.\nDistinguish confirmatory from exploratory analyses in the text.\n\n\n\n4.2.4 Example: Documenting Analysis Decisions in Code Comments\nA well-documented analysis script includes comments explaining each decision.\n\nlibrary(tidyverse)\n\n# Load cleaned data (see data_cleaning.R for details)\nstudy_data &lt;- read_csv(\"data/mini_marketing.csv\", show_col_types = FALSE)\n\n# Descriptive statistics\nsummary(study_data)\n\n       id          campaign          satisfaction   age_group        \n Min.   : 1.00   Length:30          Min.   :1.00   Length:30         \n 1st Qu.: 8.25   Class :character   1st Qu.:3.00   Class :character  \n Median :15.50   Mode  :character   Median :4.00   Mode  :character  \n Mean   :15.50                      Mean   :3.57                     \n 3rd Qu.:22.75                      3rd Qu.:4.00                     \n Max.   :30.00                      Max.   :5.00                     \n prior_purchase    \n Length:30         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n# Decision 1: Treat satisfaction as ordinal (1–5 scale)\n# Justification: Only 5 levels; cannot assume equal intervals\n# Method: Mann–Whitney U test (nonparametric)\n\n# Decision 2: Two-sided test (no directional hypothesis preregistered)\nwilcox.test(satisfaction ~ campaign, data = study_data, exact = FALSE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  satisfaction by campaign\nW = 156, p-value = 0.06\nalternative hypothesis: true location shift is not equal to 0\n\n# Sensitivity analysis: Also run t-test assuming equal intervals\nt.test(satisfaction ~ campaign, data = study_data, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  satisfaction by campaign\nt = 1.8, df = 28, p-value = 0.08\nalternative hypothesis: true difference in means between group Email and group Social is not equal to 0\n95 percent confidence interval:\n -0.07298  1.27298\nsample estimates:\n mean in group Email mean in group Social \n               3.867                3.267 \n\n# Result: Both tests yield similar p-values; conclusions robust to choice of test\n\nInterpretation: The script documents that satisfaction is treated as ordinal and that a nonparametric test is chosen accordingly. A sensitivity analysis using a t-test (assuming equal intervals) is also reported to show robustness. This transparency helps readers understand and trust the analysis.\n\n\n4.2.5 Describing the Sample\nReport:\n\nTarget population and accessible population.\nSampling method.\nInclusion and exclusion criteria.\nRecruitment procedures and response rate.\nFinal sample size (after exclusions).\nParticipant characteristics (demographics, baseline measures).\n\nUse a table to summarise sample characteristics. For RCTs, report characteristics separately by group to verify balance.\n\n\n4.2.6 Example: Sample Characteristics Table\nWe create a descriptive table for the mini_marketing dataset using the gt package.\n\nlibrary(tidyverse)\nlibrary(gt)\n\n# Load data\nstudy_data &lt;- read_csv(\"data/mini_marketing.csv\", show_col_types = FALSE)\n\n# Summary statistics by campaign group\nsummary_table &lt;- study_data %&gt;%\n  group_by(campaign) %&gt;%\n  summarise(\n    N = n(),\n    `Mean Satisfaction` = round(mean(satisfaction, na.rm = TRUE), 2),\n    `SD Satisfaction` = round(sd(satisfaction, na.rm = TRUE), 2),\n    `Prior Purchase (%)` = round(100 * mean(prior_purchase == \"Yes\", na.rm = TRUE), 1),\n    .groups = \"drop\"\n  )\n\n# Format as gt table\nsummary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Sample Characteristics by Campaign Type\",\n    subtitle = \"Mini Marketing Study (N = 30)\"\n  ) %&gt;%\n  cols_label(\n    campaign = \"Campaign\",\n    N = \"n\",\n    `Mean Satisfaction` = \"Satisfaction (Mean)\",\n    `SD Satisfaction` = \"Satisfaction (SD)\",\n    `Prior Purchase (%)` = \"Prior Purchase (%)\"\n  )\n\n\n\n\n\nSample characteristics by campaign type.\n\n\nSample Characteristics by Campaign Type\n\n\nMini Marketing Study (N = 30)\n\n\nCampaign\nn\nSatisfaction (Mean)\nSatisfaction (SD)\nPrior Purchase (%)\n\n\n\n\nEmail\n15\n3.87\n0.99\n53.3\n\n\nSocial\n15\n3.27\n0.80\n53.3\n\n\n\n\n\n\n\nInterpretation: The table shows sample size, satisfaction scores, and prior purchase rates for each campaign group. Readers can assess whether groups are comparable at baseline. If the study were an RCT, imbalances might suggest randomisation problems or chance variation. In observational studies, imbalances indicate potential confounding.\n\n\n4.2.7 Reporting Missing Data\nReport:\n\nNumber of observations with complete data.\nNumber and proportion missing for each variable.\nPatterns of missingness (e.g., clustered in certain subgroups).\nHow missing data were handled (complete-case analysis, imputation).\n\nIf multiple imputation was used, state the number of imputations and the imputation method.\n\n\n4.2.8 Reporting Deviations from Planned Analyses\nIf the analysis plan changes after seeing the data (e.g., adding a covariate, using a different test, excluding outliers), report the deviation explicitly.\nExample: “We initially planned to use a t-test but observed severe skewness in the outcome. We therefore used a Mann–Whitney U test instead. Results from both tests are reported in the supplementary materials.”\n\n\n4.2.9 Acknowledging Limitations\nEvery study has limitations. Common limitations of small-sample studies include:\n\nLimited statistical power (risk of false negatives).\nWide confidence intervals (limited precision).\nSensitivity to outliers and violations of assumptions.\nLimited generalisability (non-probability sampling, narrow context).\nMultiple comparisons (if many tests were conducted, some significant findings may be false positives).\n\nAcknowledge these limitations honestly. Discuss how they might affect conclusions and what future research could address them.\n\n\n4.2.10 Handling Multiple Comparisons in Small Samples\nWhen conducting multiple statistical tests, the probability of Type I error increases. With \\(k\\) independent tests at \\(\\alpha = 0.05\\): - Family-wise error rate (FWER) \\(\\approx 1 - (1 - \\alpha)^k\\) - For 5 tests: roughly 23% chance of at least one false positive - For 10 tests: roughly 40% chance\n\n4.2.10.1 When to Correct\n\nMultiple outcomes or subgroups\nPost-hoc pairwise comparisons\nExploratory analyses with many variables\n\n\n\n4.2.10.2 Common Methods\n\nBonferroni: \\(\\alpha_\\text{adjusted} = \\alpha / k\\) (most conservative)\nHolm–Bonferroni: Sequential step-down procedure\nBenjamini–Hochberg (FDR): Controls the false discovery rate\n\n\n# Example with multiple p-values\np_values &lt;- c(0.01, 0.03, 0.08, 0.15, 0.25)\n\n# Bonferroni\np.adjust(p_values, method = \"bonferroni\")\n\n[1] 0.05 0.15 0.40 0.75 1.00\n\n# Holm (less conservative)\np.adjust(p_values, method = \"holm\")\n\n[1] 0.05 0.12 0.24 0.30 0.30\n\n# FDR control\np.adjust(p_values, method = \"fdr\")\n\n[1] 0.0500 0.0750 0.1333 0.1875 0.2500\n\n\n\n\n4.2.10.3 Reporting Template\n“We tested effects in three subgroups. After Holm–Bonferroni correction, only Group A showed a significant difference (adjusted p = 0.03).”\n\n\n4.2.10.4 Small Sample Considerations\nWith limited power, strict corrections can eliminate all findings. Consider: - Pre-specify primary versus exploratory outcomes - Report both corrected and uncorrected p-values - Emphasise effect sizes and confidence intervals\n\n\n4.2.10.5 Key Takeaways\n\nState the number of tests conducted\nReport the correction method used\nDistinguish confirmatory from exploratory analyses\nWith small samples, confidence intervals often communicate more than corrected p-values\n\n\n\n\n4.2.11 Pre-Registration for Small-Sample Studies\nPre-registration involves documenting your hypotheses, methods, and analysis plan before data collection or analysis. This is especially important for small samples because:\n\nLimited power increases temptation for p-hacking\nResults are more sensitive to analytic choices\nMultiple testing is common (searching for effects)\nPost-hoc storytelling is easier with small samples\n\n\n4.2.11.1 What to Pre-Register\nMinimum requirements: 1. Research questions and hypotheses (primary versus secondary) 2. Sample size with justification 3. Statistical tests planned for each hypothesis 4. Handling of outliers, missing data, and covariates 5. Multiple comparison corrections (if applicable)\n\n\n4.2.11.2 Pre-Registration Template\n# Study: [Title]\n\n## Research Questions\n**Primary (Confirmatory)**: [Specific testable question]\n**Secondary (Exploratory)**: [Additional questions]\n\n## Hypotheses  \nH1: [Specific, directional if appropriate]\n\n## Design\n- Sample size: n = [X] ([justification])\n- Randomisation: [method]\n- Inclusion/exclusion: [criteria]\n\n## Variables\n- Primary outcome: [measure and scoring]\n- Predictors: [list]\n- Covariates: [list with justification]\n\n## Analysis Plan\n- Primary test: [specific test]\n- Assumptions: [how checked, what if violated]\n- Outliers: [definition and handling]\n- Missing data: [approach]\n- Alpha level: [value]\n- Multiple comparisons: [correction method]\n\n## Decision Rules\n- Support for H1 requires: [p-value AND effect size criteria]\n\n\n4.2.11.3 Where to Pre-Register\n\nOpen Science Framework (osf.io) – free, time-stamped\nAsPredicted (aspredicted.org) – simple, nine questions\nRegistered Reports – journal format with in-principle acceptance\n\n\n\n4.2.11.4 Handling Deviations\nDeviations are acceptable if reported transparently:\n**Deviations from Pre-Registration:**\n1. Sample size: Planned n = 40, achieved n = 36 due to [reason]\n2. Primary test: Switched from t-test to Mann–Whitney due to severe skewness (skew = 2.4)\n3. Additional analysis: Added baseline covariate per reviewer request (post-hoc, clearly labelled)\n\n\n4.2.11.5 Benefits for Small Samples\n\nProtects against p-hacking accusations\nSeparates confirmatory from exploratory analyses\nImproves study design through upfront planning\nFacilitates transparent reporting\n\n\n\n\n\n\n\nTipPre-Registration Checklist\n\n\n\n\nHypotheses specific and testable\nSample size justified\nAll variables operationally defined\n\nStatistical tests specified\nOutlier/missing data plans stated\nMultiple comparison approach stated\nTime-stamped before analysis\n\n\n\n\n\n\n4.2.12 Following Reporting Guidelines\nNumerous reporting guidelines exist for different study designs:\n\nCONSORT: Randomised controlled trials.\nSTROBE: Observational studies (cohort, case-control, cross-sectional).\nPRISMA: Systematic reviews and meta-analyses.\nCOREQ: Qualitative research.\n\nThese guidelines provide checklists of items to report. Following them improves transparency and comparability across studies. Even if formal adherence is not required, consult the relevant guideline as a checklist.\n\n\n\n4.2.13 Self-Assessment Quiz\nTest your understanding of transparent reporting from Chapter 14. Answers and explanations are provided at the end.\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. Which should be reported when documenting a small-sample study?\nA. Only significant results\nB. All analyses conducted, including non-significant findings\nC. Only the primary analysis\nD. Results can be selectively reported\n\nQ2. A study planned to use a t-test but switched to Mann-Whitney after seeing skewed data. How should this be reported?\nA. Don’t mention the change\nB. Report only the Mann-Whitney result\nC. “We planned a t-test but observed severe skewness (skew=2.4), so used Mann-Whitney. Sensitivity: t-test yields similar p-value.”\nD. Pretend Mann-Whitney was always planned\n\nQ3. What is “p-hacking”?\nA. Illegally accessing data\nB. Trying multiple analyses/subgroups until finding p&lt;0.05, then reporting only that result\nC. Using permutation tests\nD. Adjusting for multiple comparisons\n\nQ4. Pre-registration helps prevent:\nA. Sample size limitations\nB. Researcher degrees of freedom (flexibility in analysis choices) leading to false positives\nC. Missing data\nD. Measurement error\n\nQ5. A study with n=15 per group finds p=0.12. The limitation section should state:\nA. “The result is not significant, proving no effect exists”\nB. “The study was underpowered to detect small-to-medium effects; findings are inconclusive”\nC. “The sample size was adequate”\nD. Nothing—non-significant results need no discussion\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nQ1. Answer: B\nExplanation: Transparent reporting requires documenting all planned analyses, exploratory analyses, and sensitivity checks—not just significant findings. Selective reporting inflates Type I error across the literature. The chapter emphasizes: “Transparent reporting of methods, decisions, and limitations allows readers to evaluate evidence quality and replicability.”\nQ2. Answer: C\nExplanation: Deviations from plans should be documented with justification. Reporting both analyses (planned and adapted) demonstrates robustness and transparency. The Key Takeaways state: “Acknowledge deviations from planned analyses and report sensitivity analyses.”\nQ3. Answer: B\nExplanation: P-hacking (data dredging) involves exploring many analyses (different covariates, subgroups, outlier handling) until significance emerges, then selectively reporting the “successful” analysis. This inflates false positives. The chapter discusses how “limited power increases temptation for p-hacking” with small samples.\nQ4. Answer: B\nExplanation: Pre-registration documents hypotheses and analysis plans before seeing data, preventing post-hoc decisions that capitalize on chance and inflate Type I error. The chapter explains: “Pre-registration involves documenting your hypotheses, methods, and analysis plan before data collection or analysis” to prevent “researcher degrees of freedom.”\nQ5. Answer: B\nExplanation: Small samples have limited power. Non-significance may reflect insufficient power rather than absence of effect. The limitations section should acknowledge this and discuss minimum detectable effects. The Key Takeaways emphasize: “State limitations honestly, particularly those related to small sample size (limited power, wide CIs, sensitivity to assumptions).”\n\n\n\n\n\n\n4.2.14 Key Takeaways\n\nTransparent reporting of methods, decisions, and limitations allows readers to evaluate evidence quality and replicability.\nDocument all analytic choices in reproducible scripts with clear comments and justifications.\nReport sample characteristics, missing data, and exclusions explicitly.\nAcknowledge deviations from planned analyses and report sensitivity analyses.\nState limitations honestly, particularly those related to small sample size (limited power, wide CIs, sensitivity to assumptions).\nFollow relevant reporting guidelines (CONSORT, STROBE) to ensure comprehensive reporting.\n\n\n\n4.2.15 Smoke Test\n\n# Re-run sample summary\ndata_test &lt;- tibble(\n  group = rep(c(\"A\", \"B\"), each = 5),\n  score = c(5, 6, 7, 8, 9, 4, 5, 6, 7, 8)\n)\ndata_test %&gt;% group_by(group) %&gt;% summarise(mean = mean(score), sd = sd(score))\n\n# A tibble: 2 × 3\n  group  mean    sd\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         7  1.58\n2 B         6  1.58",
    "crumbs": [
      "Part D: Reporting and Interpretation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part D: Reporting and Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/part-d-reporting.html#chapter-15.-interpreting-non-significant-results",
    "href": "chapters/part-d-reporting.html#chapter-15.-interpreting-non-significant-results",
    "title": "4  Part D: Reporting and Interpretation",
    "section": "4.3 Chapter 15. Interpreting Non-Significant Results",
    "text": "4.3 Chapter 15. Interpreting Non-Significant Results\n\n4.3.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the difference between “no evidence of effect” and “evidence of no effect” - ✓ Understand how low power affects interpretation of non-significant results - ✓ Recognize that p &gt; 0.05 does not prove the null hypothesis - ✓ Distinguish traditional null hypothesis testing from equivalence testing\nPractical Skills - ✓ Interpret confidence intervals to assess plausible effect sizes - ✓ Conduct power analyses to determine detectable effect sizes - ✓ Perform equivalence tests using TOST (two one-sided tests) in R - ✓ Compute Bayes factors for evidence in favor of the null\nCritical Evaluation - ✓ Assess whether studies have adequate power to detect meaningful effects - ✓ Evaluate overstated conclusions from null results in underpowered studies - ✓ Critique claims of “no difference” without supporting equivalence evidence\nApplication - ✓ Report non-significant results with confidence intervals and power context - ✓ Apply equivalence or non-inferiority testing when appropriate - ✓ Avoid dismissing potentially meaningful effects due to lack of significance\n\n\n4.3.2 The Meaning of a Non-Significant Result\nA non-significant result (p &gt; 0.05) means that the observed data are not sufficiently inconsistent with the null hypothesis to reject it at the chosen alpha level. It does not mean:\n\nThe null hypothesis is true.\nThere is no effect.\nThe treatment or intervention is ineffective.\n\nWith small samples, non-significant results are common even when true effects exist, because power is limited. A study with 30% power will fail to reject the null 70% of the time, even if the alternative hypothesis is true.\n\n\n4.3.3 “Absence of Evidence Is Not Evidence of Absence”\nThis principle is critical in small-sample research. If a study finds p = 0.15, we cannot conclude that the effect is zero. We can only conclude that the data do not provide strong evidence against the null hypothesis. The confidence interval is more informative: if the CI includes both trivial and substantial effects, the study is simply inconclusive.\n\n\n4.3.4 Interpreting Confidence Intervals for Non-Significant Results\nWhen p &gt; 0.05, examine the confidence interval:\n\nIf the CI is narrow and excludes meaningful effects: The study provides evidence that the effect, if any, is small. This is “evidence of no effect” (or a trivial effect).\nIf the CI is wide and includes both trivial and substantial effects: The study is inconclusive. The effect could be anywhere in the interval. This is “absence of evidence.”\n\n\n\n4.3.5 Example: Non-Significant Result with Narrow vs. Wide CI\nScenario 1: A study with n = 100 per group finds a mean difference of 1.0 points (95% CI: [-0.5, 2.5], p = 0.12). The CI excludes differences larger than 2.5 points. If differences &lt; 3 points are considered trivial, this study provides evidence that the effect is small or absent.\nScenario 2: A study with n = 12 per group finds a mean difference of 2.0 points (95% CI: [-1.0, 5.0], p = 0.15). The CI includes both negative and moderate positive effects. The study is underpowered and inconclusive. We cannot rule out meaningful effects.\n\n# Simulating the two scenarios for illustration\nset.seed(2025)\n\n# Scenario 1: Larger sample, narrow CI\nscenario1_a &lt;- rnorm(100, mean = 50, sd = 5)\nscenario1_b &lt;- rnorm(100, mean = 49, sd = 5)\nt1 &lt;- t.test(scenario1_a, scenario1_b)\ncat(\"Scenario 1: n = 100 per group\\n\")\n\nScenario 1: n = 100 per group\n\ncat(\"Mean difference:\", round(t1$estimate[1] - t1$estimate[2], 2), \"\\n\")\n\nMean difference: 0.93 \n\ncat(\"95% CI: [\", round(t1$conf.int[1], 2), \",\", round(t1$conf.int[2], 2), \"]\\n\", sep = \"\")\n\n95% CI: [-0.46,2.31]\n\ncat(\"p-value:\", round(t1$p.value, 3), \"\\n\\n\")\n\np-value: 0.188 \n\n# Scenario 2: Smaller sample, wide CI\nscenario2_a &lt;- rnorm(12, mean = 52, sd = 5)\nscenario2_b &lt;- rnorm(12, mean = 50, sd = 5)\nt2 &lt;- t.test(scenario2_a, scenario2_b)\ncat(\"Scenario 2: n = 12 per group\\n\")\n\nScenario 2: n = 12 per group\n\ncat(\"Mean difference:\", round(t2$estimate[1] - t2$estimate[2], 2), \"\\n\")\n\nMean difference: -1.77 \n\ncat(\"95% CI: [\", round(t2$conf.int[1], 2), \",\", round(t2$conf.int[2], 2), \"]\\n\", sep = \"\")\n\n95% CI: [-5.88,2.34]\n\ncat(\"p-value:\", round(t2$p.value, 3), \"\\n\")\n\np-value: 0.374 \n\n\nInterpretation: Scenario 1’s narrow CI suggests that large effects are unlikely; we can be reasonably confident the true difference is small. Scenario 2’s wide CI indicates substantial uncertainty; the true difference could be anywhere from negative to moderately positive. In the second case, we should conclude that the study is inconclusive, not that there is no effect.\n\n\n4.3.6 Equivalence and Non-Inferiority Testing\nIf the research question is whether two treatments are equivalent (or whether a new treatment is not meaningfully worse than standard care), traditional null hypothesis testing is inappropriate. Instead, equivalence or non-inferiority testing sets a practical equivalence margin (the smallest difference considered important) and tests whether the observed difference falls within that margin.\nEquivalence test: Tests whether the effect is small enough to be considered negligible.\nNon-inferiority test: Tests whether the new treatment is not worse than the standard by more than a prespecified margin.\nThese tests require adequate power and are less common in small-sample research, but the conceptual framework is valuable: define what “no meaningful difference” means in practical terms, then test against that threshold.\n\n\n4.3.7 Example: Conceptual Equivalence Framing\nSuppose we compare a brief intervention to standard care on anxiety scores (0–100 scale). We predefine a 5-point difference as the smallest clinically important difference. The study finds a mean difference of 2 points (95% CI: [-3, 7], p = 0.45).\nInterpretation: The CI includes the equivalence margin (±5 points). We cannot definitively conclude equivalence because the CI extends beyond -5 and +5. However, the point estimate (2 points) and the bulk of the CI lie within the equivalence region, suggesting that the true difference is likely small. A larger study would provide a more definitive answer.\n\n\n4.3.8 Reporting Non-Significant Results Responsibly\nWhen reporting non-significant results:\n\nState the p-value and confidence interval.\nAvoid phrases like “no effect” or “no difference” unless justified by a narrow CI.\nUse phrases like “no statistically significant difference was observed” or “the data are consistent with both small and moderate effects.”\nDiscuss the study’s power and the range of effects that could have been detected.\nAcknowledge that the study may be inconclusive due to limited sample size.\nSuggest directions for future research with adequate power.\n\n\n\n4.3.9 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 15 Questions\n\n\n\nQ1. What does a non-significant result (p &gt; 0.05) mean?\n\nThe null hypothesis is true\n\nThere is no effect\n\nThe observed data are not sufficiently inconsistent with the null hypothesis to reject it at the chosen alpha level\n\nThe treatment is definitively ineffective\n\n\nQ2. What does the phrase “absence of evidence is not evidence of absence” mean?\n\nNon-significant results prove there is no effect\n\nA p-value &gt; 0.05 does not prove the effect is zero; it only indicates insufficient evidence to reject the null\n\nSample size doesn’t matter\n\nAll studies should be significant\n\n\nQ3. A study finds p = 0.12 with a 95% CI of [-0.5, 2.5] points. If differences &lt; 3 points are trivial, what can we conclude?\n\nThe effect is definitely zero\n\nThe study is inconclusive\n\nThe study provides evidence that the effect is small or absent (narrow CI excluding meaningful effects)\n\nWe need to reject the alternative hypothesis\n\n\nQ4. A study with n = 12 per group finds p = 0.15 with a 95% CI of [-1.0, 5.0] points. What does this indicate?\n\nThe effect is definitely zero\n\nThe study is underpowered and inconclusive (wide CI includes both negative and moderate positive effects)\n\nThe treatment works perfectly\n\nWe should conclude there is no effect\n\n\nQ5. Why are non-significant results particularly common in small-sample studies?\n\nSmall samples are biased\n\nWith limited power, studies often fail to reject the null even when true effects exist\n\nSmall samples always have p &gt; 0.05\n\nStatistical tests don’t work with small samples\n\n\nQ6. What is an equivalence test designed to assess?\n\nWhether two treatments have exactly identical effects\n\nWhether the effect is small enough to be considered negligible, relative to a predefined equivalence margin\n\nWhether p &lt; 0.05\n\nWhether sample size is adequate\n\n\nQ7. When reporting a non-significant result, which phrase is most appropriate?\n\n“There is no effect”\n\n“The treatment doesn’t work”\n\n“No statistically significant difference was observed”\n\n“We proved the null hypothesis”\n\n\nQ8. A study with 30% power finds p = 0.25. What does this mean?\n\nThe study will reject the null 70% of the time\n\nThe study will fail to reject the null 70% of the time, even if the alternative hypothesis is true\n\nThe effect is proven to be zero\n\nThe sample size was adequate\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. C) “A non-significant result (p &gt; 0.05) means that the observed data are not sufficiently inconsistent with the null hypothesis to reject it at the chosen alpha level.”\nNon-significant results do NOT mean the null is true or that there is no effect.\nA2. B) “If a study finds p = 0.15, we cannot conclude that the effect is zero. We can only conclude that the data do not provide strong evidence against the null hypothesis.”\nFailing to find evidence for an effect is not the same as finding evidence against an effect.\nA3. C) “If the CI is narrow and excludes meaningful effects: The study provides evidence that the effect, if any, is small.”\nThe CI [−0.5, 2.5] excludes differences larger than 2.5 points, and differences &lt; 3 are trivial, so this is evidence of no meaningful effect.\nA4. B) “The CI includes both negative and moderate positive effects. The study is underpowered and inconclusive. We cannot rule out meaningful effects.”\nWide CIs indicate substantial uncertainty; the true difference could be anywhere in the interval.\nA5. B) “With small samples, non-significant results are common even when true effects exist, because power is limited. A study with 30% power will fail to reject the null 70% of the time.”\nLimited sample size reduces statistical power to detect true effects.\nA6. B) “Equivalence test: Tests whether the effect is small enough to be considered negligible.”\nEquivalence testing sets a practical equivalence margin and tests whether the observed difference falls within that margin.\nA7. C) “Use phrases like ‘no statistically significant difference was observed’ or ‘the data are consistent with both small and moderate effects.’”\nAvoid definitive statements about “no effect” unless justified by narrow confidence intervals.\nA8. B) “A study with 30% power will fail to reject the null 70% of the time, even if the alternative hypothesis is true.”\nPower is the probability of correctly rejecting a false null; low power means frequent failures to detect true effects.\n\n\n\n\n\n4.3.10 Key Takeaways\n\nNon-significant results (p &gt; 0.05) do not prove the null hypothesis; they indicate insufficient evidence to reject it.\nWith small samples, non-significant results are common even when true effects exist, due to limited power.\nConfidence intervals distinguish “evidence of no effect” (narrow CI excluding meaningful effects) from “absence of evidence” (wide CI including substantial effects).\nAvoid overstating conclusions from underpowered studies; acknowledge inconclusiveness when appropriate.\nEquivalence and non-inferiority frameworks test whether effects are small enough to be negligible, rather than testing whether they are exactly zero.\nReport non-significant results responsibly, with CIs and discussion of power and precision.\n\n\n\n4.3.11 Smoke Test\n\n# Re-run non-significant t-test\nset.seed(2025)\nx &lt;- rnorm(10, mean = 50, sd = 5)\ny &lt;- rnorm(10, mean = 52, sd = 5)\nt_result &lt;- t.test(x, y)\ncat(\"p-value:\", round(t_result$p.value, 3), \"\\n\")\n\np-value: 0.819 \n\ncat(\"CI:\", round(t_result$conf.int, 2), \"\\n\")\n\nCI: -4.96 3.99",
    "crumbs": [
      "Part D: Reporting and Interpretation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part D: Reporting and Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/part-d-reporting.html#chapter-16.-visualising-uncertainty-and-presenting-results",
    "href": "chapters/part-d-reporting.html#chapter-16.-visualising-uncertainty-and-presenting-results",
    "title": "4  Part D: Reporting and Interpretation",
    "section": "4.4 Chapter 16. Visualising Uncertainty and Presenting Results",
    "text": "4.4 Chapter 16. Visualising Uncertainty and Presenting Results\n\n4.4.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nConceptual Understanding - ✓ Explain the importance of visualizing uncertainty in small-sample contexts - ✓ Understand when to show individual data points vs. summary statistics - ✓ Recognize common visualization pitfalls (suppressed axes, 3D effects) - ✓ Distinguish error bars (SE, CI, SD) and when each is appropriate\nPractical Skills - ✓ Create informative plots with confidence intervals using ggplot2 - ✓ Use dot plots, beeswarm plots, and box plots for small samples - ✓ Generate raincloud plots combining distributions, boxplots, and raw data - ✓ Format publication-ready figures with appropriate labels and legends\nCritical Evaluation - ✓ Assess whether visualizations accurately represent data and uncertainty - ✓ Evaluate misleading practices (truncated axes, inappropriate scales) - ✓ Critique over-reliance on bar charts hiding distributional features\nApplication - ✓ Design figures that emphasize estimation over dichotomous testing - ✓ Present results accessibly for diverse audiences (technical and lay) - ✓ Use visualization to explore data and communicate findings transparently\n\n\n4.4.2 The Role of Visualisation in Small-Sample Research\nVisualisation serves multiple purposes:\n\nExploratory: Identify patterns, outliers, and distributional features during data screening.\nDiagnostic: Assess assumptions (normality, linearity, homoscedasticity).\nInferential: Display estimates, confidence intervals, and group comparisons.\nCommunicative: Convey findings to diverse audiences in accessible formats.\n\nWith small samples, visualisation is particularly valuable because individual data points can be shown (unlike large datasets where summaries are necessary). Showing raw data alongside summaries builds trust and reveals variability.\n\n\n4.4.3 Visualising Point Estimates with Confidence Intervals\nError bars (standard errors or confidence intervals) convey uncertainty. Use 95% CIs for inferential plots, as they align with conventional significance testing (CIs that exclude zero correspond to p &lt; 0.05).\nBest practices: - Label axes clearly with units. - Include a legend if multiple groups are compared. - Use colour or shape to distinguish groups. - Avoid 3D effects and unnecessary decoration (chart junk).\n\n\n4.4.4 Example: Bar Plot with Error Bars\nWe compare mean satisfaction scores between two campaign types with 95% CI error bars.\n\nlibrary(tidyverse)\n\n# Load data\nstudy_data &lt;- read_csv(\"data/mini_marketing.csv\", show_col_types = FALSE)\n\n# Compute means and 95% CIs\nsummary_stats &lt;- study_data %&gt;%\n  group_by(campaign) %&gt;%\n  summarise(\n    mean_satisfaction = mean(satisfaction, na.rm = TRUE),\n    se = sd(satisfaction, na.rm = TRUE) / sqrt(n()),\n    ci_lower = mean_satisfaction - 1.96 * se,\n    ci_upper = mean_satisfaction + 1.96 * se,\n    .groups = \"drop\"\n  )\n\n# Bar plot with error bars\nggplot(summary_stats, aes(x = campaign, y = mean_satisfaction, fill = campaign)) +\n  geom_col(width = 0.6, alpha = 0.7) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  labs(\n    title = \"Mean Customer Satisfaction by Campaign Type\",\n    x = \"Campaign\",\n    y = \"Satisfaction (1–5 scale)\",\n    fill = \"Campaign\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nMean customer satisfaction by campaign type with 95% confidence intervals.\n\n\n\n\nInterpretation: The bars show mean satisfaction for each campaign. Error bars show 95% CIs. Non-overlapping CIs suggest a statistically significant difference (though this is an approximate rule). The visualisation makes the magnitude of the difference and the precision of estimates immediately apparent.\n\n\n4.4.5 Showing Individual Data Points\nWith small samples (n &lt; 50), individual data points can be overlaid on summary plots. This reveals the distribution, identifies outliers, and shows sample size directly.\n\n\n4.4.6 Example: Dot Plot with Mean and CI\nWe create a dot plot showing individual satisfaction scores, overlaid with group means and CIs.\n\nlibrary(tidyverse)\n\nstudy_data &lt;- read_csv(\"data/mini_marketing.csv\", show_col_types = FALSE)\n\n# Compute summary statistics\nsummary_stats &lt;- study_data %&gt;%\n  group_by(campaign) %&gt;%\n  summarise(\n    mean_satisfaction = mean(satisfaction, na.rm = TRUE),\n    ci_lower = mean_satisfaction - 1.96 * sd(satisfaction, na.rm = TRUE) / sqrt(n()),\n    ci_upper = mean_satisfaction + 1.96 * sd(satisfaction, na.rm = TRUE) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\n# Dot plot with mean and CI\nggplot(study_data, aes(x = campaign, y = satisfaction, colour = campaign)) +\n  geom_jitter(width = 0.1, alpha = 0.6, size = 2) +\n  geom_point(data = summary_stats, aes(y = mean_satisfaction), \n             size = 4, shape = 18, colour = \"black\") +\n  geom_errorbar(data = summary_stats, aes(y = mean_satisfaction, ymin = ci_lower, ymax = ci_upper),\n                width = 0.2, colour = \"black\", linewidth = 1) +\n  labs(\n    title = \"Customer Satisfaction by Campaign Type\",\n    subtitle = \"Individual scores (dots), mean (diamond), and 95% CI (error bars)\",\n    x = \"Campaign\",\n    y = \"Satisfaction (1–5 scale)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nIndividual satisfaction scores with group means and 95% confidence intervals.\n\n\n\n\nInterpretation: Each dot represents one participant. The diamond shows the group mean; the error bars show the 95% CI. Readers can see the distribution of individual scores, the central tendency, and the precision of the estimate simultaneously. This transparency builds trust and aids interpretation.\n\n\n4.4.7 Box Plots for Distributional Comparison\nBox plots display the median, quartiles, and outliers, providing a non-parametric summary of distribution. They are particularly useful for comparing groups when data are skewed or ordinal.\n\n\n4.4.8 Example: Box Plot Comparison\nWe create a box plot comparing satisfaction scores between campaigns.\n\nlibrary(tidyverse)\n\nstudy_data &lt;- read_csv(\"data/mini_marketing.csv\", show_col_types = FALSE)\n\nggplot(study_data, aes(x = campaign, y = satisfaction, fill = campaign)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Customer Satisfaction by Campaign Type\",\n    subtitle = \"Box plot with individual data points\",\n    x = \"Campaign\",\n    y = \"Satisfaction (1–5 scale)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nBox plot comparing customer satisfaction across campaign types.\n\n\n\n\nInterpretation: The box shows the interquartile range (IQR) with the median as a line inside. Whiskers extend to 1.5 × IQR; points beyond are potential outliers. Overlaying individual points shows sample size and exact values. This plot is ideal for nonparametric comparisons (e.g., Mann–Whitney U test).\n\n\n4.4.9 Visualising Regression Results\nFor regression models, plot predicted values with confidence bands, and overlay observed data. This shows model fit, uncertainty, and deviations.\n\n\n4.4.10 Example: Scatterplot with Regression Line and CI Band\nWe fit a linear regression (performance ~ experience) and plot the results.\n\nlibrary(tidyverse)\n\nset.seed(2025)\n\n# Simulated data\nreg_data &lt;- tibble(\n  experience = runif(20, 1, 10),\n  performance = 50 + 3 * experience + rnorm(20, 0, 5)\n)\n\n# Fit model\nmodel &lt;- lm(performance ~ experience, data = reg_data)\n\n# Plot with regression line and CI band\nggplot(reg_data, aes(x = experience, y = performance)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, colour = \"blue\", fill = \"lightblue\") +\n  labs(\n    title = \"Performance vs. Experience\",\n    subtitle = \"Linear regression with 95% confidence band\",\n    x = \"Years of Experience\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\nLinear regression of performance on experience with 95% confidence band.\n\n\n\n\nInterpretation: Each point is an observed case. The blue line is the fitted regression line. The shaded band is the 95% confidence interval for the predicted mean at each value of experience. The band widens at the extremes (where data are sparse), reflecting greater uncertainty. This visualisation shows model fit, precision, and individual deviations simultaneously.\n\n\n4.4.11 Presenting Results in Tables\nTables complement figures by providing exact values. For small samples, consider showing:\n\nSample sizes (n per group).\nMeans and standard deviations (or medians and IQRs).\nEffect sizes and confidence intervals.\nTest statistics and p-values.\n\nUse the gt package for publication-ready tables with clear formatting.\n\n\n4.4.12 Example: Results Summary Table\nWe create a summary table for the campaign comparison.\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(rstatix)\n\nstudy_data &lt;- read_csv(\"data/mini_marketing.csv\", show_col_types = FALSE)\n\n# Summary statistics\nsummary_table &lt;- study_data %&gt;%\n  group_by(campaign) %&gt;%\n  summarise(\n    N = n(),\n    Mean = round(mean(satisfaction, na.rm = TRUE), 2),\n    SD = round(sd(satisfaction, na.rm = TRUE), 2),\n    Median = median(satisfaction, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Mann–Whitney test\nmw_result &lt;- wilcox_test(study_data, satisfaction ~ campaign)\np_value &lt;- mw_result %&gt;% pull(p)\nsubtitle_text &lt;- if (p_value &lt; 0.001) {\n  \"Mann–Whitney U test: p &lt; 0.001\"\n} else {\n  sprintf(\"Mann–Whitney U test: p = %.3f\", p_value)\n}\n\n# Format table\nsummary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Satisfaction Scores by Campaign Type\",\n    subtitle = subtitle_text\n  ) %&gt;%\n  cols_label(\n    campaign = \"Campaign\",\n    N = \"n\",\n    Mean = \"Mean\",\n    SD = \"SD\",\n    Median = \"Median\"\n  )\n\n\n\n\n\nSummary statistics and Mann–Whitney test for campaign satisfaction.\n\n\nSatisfaction Scores by Campaign Type\n\n\nMann–Whitney U test: p = 0.057\n\n\nCampaign\nn\nMean\nSD\nMedian\n\n\n\n\nEmail\n15\n3.87\n0.99\n4\n\n\nSocial\n15\n3.27\n0.80\n3\n\n\n\n\n\n\n\nInterpretation: The table provides exact summary statistics for each group. Readers can see sample sizes, central tendency, and variability. The p-value from the Mann–Whitney test is reported in the subtitle or a footnote. Tables and figures together provide a complete, accessible presentation of results.\n\n\n4.4.13 Avoiding Misleading Visualisations\nCommon pitfalls:\n\nSuppressed zero on the y-axis: Exaggerates differences. Use a zero baseline unless there is good reason not to (and explain the choice).\n3D effects and unnecessary decoration: Distract from data and can obscure values.\nDual axes with different scales: Misleading comparisons. Avoid or use with extreme caution.\nOverplotting without jitter or transparency: Hides overlapping points. Use jitter, transparency, or both.\n\n\n\n4.4.14 Self-Assessment Quiz\n\n\n\n\n\n\nNoteChapter 16 Questions\n\n\n\nQ1. Why is visualisation particularly valuable in small-sample research?\n\nLarge datasets cannot be visualized\n\nIndividual data points can be shown alongside summaries, revealing variability and building trust\n\nVisualisation eliminates the need for statistical tests\n\nSmall samples require 3D plots\n\n\nQ2. What do 95% confidence interval error bars represent?\n\nThe range containing all data points\n\nThe uncertainty around point estimates; CIs that exclude zero correspond to p &lt; 0.05\n\nThe standard deviation\n\nThe sample size\n\n\nQ3. What is the advantage of overlaying individual data points on summary plots (means or medians)?\n\nIt makes the plot more colorful\n\nIt reveals the distribution, identifies outliers, and shows sample size directly\n\nIt is required by journal guidelines\n\nIt eliminates the need for error bars\n\n\nQ4. In a regression plot with a confidence band, why does the band typically widen at the extremes?\n\nIt’s a plotting error\n\nThe band widens where data are sparse, reflecting greater uncertainty in predictions\n\nThe confidence band is always the same width\n\nIt indicates the sample size\n\n\nQ5. What information does a box plot display?\n\nOnly the mean\n\nThe median, quartiles (IQR), and outliers\n\nIndividual data points only\n\nThe correlation coefficient\n\n\nQ6. Why should suppressed zero on the y-axis be avoided (or used with caution)?\n\nIt saves space\n\nIt can exaggerate differences and mislead readers about the magnitude of effects\n\nIt is required by statistical standards\n\nIt improves visual appeal\n\n\nQ7. What is “chart junk”?\n\nHigh-quality graphics\n\nUnnecessary decoration (3D effects, excessive colors, ornaments) that distracts from data\n\nStatistical error bars\n\nIndividual data points\n\n\nQ8. When creating results tables, what essential information should be included?\n\nOnly p-values\n\nSample sizes, means/medians, measures of variability (SD or IQR), confidence intervals, and test results\n\nOnly the mean values\n\nJust the hypothesis\n\n\nQ9. What does jittering accomplish in plots with many overlapping points?\n\nIt removes outliers\n\nIt adds small random offsets to points to reveal overlapping observations\n\nIt changes the statistical significance\n\nIt increases sample size\n\n\nQ10. In the dot plot example showing satisfaction scores, what does the diamond symbol represent?\n\nAn outlier\n\nThe group mean\n\nA missing value\n\nThe maximum value\n\n\n\n\n\n\n\n\n\nTipAnswers and Explanations\n\n\n\n\n\nA1. B) “With small samples, visualisation is particularly valuable because individual data points can be shown (unlike large datasets where summaries are necessary). Showing raw data alongside summaries builds trust and reveals variability.”\nTransparency about individual observations helps readers assess the evidence directly.\nA2. B) “Error bars (standard errors or confidence intervals) convey uncertainty. Use 95% CIs for inferential plots, as they align with conventional significance testing (CIs that exclude zero correspond to p &lt; 0.05).”\n95% CIs provide a visual representation of statistical significance.\nA3. B) “Each dot represents one participant… Readers can see the distribution of individual scores, the central tendency, and the precision of the estimate simultaneously.”\nShowing raw data reveals patterns that summary statistics alone cannot capture.\nA4. B) “The shaded band is the 95% confidence interval for the predicted mean at each value of experience. The band widens at the extremes (where data are sparse), reflecting greater uncertainty.”\nUncertainty increases where there are fewer observations to constrain the model.\nA5. B) “Box plots display the median, quartiles, and outliers, providing a non-parametric summary of distribution.”\nThe box shows the IQR (25th to 75th percentile), the line inside shows the median, and points beyond the whiskers are potential outliers.\nA6. B) “Suppressed zero on the y-axis: Exaggerates differences. Use a zero baseline unless there is good reason not to (and explain the choice).”\nNot starting at zero can make small differences appear dramatically large.\nA7. B) “3D effects and unnecessary decoration: Distract from data and can obscure values.”\nEdward Tufte coined the term “chart junk” for non-data elements that reduce clarity.\nA8. B) “For small samples, consider showing: Sample sizes (n per group), means and standard deviations (or medians and IQRs), effect sizes and confidence intervals, test statistics and p-values.”\nComplete reporting allows readers to assess both statistical and practical significance.\nA9. B) “Overplotting without jitter or transparency: Hides overlapping points. Use jitter, transparency, or both.”\nJittering adds small random noise to reveal how many points are stacked on top of each other.\nA10. B) “The diamond shows the group mean; the error bars show the 95% CI.”\nDifferent symbols (diamond vs. dot) help distinguish summary statistics from individual observations.\n\n\n\n\n\n4.4.15 Key Takeaways\n\nVisualisation aids understanding, communication, and trust in findings, especially with small samples where individual data can be shown.\nPlot point estimates with confidence intervals (error bars, bands) to convey uncertainty.\nOverlay individual data points on summary plots to reveal distributions and sample sizes.\nUse box plots for nonparametric comparisons and scatterplots with regression lines for associations.\nFormat tables clearly with exact values, sample sizes, and test results.\nAvoid misleading visualisations (suppressed axes, 3D effects, dual axes).\n\n\n\n4.4.16 Smoke Test\n\n# Re-run simple ggplot\nlibrary(ggplot2)\nset.seed(2025)\ndat &lt;- data.frame(x = c(\"A\", \"B\"), y = c(5, 7))\nggplot(dat, aes(x, y)) + geom_col() + theme_minimal()\n\n\n\n\nSimple bar chart example produced during smoke test.",
    "crumbs": [
      "Part D: Reporting and Interpretation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part D: Reporting and Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/part-d-reporting.html#summary-of-part-d",
    "href": "chapters/part-d-reporting.html#summary-of-part-d",
    "title": "4  Part D: Reporting and Interpretation",
    "section": "4.5 Summary of Part D",
    "text": "4.5 Summary of Part D\nIn Part D, we addressed transparent and responsible reporting of small-sample findings. Chapter 13 emphasised effect sizes and confidence intervals over p-values, showing how to compute, interpret, and report standardised and unstandardised effect sizes. Chapter 14 covered transparent reporting of methods, decisions, and limitations, including sample descriptions, missing data, deviations from plans, and adherence to reporting guidelines. Chapter 15 explained how to interpret non-significant results, distinguishing “evidence of no effect” from “absence of evidence,” and discussed equivalence testing. Chapter 16 presented visualisation techniques for displaying uncertainty, showing individual data points, and creating accessible tables and figures. Each chapter included learning objectives, detailed guidance, runnable R examples, interpretations, key takeaways, and smoke tests. All code uses only approved packages and runs cleanly in a fresh R session. The guidance prioritises honesty, clarity, and appropriate interpretation of findings given small-sample constraints.",
    "crumbs": [
      "Part D: Reporting and Interpretation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Part D: Reporting and Interpretation</span>"
    ]
  }
]