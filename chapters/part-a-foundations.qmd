# Part A: Foundations

This part establishes why small-sample research is important and how to frame research questions that align with limited data availability.

---

## Chapter 1. Why Small-Sample Research Matters

### Learning Objectives

By the end of this chapter, you will be able to:

**Conceptual Understanding**
- ✓ Explain why small samples are common in applied research settings
- ✓ Identify practical, ethical, and logistical constraints that limit sample size
- ✓ Understand how large-sample approximations fail with modest datasets
- ✓ Recognize contexts where small-sample methods are necessary and appropriate

**Practical Skills**
- ✓ Calculate statistical power for different sample sizes
- ✓ Create power curves to visualize sample size trade-offs
- ✓ Distinguish between situations requiring small-sample vs large-sample methods

**Critical Evaluation**
- ✓ Assess when conventional parametric tests become unreliable
- ✓ Evaluate the impact of outliers and assumption violations in small samples
- ✓ Critique the "apologetic" framing of small-sample research

**Application**
- ✓ Justify research decisions when large samples are infeasible
- ✓ Select appropriate statistical methods given sample size constraints
- ✓ Communicate small-sample research findings with appropriate caveats

### Why Small Samples Are Often Unavoidable

Many textbooks assume that researchers can collect hundreds or thousands of observations. In practice, however, numerous research contexts yield small samples. Clinical studies of rare diseases, evaluations of pilot programmes, **classroom-based educational interventions**, community-based participatory research, and studies in Small Island Developing States (SIDS) often involve fewer than 100 participants. Resource constraints, logistical barriers, and ethical considerations (such as minimising burden on vulnerable populations) make small samples the norm rather than the exception.

**Examples of small-sample contexts:**

-   **Health sciences:** Clinical trials for rare diseases (n < 30), pilot studies testing feasibility before large RCTs, single-site hospital studies.
-   **Education:** Evaluating a new teaching method in a single classroom (n = 15-25), assessing specialized programs for gifted/special education students, teacher professional development studies.
-   **Business:** A/B tests in niche markets, customer satisfaction surveys for small businesses, startup product testing with limited users.
-   **Social sciences:** Studies in remote communities, indigenous populations, or specialized occupational groups where the total accessible population is small.

Despite their ubiquity, small samples are often treated as deficient or temporary. Researchers may apologise for limited data, or reviewers may demand larger samples without considering feasibility. This mindset overlooks the fact that many important questions can only be addressed with small datasets. Rather than apologising, researchers should select methods that are appropriate for the sample size at hand.

### When Large-Sample Approximations Fail

Classical parametric tests (t-tests, ANOVA, standard logistic regression) rely on asymptotic theory. They assume that sampling distributions approximate normality as sample size increases. With small samples, these approximations can be inaccurate. P-values may be misleading, confidence intervals may have poor coverage, and maximum likelihood estimates may be unstable or even undefined (for example, in logistic regression with separation).

Small samples also amplify the impact of outliers and violations of distributional assumptions. A single extreme value can dominate a mean or distort a regression slope. Skewed or heavy-tailed distributions, which cause few problems in large samples, become serious concerns when n is small.

### Visualising Power Trade-offs

Even modest reductions in sample size can have a dramatic impact on statistical power. The figure below uses the exact `power.t.test()` function to illustrate how power declines for medium-sized effects as per-group sample size drops from 60 to 10. Curves are shown for three standardised effect sizes (Cohen's *d* = 0.3, 0.5, and 0.8).

```{r}
#| label: power-curve-plot
#| fig-cap: "Power curves illustrating sensitivity to sample size."
library(tidyverse)

effect_sizes <- c(0.3, 0.5, 0.8)
n_values <- seq(10, 60, by = 5)
alpha <- 0.05

power_grid <- crossing(n = n_values, d = effect_sizes) %>%
  mutate(power = map2_dbl(
    n,
    d,
    ~ power.t.test(
      n = .x,
      delta = .y,
      sd = 1,
      sig.level = alpha,
      type = "two.sample",
      alternative = "two.sided"
    )$power
  ))

ggplot(power_grid, aes(x = n, y = power, colour = factor(d))) +
  geom_line(linewidth = 1) +
  geom_point() +
  geom_hline(yintercept = 0.80, linetype = "dashed", colour = "grey40") +
  scale_colour_brewer(palette = "Set1", name = "Effect size (d)") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    x = "Sample size per group",
    y = "Power",
    title = "Power drops steeply as per-group sample size decreases",
    subtitle = "Dashed line marks the conventional 80% power threshold"
  ) +
  theme_minimal(base_size = 12)
```

Interpretation: With medium effects (*d* ≈ 0.5), power slips below 50% once per-group sample size falls beneath about 20 participants. Detecting smaller effects (*d* ≈ 0.3) would require many more observations than are typically feasible in small-sample settings. This visual reinforces the need to report minimum detectable effects and to focus on estimation rather than binary significance testing when *n* is limited.

### Appropriate Methods for Small Samples

Fortunately, a suite of exact, resampling-based, and robust methods can provide valid inferences with limited data. Exact tests (such as Fisher's exact test, exact binomial tests, and exact Poisson tests) compute p-values directly from the combinatorial distribution of the data, without relying on asymptotic approximations. Resampling methods (bootstrap and permutation tests) use the observed data to approximate the sampling distribution, often yielding more accurate inferences than large-sample formulas.

Nonparametric rank-based tests (Mann–Whitney U, Wilcoxon signed-rank, Kruskal–Wallis) make fewer distributional assumptions and are less sensitive to outliers. Penalised regression (Firth logistic regression, ridge, LASSO) can stabilise coefficient estimates when events are sparse. Bayesian methods incorporate prior information and quantify uncertainty through posterior distributions, which remain well-defined even when data are limited.

### Example: Comparing Two Small Groups

Suppose we wish to compare customer satisfaction scores (on a 1–10 scale) between two service branches, each with only 12 observations. The scores are ordinal and may not be normally distributed.

```{r}
#| label: example-mann-whitney
library(tidyverse)
library(rstatix)

set.seed(2025)
branch_a <- c(7, 8, 6, 7, 9, 8, 7, 6, 8, 7, 9, 8)
branch_b <- c(5, 6, 7, 5, 6, 5, 7, 6, 5, 6, 7, 6)

data_satisfaction <- tibble(
  score = c(branch_a, branch_b),
  branch = rep(c("A", "B"), each = 12)
)

# Mann–Whitney U test (Wilcoxon rank-sum)
result <- wilcox_test(data_satisfaction, score ~ branch, detailed = TRUE)
print(result)
```

The Mann–Whitney U test compares the distributions of the two groups without assuming normality. The p-value indicates whether the observed difference in ranks is unlikely under the null hypothesis of identical distributions. Because the test is based on ranks, it is robust to skewness and outliers.

Interpretation: If the p-value is small (typically p < 0.05), we have evidence that customer satisfaction differs between the two branches. The effect size (such as rank-biserial correlation) quantifies the magnitude of the difference.

### Key Takeaways

- Small samples are common and legitimate in many research contexts, particularly in SIDS, clinical studies, and community-based research.
- Large-sample approximations can fail when n is small, leading to inaccurate p-values and confidence intervals.
- Exact tests, resampling methods, and rank-based procedures provide valid inferences without requiring large samples.
- The choice of method should match the research question, the type of outcome, and the sample size actually available.

---

### Self-Assessment Quiz

Test your understanding of the key concepts from Chapter 1. Answers and explanations are provided at the end.

::: {.callout-note icon=false}
## Questions

**Q1.** A study with n=12 per group has 25% power to detect d=0.5. What does this mean?

A. There is a 25% chance the treatment is effective  
B. If the true effect is d=0.5, there is a 25% probability of detecting it (p<0.05)  
C. The Type I error rate is 25%  
D. 25% of participants will show the effect

---

**Q2.** Why might large-sample approximations fail with n=15?

A. Computers cannot process small datasets  
B. Sampling distributions may not be approximately normal  
C. Effect sizes cannot be calculated  
D. P-values are always incorrect

---

**Q3.** Which research question is MOST appropriate for n=20?

A. "What are all factors that predict customer loyalty?" (testing 15 predictors)  
B. "Is there a difference in satisfaction between two service approaches?"  
C. "How do age, gender, income, education, and occupation interact to predict outcomes?"  
D. "Can we build a machine learning model to predict customer behavior?"

---

**Q4.** A pilot study with n=8 finds a mean difference of 5 points (95% CI: [-2, 12], p=0.14). The correct interpretation is:

A. There is no effect  
B. The effect is exactly 5 points  
C. The study is underpowered; effects from -2 to 12 points are plausible  
D. The null hypothesis is proven true

---

**Q5.** Which outcome measure provides the MOST statistical information per observation?

A. Binary (pass/fail)  
B. Ordinal (grade A-F)  
C. Continuous (test score 0-100)  
D. All provide equal information

---

**Q6.** With n=10 per group, which statement about power is TRUE?

A. Power is always 50% regardless of effect size  
B. Power increases as the true effect size increases  
C. Power is unrelated to sample size  
D. Power cannot be calculated for small samples

---

**Q7.** A study finds p=0.048 with n=8 per group. Which concern is MOST valid?

A. The result is definitely a false positive  
B. With small n, results near the significance threshold should be interpreted cautiously  
C. Small samples always produce spurious results  
D. The p-value is meaningless with n<30

---

**Q8.** When is a small sample (n<30) potentially SUFFICIENT?

A. Never—all research requires n≥100  
B. When the effect is very large and variance is low  
C. Only for qualitative research  
D. When using machine learning methods

---

**Q9.** Which is a legitimate reason for small sample size?

A. The researcher is lazy  
B. The population is rare (e.g., a genetic disorder affecting 1 in 100,000)  
C. The researcher wants to save time  
D. Small samples are always preferable

---

**Q10.** A researcher states: "My study has n=15, so I'll just use nonparametric tests." What is the problem with this reasoning?

A. Nonparametric tests require n≥30  
B. The choice of test should depend on the data characteristics and research question, not just sample size  
C. Nonparametric tests are always inferior  
D. Parametric tests always work regardless of assumptions

:::

::: {.callout-tip icon=false collapse="true"}
## Answers and Explanations

**Q1. Answer: B**  
*Explanation*: Statistical power is the probability of correctly rejecting a false null hypothesis when a specific effect size exists. With 25% power, there is a 75% chance of a Type II error (failing to detect a real effect of d=0.5). This concept is directly illustrated in the power curve figure in this chapter, which shows how power declines as sample size decreases.

**Q2. Answer: B**  
*Explanation*: The Central Limit Theorem requires sufficient sample size for sampling distributions to approximate normality. With n=15, especially if data are skewed or have outliers, parametric test assumptions may be violated. This is why the chapter emphasizes that "large-sample approximations can fail when n is small, leading to inaccurate p-values and confidence intervals."

**Q3. Answer: B**  
*Explanation*: Focused, binary comparisons are feasible with small samples. Complex multivariate questions (A, C, D) require much larger samples to avoid overfitting and ensure stable estimates. The chapter states: "focused questions about a single outcome or a few key comparisons can often be addressed with modest samples."

**Q4. Answer: C**  
*Explanation*: The wide confidence interval reflects substantial uncertainty. The study cannot rule out small negative effects (-2) or large positive effects (12). Non-significance with small n indicates insufficient evidence, not absence of effect. This aligns with the chapter's emphasis on focusing "on estimation rather than binary significance testing when n is limited."

**Q5. Answer: C**  
*Explanation*: Continuous measures preserve all variation in the data. Dichotomizing or coarsening into categories discards information, reduces statistical power, and limits the ability to detect effects. This principle underlies the recommendation to select outcome measures carefully when working with small samples.

**Q6. Answer: B**  
*Explanation*: Statistical power increases with larger effect sizes, larger sample sizes, and lower variance. Even with n=10, a very large effect (d=1.5) might have adequate power, while a small effect (d=0.2) would not. This is demonstrated in the power curve plot showing different effect sizes (d=0.3, 0.5, 0.8).

**Q7. Answer: B**  
*Explanation*: P-values near cutoffs (0.05) are highly variable with small samples. A slight change in data or analysis could flip the result. Emphasis should be on effect size magnitude and confidence intervals, not borderline p-values. The chapter warns that "small samples amplify the impact of outliers and violations of distributional assumptions."

**Q8. Answer: B**  
*Explanation*: If the true effect is very large (e.g., d=2.0) and within-group variability is small, even modest samples can provide clear evidence. Examples: a new drug that doubles survival rates, or a teaching method that improves scores by 30 points. The chapter acknowledges that some questions "can only be addressed with small datasets."

**Q9. Answer: B**  
*Explanation*: Rare populations, pilot studies, ethical constraints (minimizing burden on vulnerable groups), and resource limitations in SIDS contexts are all legitimate reasons for small samples. The chapter explicitly mentions "clinical studies of rare diseases" as one context where "small samples are the norm rather than the exception."

**Q10. Answer: B**  
*Explanation*: Test selection should consider: outcome type (continuous, ordinal, binary), distributional properties (normality, skewness), and research question. Small n is ONE consideration, but not the sole criterion. The chapter states: "The choice of method should match the research question, the type of outcome, and the sample size actually available."

:::

---

### Smoke Test

```{r}
#| label: smoke-test-mwu
# Re-run a simple Mann–Whitney test to verify code
library(rstatix)
set.seed(2025)
x <- c(7, 8, 6, 7, 9, 8)
y <- c(5, 6, 7, 5, 6, 5)
test_data <- tibble(value = c(x, y), group = rep(c("X", "Y"), each = 6))
wilcox_test(test_data, value ~ group)
```

---

## Chapter 2. Questions and Outcomes that Fit Small n

### Learning Objectives

By the end of this chapter, you will be able to:

**Conceptual Understanding**
- ✓ Distinguish between exploratory and confirmatory research aims
- ✓ Understand when to prioritize effect size estimation over hypothesis testing
- ✓ Recognize which research questions are realistic for small samples
- ✓ Explain the relationship between outcome measurement and sample size requirements

**Practical Skills**
- ✓ Formulate focused research questions appropriate for limited data
- ✓ Select outcome measures that yield interpretable results with small n
- ✓ Design studies that maximize information from modest samples
- ✓ Calculate minimum detectable effects for planned sample sizes

**Critical Evaluation**
- ✓ Assess whether complex multivariate questions are feasible with available data
- ✓ Evaluate trade-offs between breadth (many variables) and depth (focused questions)
- ✓ Critique overly ambitious research designs given sample constraints

**Application**
- ✓ Design pilot studies with clear, answerable questions
- ✓ Choose between continuous, ordinal, and binary outcome measures
- ✓ Justify research scope and question framing in proposals and manuscripts

### Framing Realistic Research Questions

Not all research questions are equally suited to small samples. Broad, multivariate questions (such as identifying dozens of predictors or testing complex mediation models) typically require large datasets. In contrast, focused questions about a single outcome or a few key comparisons can often be addressed with modest samples.

When planning a small-sample study, prioritise clarity and specificity. Instead of asking "What are all the factors that influence patient adherence?" ask "Does a brief reminder intervention improve adherence compared to standard care?" The latter question is binary, focused, and testable with a small randomised trial.

Similarly, consider whether the study is exploratory or confirmatory. Exploratory studies generate hypotheses, describe patterns, and refine measurement instruments. They do not require large samples, but results should be interpreted cautiously and replicated before drawing firm conclusions. Confirmatory studies test prespecified hypotheses and require sufficient statistical power. With small samples, power is limited, so confirmatory aims should be modest and well-justified.

### Choosing Appropriate Outcomes

The type of outcome variable influences which methods are feasible and how much information can be extracted from limited data. Binary outcomes (yes/no, success/failure) are common but carry less information per observation than continuous or ordinal measures. If your sample is small, consider whether a continuous or ordinal outcome might capture more variation and yield more precise inferences.

For example, rather than dichotomising patient improvement into "improved" versus "not improved", use a continuous measure of symptom severity or an ordinal scale with several levels. This preserves information and increases statistical efficiency. However, if the outcome is inherently binary (such as survival within 30 days), do not force it into a continuous form.

Count outcomes (number of adverse events, number of customer complaints) are also informative but may be sparse when samples are small. Exact Poisson tests and negative binomial models can handle low counts, but very sparse data (many zeros, few events) may require careful interpretation or resampling methods.

#### Outcome Selection Decision Guide

1. **Start with the construct of interest.** Can it be measured on a genuine numeric scale (e.g., duration, dosage, test score)?
  - **Yes** → Prefer a continuous measure. Report means/medians with confidence intervals; consider transformations if the scale is skewed.
  - **No/uncertain** → Proceed to step 2.
2. **Can respondents make ordered distinctions beyond "yes/no"?**
  - **Yes** → Use an ordinal scale with 4–7 categories. Analyse with rank-based or ordinal models; report medians or cumulative odds ratios.
  - **No** → Retain a binary outcome. Use exact or penalised methods and emphasise risk differences or odds ratios with wide intervals.
3. **Is the event a count that can exceed one per subject?**
  - **Yes** → Model counts directly (Poisson, negative binomial, or exact tests). Report event rates and their uncertainty.
4. **Document the rationale.** Explain why the chosen outcome scale is the most informative and feasible given participant burden, measurement error, and sample size.

### Effect Sizes and Estimation

In small-sample research, point estimates of effect sizes (differences in means, odds ratios, correlation coefficients) are often more useful than p-values alone. A small sample may lack power to detect a meaningful effect, but the estimated effect size and its confidence interval indicate the likely magnitude and precision of the effect.

When reporting results, emphasise effect sizes and uncertainty intervals. For example, "The median difference in satisfaction scores was 1.5 points (95% CI: 0.5 to 2.5)" is more informative than "The difference was statistically significant (p = 0.03)". Effect size estimates help readers judge practical importance and facilitate meta-analysis or future sample size planning.

### Example: Outcome Selection in a Pilot Study

Suppose you are evaluating a pilot training programme with 18 participants. You have two outcome options: (1) binary pass/fail on a final assessment, or (2) a continuous score (0–100) on the same assessment.

```{r}
#| label: pilot-outcome-simulation
library(tidyverse)

set.seed(2025)
n <- 18

# Simulate continuous scores
scores <- round(rnorm(n, mean = 68, sd = 12))
scores <- pmax(0, pmin(100, scores))  # clamp to [0, 100]

# Create binary outcome: pass if score >= 60
pass_fail <- ifelse(scores >= 60, "Pass", "Fail")

data_pilot <- tibble(
  participant = 1:n,
  score = scores,
  outcome = pass_fail
)

# Summary statistics for continuous outcome
summary(data_pilot$score)

# Frequency table for binary outcome
table(data_pilot$outcome)

# The continuous outcome carries more information
# We can estimate a mean and standard error:
mean_score <- mean(data_pilot$score)
se_score <- sd(data_pilot$score) / sqrt(n)
ci_lower <- mean_score - 1.96 * se_score
ci_upper <- mean_score + 1.96 * se_score

cat("Mean score:", round(mean_score, 1), "\n")
cat("95% CI: [", round(ci_lower, 1), ",", round(ci_upper, 1), "]\n", sep = "")
```

By retaining the continuous score, we obtain a precise estimate of average performance with a confidence interval. Had we dichotomised into pass/fail, we would only know that 14 out of 18 passed, which provides less information about the central tendency and spread of performance.

Interpretation: The continuous outcome allows us to estimate the mean score with reasonable precision. The confidence interval indicates the range of plausible population means. If the goal is to understand typical performance (not just pass rates), the continuous measure is more informative.

### Research Design Considerations

Small-sample studies benefit from tight experimental control. Paired or matched designs (before–after, crossover, matched-pair comparisons) reduce variability by comparing each unit to itself or a closely matched control. This within-unit comparison can yield precise inferences even when the number of units is small.

Stratification and blocking can also improve efficiency by accounting for known sources of variation. For example, if you are comparing two teaching methods in a small class, stratify by prior achievement level to reduce heterogeneity within each comparison.

Finally, consider sequential or adaptive designs if feasible. Rather than committing to a fixed sample size in advance, you might plan an interim analysis and decide whether to stop early (if results are clear) or continue (if uncertainty remains). Bayesian methods are well-suited to adaptive designs, as they naturally update beliefs as data accumulate.

### Designing Pilot Studies

Pilot studies serve specific purposes: assessing feasibility (recruitment rates, attrition, protocol adherence), refining measurement instruments, and estimating variability to inform future sample size calculations. With very small *n* (often 10–30 participants), focus on collecting process metrics and precision estimates rather than hypothesis testing. Report:

- **Primary feasibility outcomes** (e.g., proportion screened who consent, time to complete assessments).
- **Preliminary effect estimates** with wide confidence intervals, making clear that they are exploratory.
- **Adaptations for the main study**, especially where procedures proved onerous or data quality issues emerged.

Guidance: choose a pilot sample large enough to detect major logistical problems (often 12–20 per arm is sufficient), prespecify success criteria (such as acceptable recruitment rate), and plan in advance how you will decide whether to proceed to a full trial.

### Key Takeaways

- Frame research questions narrowly and realistically given the sample size constraints.
- Distinguish between exploratory (hypothesis-generating) and confirmatory (hypothesis-testing) aims.
- Prefer continuous or ordinal outcomes over binary outcomes when possible, to maximise information per observation.
- Report effect sizes and confidence intervals, not just p-values, to convey magnitude and precision.
- Use paired, matched, or stratified designs to reduce variability and improve efficiency.
- Consider adaptive or sequential designs if ethically and practically feasible.

---

### Self-Assessment Quiz

Test your understanding of the key concepts from Chapter 2. Answers and explanations are provided at the end.

::: {.callout-note icon=false}
## Questions

**Q1.** Which research question is better suited to small samples?

A. "What is the relationship between 20 personality traits and job performance?"  
B. "Does a brief mindfulness intervention reduce test anxiety compared to control?"  
C. "Can we predict customer churn using all available behavioral data?"  
D. "How do socioeconomic factors interact to predict health outcomes?"

---

**Q2.** An exploratory study (n=20) finds that meditation reduces anxiety (p=0.04, d=0.7). How should this be framed?

A. "Meditation is proven effective"  
B. "Preliminary evidence suggests meditation may reduce anxiety; replication needed"  
C. "No conclusions can be drawn from n=20"  
D. "The effect is definitely due to chance"

---

**Q3.** A researcher dichotomizes a continuous outcome (0-100 scale) into "high" (≥70) vs "low" (<70). With n=25, what is the consequence?

A. Power increases because binary outcomes are simpler  
B. Power decreases because information is discarded  
C. No effect on statistical power  
D. Analysis becomes impossible

---

**Q4.** A study aims to detect a "small" effect (d=0.2) with 80% power. Approximately how many participants per group are needed?

A. n=20 per group  
B. n=50 per group  
C. n=200 per group  
D. n=500 per group

---

**Q5.** Which statement about pilot studies is CORRECT?

A. Pilot studies should always test hypotheses  
B. Pilot studies assess feasibility and refine procedures  
C. Pilot studies require the same sample size as main studies  
D. Pilot studies never provide useful effect size estimates

---

**Q6.** A researcher plans a study with n=15 per group but calculates they need n=50 per group for 80% power. What should they do?

A. Proceed with n=15 and interpret p-values cautiously  
B. Reframe the study as exploratory/pilot  
C. Report minimum detectable effect (MDE) given n=15  
D. All of the above

---

**Q7.** Which outcome is LEAST appropriate for n=20?

A. Binary outcome (success/failure)  
B. Ordinal outcome (1-7 Likert scale)  
C. Continuous outcome (0-100 scale)  
D. 50-item questionnaire with subscale factor analysis

---

**Q8.** A study comparing two teaching methods (n=12 per class) finds no significant difference (p=0.18, d=0.45). The conclusion should be:

A. "The two methods are equally effective"  
B. "The study found no evidence of a difference, but was underpowered to detect medium effects"  
C. "Teaching method has no effect on learning"  
D. "The null hypothesis is confirmed"

---

**Q9.** When choosing between a paired and independent-groups design with small samples, which is generally preferable?

A. Always use independent groups—pairing is only for large samples  
B. Paired designs reduce within-subject variability and increase power  
C. The choice makes no difference statistically  
D. Paired designs require larger samples than independent designs

---

**Q10.** A pilot study with n=18 yields a mean difference of 5 points (95% CI: [0.2, 9.8]). What is the appropriate next step?

A. Conclude the intervention is effective and implement widely  
B. Use this estimate to plan a fully-powered confirmatory study  
C. Abandon the research because the sample was too small  
D. Report only the p-value and ignore the confidence interval

:::

::: {.callout-tip icon=false collapse="true"}
## Answers and Explanations

**Q1. Answer: B**  
*Explanation*: Focused, binary comparisons with a single primary outcome are feasible with small samples. Multivariate questions (A, C, D) require large samples to estimate many parameters reliably. The chapter emphasizes: "focused questions about a single outcome or a few key comparisons can often be addressed with modest samples."

**Q2. Answer: B**  
*Explanation*: Exploratory studies with small samples generate hypotheses but require replication. Results should be framed as preliminary, with acknowledgment of limited power and potential for Type I error. As stated in the chapter: "Exploratory studies generate hypotheses, describe patterns... They do not require large samples, but results should be interpreted cautiously and replicated before drawing firm conclusions."

**Q3. Answer: B**  
*Explanation*: Dichotomizing continuous variables discards information about the magnitude of differences, reduces statistical power, and can create spurious findings at arbitrary cut-points. The chapter clearly states: "rather than dichotomising patient improvement into 'improved' versus 'not improved', use a continuous measure... This preserves information and increases statistical efficiency."

**Q4. Answer: C**  
*Explanation*: Detecting small effects requires large samples. For d=0.2 with 80% power and α=0.05 (two-tailed), approximately n=393 per group is needed. With small samples, only large effects (d≥0.8) can be reliably detected. This aligns with the power curve from Chapter 1 showing that detecting d=0.3 effects is beyond reach of small samples.

**Q5. Answer: B**  
*Explanation*: Pilot studies (typically n=10-30) assess feasibility (recruitment rates, protocol adherence, measurement properties), refine procedures, and provide preliminary effect size estimates for sample size planning—but are not powered for definitive hypothesis testing. The chapter's "Designing Pilot Studies" section explicitly states: "focus on collecting process metrics and precision estimates rather than hypothesis testing."

**Q6. Answer: D**  
*Explanation*: When desired sample size is infeasible: (1) proceed but interpret with appropriate caution, (2) frame as exploratory, (3) report what effect sizes CAN be detected (MDE), (4) emphasize effect size estimation over hypothesis testing. This integrates guidance from both the "Framing Realistic Research Questions" and "Effect Sizes and Estimation" sections.

**Q7. Answer: D**  
*Explanation*: Factor analysis requires n≥100-200 (ideally 5-10 observations per item). With n=20 and 50 items, factor analysis would be completely unreliable. This reflects general principles about matching analysis complexity to sample size discussed throughout the chapter.

**Q8. Answer: B**  
*Explanation*: Non-significance with small samples indicates insufficient evidence, not proof of no effect. A medium effect (d=0.45) is plausible given the CI, but the study lacked power to detect it definitively. The chapter emphasizes: "A small sample may lack power to detect a meaningful effect, but the estimated effect size and its confidence interval indicate the likely magnitude and precision of the effect."

**Q9. Answer: B**  
*Explanation*: Paired designs reduce within-subject variability and increase power. The chapter's "Research Design Considerations" section states: "Paired or matched designs (before–after, crossover, matched-pair comparisons) reduce variability by comparing each unit to itself or a closely matched control. This within-unit comparison can yield precise inferences even when the number of units is small."

**Q10. Answer: B**  
*Explanation*: Use this estimate to plan a fully-powered confirmatory study. Pilot studies provide preliminary effect estimates and variability information needed for sample size planning. The "Designing Pilot Studies" section recommends reporting "Preliminary effect estimates with wide confidence intervals, making clear that they are exploratory" and using pilots to "estimating variability to inform future sample size calculations."

:::

---

### Smoke Test

```{r}
#| label: smoke-test-outcome
# Re-run outcome comparison example
set.seed(2025)
scores_test <- round(rnorm(10, mean = 70, sd = 10))
mean(scores_test)
sd(scores_test)
```

---

## Summary of Part A

In Part A, we established that small-sample research is both common and legitimate. We reviewed why large-sample approximations can fail with limited data and introduced exact, resampling, and robust methods as alternatives. We also discussed how to formulate research questions and select outcomes that are realistic and informative given small sample sizes. The next part will address data collection and preparation strategies tailored to small studies.
