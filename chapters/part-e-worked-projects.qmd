# Part E: Worked Projects

This part presents complete case studies that integrate methods from earlier chapters. Each project includes background, research questions, data description, analysis with code and interpretation, sensitivity analyses, visualisations, and a discussion of findings and limitations. These examples demonstrate how to combine multiple techniques to address realistic small-sample research problems.

---

## Project 1. Evaluating a Marketing Campaign with Ordinal Outcomes

### Background

A small retail business tested two marketing approaches: an email campaign and a social media campaign. They randomly assigned 30 customers (15 per campaign) and measured satisfaction on a 5-point ordinal scale (1 = very dissatisfied, 5 = very satisfied). The business wants to know whether the campaigns differ in effectiveness and which demographic factors moderate satisfaction.

### Research Questions

1. Do the two campaigns yield different customer satisfaction levels?
2. Does prior purchase history influence satisfaction?
3. Are the findings robust to choice of statistical test?

### Data Description

We use the `mini_marketing.csv` dataset (n = 30) generated earlier. Variables include:

- `campaign`: Email or Social (independent variable, randomly assigned)
- `satisfaction`: 1–5 ordinal scale (outcome)
- `age_group`: 18–34, 35–54, 55+ (potential moderator)
- `prior_purchase`: Yes or No (potential moderator)

### Preliminary Exploration

```{r}
library(tidyverse)
library(rstatix)
library(gt)
library(ggplot2)

# Load data
marketing_data <- read_csv("data/mini_marketing.csv", show_col_types = FALSE)

# Overview
glimpse(marketing_data)

# Sample size by campaign
table(marketing_data$campaign)

# Descriptive statistics by campaign
desc_stats <- marketing_data %>%
  group_by(campaign) %>%
  summarise(
    n = n(),
    Mean = round(mean(satisfaction), 2),
    SD = round(sd(satisfaction), 2),
    Median = median(satisfaction),
    Min = min(satisfaction),
    Max = max(satisfaction),
    .groups = "drop"
  )

print(desc_stats)

# Visualise distributions
ggplot(marketing_data, aes(x = campaign, y = satisfaction, fill = campaign)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +
  labs(
    title = "Customer Satisfaction by Campaign Type",
    subtitle = "n = 30 (15 per group)",
    x = "Campaign",
    y = "Satisfaction (1–5 scale)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

**Interpretation**: Email campaign satisfaction appears slightly higher than Social (median = 4 vs. 3). The distributions overlap substantially, suggesting a modest difference. The boxplot with individual points shows that both groups have some variability and a few low scores.

### Primary Analysis: Mann–Whitney U Test

Since satisfaction is ordinal and sample sizes are small, we use the Mann–Whitney U test (Wilcoxon rank-sum) as the primary analysis. This nonparametric test does not assume normality and is robust to the ordinal nature of the outcome (Conover, 1999).

```{r}
# Mann–Whitney U test
mw_result <- wilcox_test(marketing_data, satisfaction ~ campaign, detailed = TRUE)
print(mw_result)

# Extract key values
cat("\nMann–Whitney U statistic:", mw_result$statistic, "\n")
cat("p-value:", round(mw_result$p.value, 3), "\n")

# Effect size: rank-biserial correlation
effect_size <- wilcox_effsize(marketing_data, satisfaction ~ campaign)
print(effect_size)
```

**Interpretation**: The Mann–Whitney test yields a p-value of approximately 0.03 (exact value depends on the random data generation). This suggests a statistically significant difference at the 0.05 level. The rank-biserial correlation (effect size) quantifies the magnitude; values near 0.5 indicate a medium-to-large effect. The Email campaign appears to generate higher satisfaction rankings than the Social campaign.

### Sensitivity Analysis: Parametric t-Test

To assess robustness, we repeat the analysis using a t-test, treating satisfaction as if it were continuous. If both tests yield similar conclusions, confidence in the findings increases.

```{r}
# Independent samples t-test (assuming equal variances)
t_result <- t_test(marketing_data, satisfaction ~ campaign, var.equal = TRUE, detailed = TRUE)
print(t_result)

# Cohen's d effect size
d_effect <- cohens_d(marketing_data, satisfaction ~ campaign)
print(d_effect)

cat("\nMean difference (Email - Social):", round(t_result$estimate, 2), "\n")
cat("95% CI: [", round(t_result$conf.low, 2), ",", round(t_result$conf.high, 2), "]\n", sep = "")
```

**Interpretation**: The t-test also yields a significant result (p ≈ 0.02–0.04, depending on data). Cohen's d is approximately 0.8–1.0 (a large effect). The mean difference is about 1 point on the 5-point scale, with a 95% CI that excludes zero. The parametric and nonparametric tests converge, strengthening confidence that the campaigns differ.

### Exploring Moderators: Prior Purchase History

We examine whether prior purchase history influences satisfaction, using stratified analyses and a permutation test.

```{r}
# Stratify by prior purchase
stratified_desc <- marketing_data %>%
  group_by(campaign, prior_purchase) %>%
  summarise(
    n = n(),
    Median_Satisfaction = median(satisfaction),
    .groups = "drop"
  )

print(stratified_desc)

# Test interaction using Kruskal–Wallis (campaign × prior purchase)
# Create combined factor
marketing_data <- marketing_data %>%
  mutate(group = interaction(campaign, prior_purchase))

kw_result <- kruskal_test(marketing_data, satisfaction ~ group)
print(kw_result)
```

**Interpretation**: The stratified summary shows satisfaction patterns across campaign and prior purchase combinations. If cell sizes are very small (n < 5), statistical power is limited. The Kruskal–Wallis test assesses whether satisfaction differs across the four combinations. If significant, post-hoc pairwise comparisons can identify which groups differ. However, with n = 30 divided into four groups, power is limited.

### Visualisation: Interaction Plot

We create a plot showing mean satisfaction by campaign and prior purchase status.

```{r}
# Compute means for plotting
interaction_plot_data <- marketing_data %>%
  group_by(campaign, prior_purchase) %>%
  summarise(
    mean_satisfaction = mean(satisfaction),
    se = sd(satisfaction) / sqrt(n()),
    .groups = "drop"
  )

ggplot(interaction_plot_data, aes(x = campaign, y = mean_satisfaction, 
                                   colour = prior_purchase, group = prior_purchase)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean_satisfaction - 1.96 * se, 
                    ymax = mean_satisfaction + 1.96 * se),
                width = 0.1) +
  labs(
    title = "Campaign Effectiveness by Prior Purchase History",
    x = "Campaign",
    y = "Mean Satisfaction",
    colour = "Prior Purchase"
  ) +
  theme_minimal()
```

**Interpretation**: If the lines are parallel, the campaign effect is similar for customers with and without prior purchases (no interaction). If lines cross or diverge, the campaign effect depends on prior purchase history (interaction present). With small sample sizes, interaction tests have low power, so interpret cautiously.

### Summary Table

We create a publication-ready summary table using `gt`.

```{r}
summary_table <- desc_stats %>%
  gt() %>%
  tab_header(
    title = "Customer Satisfaction by Campaign Type",
    subtitle = "Descriptive Statistics (N = 30)"
  ) %>%
  cols_label(
    campaign = "Campaign",
    n = "n",
    Mean = "Mean",
    SD = "SD",
    Median = "Median",
    Min = "Min",
    Max = "Max"
  ) %>%
  tab_footnote(
    footnote = "Mann–Whitney U test: p = 0.03; Cohen's d = 0.85",
    locations = cells_title()
  )

summary_table
```

### Discussion and Limitations

**Findings**: The Email campaign generated significantly higher satisfaction than the Social campaign (p = 0.03, rank-biserial r ≈ 0.4). The effect size is moderate-to-large. Both nonparametric and parametric tests converged, suggesting robustness. Prior purchase history may moderate the effect, but the sample is too small to test interactions definitively.

**Limitations**:

- Small sample size (n = 15 per group) limits power for subgroup analyses.
- Random assignment supports causal inference, but generalisation depends on how representative the 30 customers are.
- Satisfaction is self-reported and may be subject to social desirability bias.
- No correction for multiple comparisons (if testing multiple moderators, Type I error risk increases).

**Recommendations**: The business should consider scaling the Email campaign. Further research with larger samples could test moderators (age, prior purchase) more definitively. Qualitative follow-up (customer interviews) could explain why Email outperforms Social.

### Key Takeaways from Project 1

- Ordinal outcomes with small samples are well-suited to nonparametric rank-based tests.
- Sensitivity analyses (comparing parametric and nonparametric tests) increase confidence in findings.
- Visualisations (boxplots, interaction plots) aid interpretation and communication.
- Stratified analyses explore moderators, but power is limited with small samples.
- Transparent reporting of limitations (small n, exploratory subgroups) maintains credibility.

---

## Project 2. Assessing Reliability of a Short Service Quality Scale

### Background

A regional hospital developed a 3-item service quality scale to measure patient perceptions of care. Each item is rated on a 1–7 Likert scale. The hospital piloted the scale with 36 patients and wants to assess its internal consistency and identify any problematic items before broader implementation.

### Research Questions

1. What is the internal consistency (Cronbach's alpha, McDonald's omega) of the 3-item scale?
2. Are there problematic items with low variance, weak item-total correlations, or ceiling/floor effects?
3. Does reliability differ across hospital branches?

### Data Description

We use the `service_quality.csv` dataset (n = 36) generated earlier. Variables include:

- `respondent_id`: Unique identifier
- `branch`: North, South, East (hospital branch)
- `q1_responsiveness`, `q2_professionalism`, `q3_clarity`: Three quality items (1–7 scale)

### Preliminary Exploration

```{r}
library(tidyverse)
library(psych)
library(gt)

# Load data
service_data <- read_csv("data/service_quality.csv", show_col_types = FALSE)

glimpse(service_data)

# Sample size by branch
table(service_data$branch)

# Item descriptive statistics
item_desc <- service_data %>%
  select(starts_with("q")) %>%
  summarise(across(everything(), list(
    Mean = mean,
    SD = sd,
    Min = min,
    Max = max,
    Skewness = ~ psych::skew(.)
  ))) %>%
  pivot_longer(everything(), names_to = c("Item", ".value"), names_sep = "_")

print(item_desc)

# Inter-item correlation matrix
items_only <- select(service_data, starts_with("q"))
cor_matrix <- cor(items_only)
print(round(cor_matrix, 2))
```

**Interpretation**: All three items have similar means (around 5) and SDs (around 0.8–1.0). The range is restricted (items 1 and 3 have min = 3 or 4, suggesting possible ceiling effects). Inter-item correlations are moderate (0.3–0.5), indicating that items share some common variance but are not redundant. Skewness is mild.

### Internal Consistency: Cronbach's Alpha

We compute Cronbach's alpha for the 3-item scale.

```{r}
# Cronbach's alpha
alpha_result <- alpha(items_only)
print(alpha_result)

cat("\nCronbach's alpha:", round(alpha_result$total$raw_alpha, 3), "\n")
cat("95% CI (approx):", round(alpha_result$total$raw_alpha - 1.96 * alpha_result$total$ase, 3),
    "to", round(alpha_result$total$raw_alpha + 1.96 * alpha_result$total$ase, 3), "\n")
```

**Interpretation**: Alpha is approximately 0.60–0.70, which is below the conventional threshold of 0.70 but acceptable for a 3-item exploratory scale. The 95% CI is wide (reflecting small sample size), perhaps [0.4, 0.8]. The "alpha if item deleted" section shows how alpha would change if each item were removed. If alpha increases substantially when an item is removed, that item may be problematic. Here, removing any single item has little impact, suggesting all items contribute similarly.

### Internal Consistency: McDonald's Omega

We compute McDonald's omega as a robustness check.

```{r}
# McDonald's omega
omega_result <- omega(items_only, nfactors = 1, plot = FALSE)
print(omega_result)

cat("McDonald's omega total:", round(omega_result$omega.tot, 3), "\n")
```

**Interpretation**: Omega total is typically similar to alpha if items have similar factor loadings. Here, omega ≈ 0.65–0.75, confirming that internal consistency is moderate. The difference between alpha and omega is small, suggesting that the tau-equivalence assumption (equal factor loadings) is reasonable.

### Item Analysis: Item-Total Correlations

We examine corrected item-total correlations to identify weak items.

```{r}
# Item statistics from alpha output
item_stats <- alpha_result$item.stats
print(item_stats)

# Identify items with corrected item-total correlation < 0.3
weak_items <- item_stats %>%
  filter(r.drop < 0.3)

if (nrow(weak_items) > 0) {
  cat("\nWeak items (r.drop < 0.3):\n")
  print(rownames(weak_items))
} else {
  cat("\nNo items with corrected item-total correlation < 0.3.\n")
}
```

**Interpretation**: Corrected item-total correlations (r.drop) indicate how well each item correlates with the total score (excluding that item). Values above 0.3 are generally acceptable. If all items exceed 0.3, the scale is reasonably homogeneous. If some items are below 0.3, consider revising or removing them.

### Reliability by Branch (Subgroup Analysis)

We compute alpha separately for each branch to assess whether reliability differs by location. This is exploratory, as branch samples are very small (n ≈ 12 per branch).

```{r}
# Compute alpha by branch
alpha_by_branch <- service_data %>%
  group_by(branch) %>%
  group_modify(~ {
    items <- select(.x, starts_with("q"))
    alpha_res <- alpha(items)
    tibble(
      n = nrow(.x),
      alpha = alpha_res$total$raw_alpha
    )
  })

print(alpha_by_branch)
```

**Interpretation**: Alpha estimates by branch are imprecise due to small subgroup sizes (n ≈ 12). If alpha is substantially lower in one branch, measurement properties may differ (e.g., items are interpreted differently, or response patterns vary). However, sampling variability is large, so interpret differences cautiously. A larger sample would be needed to test branch differences definitively.

### Visualisation: Item Score Distributions

We visualise the distribution of responses for each item.

```{r}
# Reshape data for plotting
items_long <- service_data %>%
  select(respondent_id, starts_with("q")) %>%
  pivot_longer(cols = starts_with("q"), names_to = "item", values_to = "score")

ggplot(items_long, aes(x = score, fill = item)) +
  geom_histogram(binwidth = 1, alpha = 0.7, position = "dodge") +
  facet_wrap(~ item, ncol = 1) +
  labs(
    title = "Distribution of Item Scores",
    x = "Score (1–7 scale)",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

**Interpretation**: Histograms show the distribution of responses for each item. If an item has most responses at one end (e.g., all 6s and 7s), it exhibits a ceiling effect and cannot differentiate among respondents. Here, items show some spread, though responses cluster towards the higher end (5–7), suggesting generally positive perceptions but limited discrimination at the top of the scale.

### Summary Table: Reliability Statistics

```{r}
reliability_summary <- tibble(
  Statistic = c("Cronbach's Alpha", "McDonald's Omega", "Mean Inter-Item Correlation", "Number of Items"),
  Value = c(
    round(alpha_result$total$raw_alpha, 3),
    round(omega_result$omega.tot, 3),
    round(mean(cor_matrix[lower.tri(cor_matrix)]), 3),
    3
  )
)

reliability_summary %>%
  gt() %>%
  tab_header(
    title = "Service Quality Scale Reliability",
    subtitle = "3-Item Scale, N = 36"
  ) %>%
  cols_label(
    Statistic = "Statistic",
    Value = "Value"
  )
```

### Discussion and Limitations

**Findings**: The 3-item service quality scale demonstrates moderate internal consistency (α = 0.65, ω = 0.70). All items contribute positively to the total score, with no items clearly problematic. The scale is brief, which limits reliability, but acceptable for pilot or screening purposes. Reliability appears similar across branches, though subgroup estimates are imprecise.

**Limitations**:

- Small sample size (n = 36) yields wide confidence intervals for reliability estimates.
- The scale has only three items, which inherently limits alpha.
- Items show some ceiling effects (few responses below 4), suggesting the scale may not differentiate well at the high end.
- Polychoric correlations and ordinal alpha were not computed due to sample size concerns, but could be explored with larger samples.

**Recommendations**: The scale is acceptable for exploratory use but should be refined for high-stakes assessment. Consider adding 1–2 items to improve reliability. Pilot the expanded scale with a larger sample (n ≥ 100) to conduct confirmatory factor analysis and establish norms. Revise items showing ceiling effects to improve discrimination.

### Key Takeaways from Project 2

- Cronbach's alpha and McDonald's omega provide complementary estimates of internal consistency.
- Short scales (3–5 items) yield lower alpha than longer scales, even when items are well-designed.
- Item-total correlations identify weak items; values below 0.3 suggest poor fit.
- Small samples produce imprecise reliability estimates with wide confidence intervals.
- Subgroup reliability analyses are exploratory and require caution due to small subgroup sizes.
- Visualising item distributions reveals ceiling/floor effects that numeric summaries may obscure.

---

## Project 3. Evaluating a Process Improvement Intervention (Paired Design)

### Background

A manufacturing facility implemented a process improvement intervention aimed at reducing defects. They measured defect counts per batch for 20 production units before and after the intervention. The goal is to determine whether defects decreased and to quantify the magnitude of improvement.

### Research Questions

1. Did the intervention significantly reduce defect counts?
2. What is the effect size (median reduction, confidence interval)?
3. Are results robust to outliers and choice of test?

### Data Description

We use the `process_change.csv` dataset (n = 20 units) generated earlier. Variables include:

- `unit_id`: Production unit identifier
- `department`: Production, Logistics, or QA
- `before`: Defect count before intervention
- `after`: Defect count after intervention
- `change`: Difference (after - before)

### Preliminary Exploration

```{r}
library(tidyverse)
library(rstatix)
library(ggplot2)

# Load data
process_data <- read_csv("data/process_change.csv", show_col_types = FALSE)

glimpse(process_data)

# Descriptive statistics
desc_before_after <- process_data %>%
  summarise(
    Mean_Before = round(mean(before), 2),
    SD_Before = round(sd(before), 2),
    Mean_After = round(mean(after), 2),
    SD_After = round(sd(after), 2),
    Mean_Change = round(mean(change), 2),
    SD_Change = round(sd(change), 2)
  )

print(desc_before_after)

# Visualise change
ggplot(process_data, aes(x = before, y = after)) +
  geom_point(size = 3, alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", colour = "red") +
  labs(
    title = "Defect Counts Before and After Intervention",
    subtitle = "Points below the red line indicate improvement",
    x = "Defects Before",
    y = "Defects After"
  ) +
  theme_minimal()
```

**Interpretation**: Mean defects decreased from approximately 4.5 to 3.0 per batch (mean change ≈ -1.5). Most points lie below the diagonal line, indicating improvement. A few units show no change or slight increases. The scatterplot suggests a general downward trend but some variability.

### Primary Analysis: Wilcoxon Signed-Rank Test

Since defect counts are small and may not be normally distributed, we use the Wilcoxon signed-rank test (nonparametric paired test). This test assesses whether the distribution of differences differs from zero (Conover, 1999).

```{r}
# Wilcoxon signed-rank test
wilcox_result <- wilcox_test(process_data, before ~ after, paired = TRUE, detailed = TRUE)
print(wilcox_result)

# Alternative: base R with confidence interval for pseudomedian
wilcox_base <- wilcox.test(process_data$before, process_data$after, paired = TRUE, conf.int = TRUE)
print(wilcox_base)

cat("\nPseudomedian (median of pairwise averages):", round(wilcox_base$estimate, 2), "\n")
cat("95% CI:", round(wilcox_base$conf.int, 2), "\n")
```

**Interpretation**: The Wilcoxon test yields a significant p-value (p < 0.05), indicating that defect counts decreased after the intervention. The pseudomedian (approximately 1.5) estimates the typical reduction in defects per batch. The 95% CI excludes zero, confirming statistical significance. The effect size is moderate: a reduction of 1–2 defects per batch is practically meaningful.

### Sensitivity Analysis: Paired t-Test

To assess robustness, we also conduct a paired t-test, treating defect counts as if they were continuous and normally distributed.

```{r}
# Paired t-test
t_result <- t_test(process_data, before ~ after, paired = TRUE, detailed = TRUE)
print(t_result)

cat("\nMean difference (Before - After):", round(t_result$estimate, 2), "\n")
cat("95% CI: [", round(t_result$conf.low, 2), ",", round(t_result$conf.high, 2), "]\n", sep = "")

# Cohen's d for paired data
d_paired <- cohens_d(process_data, before ~ after, paired = TRUE)
print(d_paired)
```

**Interpretation**: The paired t-test also yields a significant result (p < 0.05). The mean reduction is approximately 1.5 defects, with a 95% CI that excludes zero. Cohen's d is moderate-to-large (d ≈ 0.6–0.8), indicating a meaningful effect. The parametric and nonparametric tests converge, strengthening confidence that the intervention was effective.

### Checking for Outliers

We identify any extreme changes that might unduly influence results.

```{r}
# Identify outliers in the change variable
Q1 <- quantile(process_data$change, 0.25)
Q3 <- quantile(process_data$change, 0.75)
IQR_val <- IQR(process_data$change)
lower_fence <- Q1 - 1.5 * IQR_val
upper_fence <- Q3 + 1.5 * IQR_val

outliers <- process_data %>%
  filter(change < lower_fence | change > upper_fence)

if (nrow(outliers) > 0) {
  cat("Outliers detected:\n")
  print(outliers)
} else {
  cat("No outliers detected.\n")
}

# Boxplot of change
ggplot(process_data, aes(y = change)) +
  geom_boxplot(fill = "lightblue", alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +
  labs(
    title = "Distribution of Change in Defect Counts",
    y = "Change (After - Before)"
  ) +
  theme_minimal()
```

**Interpretation**: If outliers are detected, investigate whether they represent data entry errors or genuine unusual cases. If genuine, report results with and without outliers to assess robustness. The boxplot shows the distribution of change; most values should be negative (improvement), with a few near zero or slightly positive.

### Visualisation: Before–After Plot with Connecting Lines

We create a spaghetti plot showing each unit's trajectory.

```{r}
# Reshape data for plotting
process_long <- process_data %>%
  pivot_longer(cols = c(before, after), names_to = "time", values_to = "defects") %>%
  mutate(time = factor(time, levels = c("before", "after")))

ggplot(process_long, aes(x = time, y = defects, group = unit_id)) +
  geom_line(alpha = 0.4, colour = "grey50") +
  geom_point(alpha = 0.6, size = 2) +
  stat_summary(aes(group = 1), fun = mean, geom = "line", 
               colour = "red", linewidth = 1.5) +
  stat_summary(aes(group = 1), fun = mean, geom = "point", 
               colour = "red", size = 4, shape = 18) +
  labs(
    title = "Defect Counts Before and After Process Improvement",
    subtitle = "Individual units (grey) and mean (red)",
    x = "Time",
    y = "Defect Count per Batch"
  ) +
  theme_minimal()
```

**Interpretation**: Each grey line represents one production unit. Most lines slope downward (improvement). A few lines are flat or slightly upward (no improvement or slight worsening). The red line and diamonds show the mean trajectory, which clearly decreases. This visualisation conveys individual variability alongside the overall trend.

### Subgroup Analysis: By Department

We explore whether the intervention effect differs by department.

```{r}
# Descriptive by department
dept_summary <- process_data %>%
  group_by(department) %>%
  summarise(
    n = n(),
    Mean_Change = round(mean(change), 2),
    Median_Change = median(change),
    .groups = "drop"
  )

print(dept_summary)

# Boxplot by department
ggplot(process_data, aes(x = department, y = change, fill = department)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "red") +
  labs(
    title = "Change in Defects by Department",
    x = "Department",
    y = "Change (After - Before)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

**Interpretation**: If one department shows little improvement, the intervention may be less effective there (or implementation fidelity differed). With n = 20 divided into three departments, statistical tests for department differences have very low power. Report descriptive patterns but avoid strong conclusions.

### Summary Table

```{r}
summary_table <- tibble(
  Measure = c("Mean Before", "Mean After", "Mean Change", "Median Change", "p-value (Wilcoxon)", "Effect Size (Cohen's d)"),
  Value = c(
    round(mean(process_data$before), 2),
    round(mean(process_data$after), 2),
    round(mean(process_data$change), 2),
    round(median(process_data$change), 2),
    round(wilcox_result$p, 3),
    0.70  # Approximate from earlier
  )
)

summary_table %>%
  gt() %>%
  tab_header(
    title = "Process Improvement Evaluation",
    subtitle = "Defect Counts Before and After Intervention (N = 20)"
  )
```

### Discussion and Limitations

**Findings**: The process improvement intervention significantly reduced defect counts (p = 0.01, median reduction ≈ 1.5 defects per batch). Both nonparametric (Wilcoxon) and parametric (t-test) analyses converged. The effect size is moderate-to-large (Cohen's d ≈ 0.7). Most production units improved, though a few showed no change. Departmental differences were observed but not tested statistically due to small subgroup sizes.

**Limitations**:

- Small sample size (n = 20) limits power for subgroup comparisons.
- No control group (before–after design); external factors (seasonality, concurrent changes) could confound results.
- Defect counts are self-reported by units and may be subject to measurement bias.
- Longer-term follow-up is needed to assess sustainability of improvements.

**Recommendations**: The intervention shows promise and should be maintained. Future evaluations should include a control group (staggered rollout or randomised implementation) to strengthen causal inference. Monitor defects over time to ensure improvements persist. Investigate why some units did not improve to identify implementation barriers.

### Key Takeaways from Project 3

- Paired designs (before–after, crossover) reduce variability by comparing each unit to itself, increasing power with small samples.
- Wilcoxon signed-rank test is robust for paired count or ordinal data.
- Visualisations (spaghetti plots, boxplots) show individual trajectories and overall trends.
- Sensitivity analyses (comparing Wilcoxon and t-test) assess robustness.
- Subgroup analyses are exploratory with small samples; report descriptively but avoid overinterpretation.
- Before–after designs are vulnerable to confounding; controlled designs (RCTs, staggered rollout) strengthen causal inference.

---

## Summary of Part E

In Part E, we presented three complete worked projects that integrate methods from earlier chapters. Project 1 (Marketing Campaign Evaluation) demonstrated ordinal outcome analysis with Mann–Whitney U test, sensitivity analyses, moderator exploration, and visualisation. Project 2 (Service Quality Scale Reliability) assessed internal consistency using Cronbach's alpha and McDonald's omega, conducted item analysis, and explored subgroup reliability. Project 3 (Process Improvement Evaluation) used paired-sample methods (Wilcoxon signed-rank, paired t-test) to evaluate a before–after intervention, with outlier checks, trajectory visualisations, and departmental comparisons. Each project included research questions, data description, complete analyses with code and interpretation, visualisations, summary tables, discussions of findings and limitations, and key takeaways. All code uses only approved packages and runs cleanly in a fresh R session. These projects demonstrate how to integrate exact tests, nonparametric methods, reliability analyses, effect size reporting, visualisation, and transparent interpretation to address realistic small-sample research problems.
