# Part C: Analysis Methods

This part presents the core toolkit for small-sample quantitative analysis. We cover exact and resampling tests, nonparametric rank-based methods, penalised and Bayesian regression, reliability analysis for short scales, multi-criteria decision-making (MCDM) techniques, and methods for sparse counts.

---

## Chapter 3. Exact Tests and Resampling Methods

### Learning Objectives

By the end of this chapter, you will be able to conduct Fisher's exact test and Barnard's exact test for 2×2 contingency tables, apply exact binomial and Poisson tests for count data, perform permutation tests for comparing groups, and construct bootstrap confidence intervals for effect sizes. You will understand when exact and resampling methods are preferable to asymptotic approximations.

### When to Use Exact and Resampling Methods

Exact tests compute p-values directly from the probability distribution of the test statistic under the null hypothesis, without relying on large-sample approximations. They are particularly useful when sample sizes are small, when distributional assumptions (such as normality) are questionable, or when the outcome is discrete (binary, count, or ordinal).

Resampling methods (permutation tests and bootstrap) use the observed data to approximate the sampling distribution of a statistic. Permutation tests shuffle group labels or treatment assignments to generate a reference distribution under the null hypothesis. Bootstrap resampling draws repeated samples (with replacement) from the observed data to estimate the variability of an estimator and construct confidence intervals.

Both approaches are computationally intensive but feasible with modern computing. They often provide more accurate inferences than classical parametric tests when samples are small or assumptions are violated.

### Fisher's Exact Test for 2×2 Tables

Fisher's exact test is the standard method for testing independence in a 2×2 contingency table when cell counts are small. It conditions on the row and column totals and computes the exact probability of observing the data (or more extreme) under the null hypothesis of no association.

The test is conservative (tends to yield p-values that are too large) in some settings, but remains widely used because it guarantees correct Type I error control. It is most appropriate for fixed-margin designs (such as retrospective case-control studies) but is often applied more broadly.

**Assumptions**: None regarding distributional form. Observations must be independent. The test conditions on marginal totals.

**When to use**: Small cell counts (any cell < 5), binary outcomes crossed with binary exposures, exact inference required.

### Example: Fisher's Exact Test

Suppose we are evaluating a new training intervention. Of 10 employees who received training, 8 met their performance target; of 10 who did not receive training, 3 met the target. We test whether training is associated with meeting the target.

```{r}
#| label: part-c-chunk-01
library(tidyverse)

# Create 2x2 table: rows = training (Yes, No), columns = target met (Yes, No)
training_yes <- c(8, 2)  # 8 met target, 2 did not
training_no <- c(3, 7)   # 3 met target, 7 did not

table_data <- matrix(c(8, 3, 2, 7), nrow = 2, byrow = TRUE,
                     dimnames = list(Training = c("Yes", "No"),
                                     Target = c("Met", "Not Met")))
print(table_data)

# Fisher's exact test
fisher_result <- fisher.test(table_data)
print(fisher_result)
```

Interpretation: The p-value indicates the probability of observing this degree of association (or stronger) if training had no effect. A small p-value (typically p < 0.05) suggests that training is associated with meeting the target. The odds ratio quantifies the strength of association: an odds ratio greater than 1 indicates higher odds of meeting the target among trained employees. The confidence interval for the odds ratio provides a range of plausible values.

### Barnard's Exact Test

Barnard's exact test is an alternative to Fisher's test that does not condition on both margins. It tends to have higher power than Fisher's test, particularly when the null hypothesis is false. However, it is computationally more demanding and less commonly implemented.

When both tests are available, Barnard's test may be preferred for exploratory analyses where power is a concern. For confirmatory analyses, Fisher's test remains the conventional choice.

```{r}
#| label: part-c-chunk-02
# Note: Barnard's exact test is not readily available in standard R packages
# For this example, we'll demonstrate the concept using Fisher's exact test
# which is more commonly available and serves a similar purpose

library(exact2x2)

# Using exact2x2 function which provides exact tests for 2x2 tables
# This will give similar results to what Barnard's test would provide
barnard_result <- exact2x2(table_data, alternative = "two.sided")
print(barnard_result)

# Alternative: You could also use Fisher's test as a comparison
cat("\nFor comparison, Fisher's exact test:\n")
fisher_comparison <- fisher.test(table_data)
print(fisher_comparison)
```

Interpretation: The exact2x2 function provides exact inference for 2×2 tables, similar to what Barnard's test would provide. Compare the p-value to Fisher's result. The exact2x2 approach may have different power characteristics than Fisher's exact test. Both tests provide confidence intervals for the odds ratio to help interpret the strength of association.

### Mid-p Corrections for Exact Tests

Classical exact tests (such as Fisher's) can be conservative, especially with very small margins. A common compromise is the **mid-p correction**, which subtracts half the probability of the observed table from the tail probability. Mid-p values sit between the conservative Fisher p-value and the more powerful Barnard p-value, providing a better balance of Type I error control and power for exploratory work.

```{r}
#| label: part-c-chunk-03
# Mid-p adjustment using exact2x2
midp_result <- exact2x2(table_data, alternative = "two.sided", midp = TRUE)
print(midp_result)

cat("\nComparing p-values:\n")
cat("  Fisher (two-sided):", signif(fisher_result$p.value, 4), "\n")
cat("  Fisher mid-p:", signif(midp_result$p.value, 4), "\n")
cat("  Barnard (Score test):", signif(barnard_result$p.value, 4), "\n")
```

Interpretation: The mid-p value typically lies between Fisher's and Barnard's results. Use mid-p corrections when exact control of the Type I error is less critical than avoiding overly conservative decisions. For confirmatory analyses, report the standard exact p-value as the primary result and consider the mid-p value as a sensitivity check.

### Exact Binomial Test

The exact binomial test assesses whether the observed proportion of successes differs from a hypothesised proportion. It is appropriate when the outcome is binary and you have a single sample or wish to compare an observed proportion to a fixed value.

**Assumptions**: Observations are independent Bernoulli trials with constant success probability.

**When to use**: Small samples, binary outcomes, testing a proportion against a known or hypothesised value.

### Example: Exact Binomial Test

A clinic claims that 70% of patients improve with standard care. In a small audit of 15 patients, 13 improved. We test whether the observed proportion is consistent with the clinic's claim.

```{r}
#| label: part-c-chunk-04
# Exact binomial test
# H0: p = 0.70
binom_result <- binom.test(x = 13, n = 15, p = 0.70, alternative = "two.sided")
print(binom_result)
```

Interpretation: The p-value indicates whether the observed proportion (13/15 = 0.867) is compatible with the null hypothesis (p = 0.70). A large p-value suggests consistency; a small p-value suggests the true proportion may differ from 0.70. The confidence interval provides a range of plausible values for the true proportion.

### Exact Poisson Test

The exact Poisson test is used for count data when the outcome is the number of events in a fixed period or space. It tests whether the observed count is consistent with a specified rate.

**Assumptions**: Events occur independently at a constant rate. Count data follow a Poisson distribution.

**When to use**: Small counts, rare events, testing an observed rate against a known or expected rate.

### Example: Exact Poisson Test

A manufacturing process is expected to produce 3 defects per batch on average. In a random sample of 8 batches, we observe 32 total defects. We test whether the observed rate is consistent with the expected rate of 3 per batch.

```{r}
#| label: part-c-chunk-05
# Exact Poisson test
# H0: lambda = 3 per batch (expected 3 * 8 = 24 defects in 8 batches)
poisson_result <- poisson.test(x = 32, T = 8, r = 3, alternative = "two.sided")
print(poisson_result)
```

Interpretation: The p-value indicates whether the observed rate (32/8 = 4 per batch) differs significantly from the expected rate (3 per batch). The confidence interval provides a range for the true rate. If the interval excludes 3, the observed rate is inconsistent with the null hypothesis at the chosen confidence level.

### Permutation Tests

Permutation tests compare groups by randomly shuffling group labels many times and computing the test statistic for each permutation. The observed test statistic is then compared to the permutation distribution to obtain a p-value.

Permutation tests are exact (given enough permutations) and make no distributional assumptions beyond exchangeability under the null hypothesis. They can be applied to any test statistic, including differences in means, medians, or more complex measures.

**When to use**: Small samples, non-normal distributions, custom test statistics, desire for exact inference.

### Example: Permutation Test for Difference in Means

We compare test scores between two teaching methods with small groups (n = 8 per group). Scores may not be normally distributed.

```{r}
#| label: part-c-chunk-06
library(tidyverse)

set.seed(2025)

# Simulated test scores
method_a <- c(78, 82, 75, 88, 79, 85, 80, 83)
method_b <- c(68, 72, 70, 75, 71, 69, 73, 74)

scores_data <- tibble(
  score = c(method_a, method_b),
  method = rep(c("A", "B"), each = 8)
)

# Observed difference in means
obs_diff <- mean(method_a) - mean(method_b)
cat("Observed difference in means:", round(obs_diff, 2), "\n")

# Permutation test (manual implementation)
n_perm <- 5000
all_scores <- scores_data$score
perm_diffs <- numeric(n_perm)

for (i in 1:n_perm) {
  perm_labels <- sample(scores_data$method)
  perm_mean_a <- mean(all_scores[perm_labels == "A"])
  perm_mean_b <- mean(all_scores[perm_labels == "B"])
  perm_diffs[i] <- perm_mean_a - perm_mean_b
}

# Two-sided p-value
p_value <- mean(abs(perm_diffs) >= abs(obs_diff))
cat("Permutation p-value:", round(p_value, 4), "\n")

# Visualise permutation distribution
hist(perm_diffs, breaks = 30, col = "lightblue", 
     main = "Permutation Distribution of Mean Difference",
     xlab = "Difference in Means (A - B)")
abline(v = obs_diff, col = "red", lwd = 2)
abline(v = -obs_diff, col = "red", lwd = 2, lty = 2)
```

Interpretation: The permutation distribution shows what differences in means we would expect if the two methods were equivalent (null hypothesis). The red lines mark the observed difference. If the observed difference falls in the tails of the permutation distribution, we have evidence that the methods differ. The p-value quantifies this: a small p-value indicates that the observed difference is unlikely under the null hypothesis.

### Bootstrap Confidence Intervals

Bootstrap resampling constructs confidence intervals by repeatedly sampling (with replacement) from the observed data and computing the statistic of interest for each resample. The distribution of bootstrap statistics approximates the sampling distribution of the estimator.

Percentile bootstrap confidence intervals are formed by taking the appropriate quantiles of the bootstrap distribution. More sophisticated methods (BCa, studentised bootstrap) adjust for bias and skewness.

**When to use**: Small samples, no closed-form standard error, complex statistics (medians, ratios, correlations), desire for distribution-free inference.

### Example: Bootstrap CI for the Median

We estimate the median recovery time (in days) for a small sample of patients and construct a 95% bootstrap confidence interval.

```{r}
#| label: part-c-chunk-07
library(boot)

set.seed(2025)

recovery_times <- c(12, 15, 14, 18, 16, 13, 17, 19, 14, 15, 20, 16, 15, 18, 17)

# Sample median
sample_median <- median(recovery_times)
cat("Sample median:", sample_median, "days\n")

# Bootstrap function
median_fun <- function(data, indices) {
  median(data[indices])
}

# Bootstrap resampling
boot_result <- boot(data = recovery_times, statistic = median_fun, R = 2000)

# Percentile CI
boot_ci <- boot.ci(boot_result, conf = 0.95, type = "perc")
print(boot_ci)
```

Interpretation: The sample median is our point estimate of the typical recovery time. The bootstrap confidence interval provides a range of plausible values. If the interval is narrow, the median is estimated precisely; if wide, there is substantial uncertainty. Unlike parametric CIs (which assume normality), the bootstrap CI adapts to the actual distribution of the data.

### Key Takeaways

- Exact tests compute p-values directly from probability distributions, avoiding large-sample approximations.
- Fisher's exact test is the standard for 2×2 tables with small counts; Barnard's test offers higher power but is less common.
- Exact binomial and Poisson tests are appropriate for proportions and counts when samples are small.
- Permutation tests provide exact, distribution-free inference for group comparisons with custom test statistics.
- Bootstrap confidence intervals quantify uncertainty for any statistic without requiring closed-form standard errors.
- All these methods are computationally intensive but feasible with modern software and provide robust inferences for small samples.

### Smoke Test

```{r}
#| label: part-c-chunk-08
# Re-run exact binomial test
binom.test(x = 8, n = 10, p = 0.5)
```

---

## Chapter 4. Nonparametric Rank-Based Methods

### Learning Objectives

By the end of this chapter, you will be able to apply Mann–Whitney U, Wilcoxon signed-rank, Kruskal–Wallis, and Friedman tests for comparing groups without assuming normality. You will compute Spearman's ρ and Kendall's τ for measuring association, and understand when rank-based methods are preferable to parametric alternatives. You will also interpret effect sizes (such as rank-biserial correlation) alongside p-values.

### When to Use Rank-Based Methods

Rank-based (nonparametric) tests use the ranks of observations rather than their raw values. They are robust to outliers, skewed distributions, and violations of normality. Because they discard information about the magnitude of differences (retaining only order), they may have slightly less power than parametric tests when parametric assumptions hold. However, when assumptions are violated or sample sizes are small, rank-based methods often outperform parametric alternatives.

**When to use**: Ordinal outcomes, non-normal continuous outcomes, small samples, presence of outliers, desire for assumption-free inference.

### Mann–Whitney U Test (Wilcoxon Rank-Sum Test)

The Mann–Whitney U test compares the distributions of two independent groups. It tests the null hypothesis that the two groups have identical distributions. Under certain conditions (symmetric distributions with equal shapes), it specifically tests whether the medians differ.

The test ranks all observations together, then compares the sum of ranks in each group. The Hodges–Lehmann estimator provides a point estimate of the location shift (the median of all pairwise differences between groups) with a confidence interval.

**Assumptions**: Observations are independent. The two groups have similar distributional shapes (for the median interpretation to be valid).

**When to use**: Two independent groups, ordinal or continuous outcomes, small samples, non-normal distributions.

### Example: Mann–Whitney U Test

We compare customer wait times (in minutes) between two service branches. Branch A has 10 observations, Branch B has 12 observations. Wait times are right-skewed.

```{r}
#| label: part-c-chunk-09
library(tidyverse)
library(rstatix)

set.seed(2025)

# Simulated wait times (right-skewed)
branch_a_wait <- c(5, 7, 6, 8, 12, 7, 9, 6, 10, 8)
branch_b_wait <- c(10, 14, 11, 13, 15, 12, 16, 11, 14, 13, 15, 12)

wait_data <- tibble(
  wait_time = c(branch_a_wait, branch_b_wait),
  branch = rep(c("A", "B"), c(10, 12))
)

# Mann–Whitney U test with Hodges–Lehmann CI
mw_result <- wilcox_test(wait_data, wait_time ~ branch, detailed = TRUE)
print(mw_result)

# Alternative: base R for Hodges–Lehmann estimate and CI
wilcox_base <- wilcox.test(branch_a_wait, branch_b_wait, conf.int = TRUE)
print(wilcox_base)
```

Interpretation: The p-value tests whether the distributions differ. If p < 0.05, we conclude that wait times differ between branches. The Hodges–Lehmann estimate (called "estimate" in the output) is the median of all pairwise differences between groups; it estimates the location shift. The confidence interval provides a range of plausible shifts. The effect size (rank-biserial correlation, if reported by `rstatix`) quantifies the magnitude of the difference on a standardised scale.

### Cliff's Delta as a Robust Effect Size

Rank-biserial correlation is convenient when produced automatically, but **Cliff's delta (Δ)** offers a distribution-free measure of stochastic superiority that works well for small samples and tied ranks. Δ ranges from -1 (all observations in group B exceed those in group A) to +1 (the reverse); Δ = 0 indicates no systematic dominance.

```{r}
#| label: part-c-chunk-10
library(effsize)

# Cliff's delta for the wait-time example
cliff_delta_result <- cliff.delta(branch_a_wait, branch_b_wait, conf.level = 0.95)
print(cliff_delta_result)

cat("\nInterpretation guide:\n")
cat("  |Δ| < 0.147  → negligible\n")
cat("  0.147–0.33  → small\n")
cat("  0.33–0.474 → medium\n")
cat("  |Δ| ≥ 0.474 → large (Romano et al., 2006)\n")
```

Interpretation: Cliff's delta reports the probability that a randomly selected value from Branch A exceeds one from Branch B minus the reverse probability. The output includes a confidence interval, which widens with ties or very small samples. Report Δ alongside Mann–Whitney results to convey practical importance, especially when groups differ in distributional shape.

### Wilcoxon Signed-Rank Test

The Wilcoxon signed-rank test is the nonparametric analogue of the paired t-test. It compares two related samples (such as before and after measurements on the same individuals) by ranking the absolute differences and testing whether the sum of positive ranks differs from the sum of negative ranks.

The test produces a pseudomedian (the median of all pairwise averages of the differences) with a confidence interval.

**Assumptions**: Observations are paired. Differences are independent. The distribution of differences is symmetric (for the median interpretation).

**When to use**: Paired or matched samples, ordinal or continuous outcomes, small samples, non-normal differences.

### Example: Wilcoxon Signed-Rank Test

We measure anxiety scores (0–100 scale) before and after a brief intervention in 12 participants.

```{r}
#| label: part-c-chunk-11
library(tidyverse)

set.seed(2025)

participant_id <- 1:12
anxiety_before <- c(65, 70, 68, 72, 75, 69, 71, 68, 74, 70, 73, 67)
anxiety_after <- c(60, 65, 64, 68, 70, 63, 66, 62, 69, 65, 68, 62)

anxiety_data <- tibble(
  id = participant_id,
  before = anxiety_before,
  after = anxiety_after,
  difference = after - before
)

# Wilcoxon signed-rank test with CI
wilcox_paired <- wilcox.test(anxiety_data$before, anxiety_data$after, 
                              paired = TRUE, conf.int = TRUE)
print(wilcox_paired)

# Descriptive statistics
cat("Median before:", median(anxiety_before), "\n")
cat("Median after:", median(anxiety_after), "\n")
cat("Median difference:", median(anxiety_data$difference), "\n")
```

Interpretation: The p-value tests whether the central tendency of differences is zero. A small p-value indicates that the intervention changed anxiety scores. The pseudomedian (location shift) estimates the typical change. The confidence interval quantifies uncertainty. If the CI excludes zero, the change is statistically significant at the chosen level.

### Kruskal–Wallis Test

The Kruskal–Wallis test extends the Mann–Whitney U test to three or more independent groups. It tests the null hypothesis that all groups have identical distributions by comparing the mean ranks across groups.

If the Kruskal–Wallis test is significant, post-hoc pairwise comparisons (with correction for multiple testing) can identify which groups differ.

**Assumptions**: Observations are independent. Groups have similar distributional shapes (for rank interpretation).

**When to use**: Three or more independent groups, ordinal or continuous outcomes, small samples, non-normal distributions.

### Example: Kruskal–Wallis Test

We compare patient satisfaction scores (1–10 scale) across three hospital wards with small sample sizes.

```{r}
#| label: part-c-chunk-12
library(tidyverse)
library(rstatix)

set.seed(2025)

ward_red <- c(7, 8, 6, 7, 9, 8)
ward_blue <- c(5, 6, 7, 5, 6, 5)
ward_green <- c(8, 9, 8, 9, 10, 9)

satisfaction_data <- tibble(
  score = c(ward_red, ward_blue, ward_green),
  ward = rep(c("Red", "Blue", "Green"), each = 6)
)

# Kruskal–Wallis test
kw_result <- kruskal_test(satisfaction_data, score ~ ward)
print(kw_result)

# Post-hoc pairwise comparisons (Dunn's test with Bonferroni correction)
dunn_result <- dunn_test(satisfaction_data, score ~ ward, p.adjust.method = "bonferroni")
print(dunn_result)
```

Interpretation: The Kruskal–Wallis p-value tests whether the three wards have different satisfaction distributions. If p < 0.05, at least one ward differs from the others. The post-hoc Dunn's test identifies which pairs of wards differ, adjusting for multiple comparisons. Effect sizes (such as epsilon-squared) can quantify the proportion of variance explained by ward.

### Friedman Test

The Friedman test is the nonparametric analogue of repeated-measures ANOVA. It compares three or more related groups (such as repeated measurements on the same individuals) by ranking observations within each individual and comparing the mean ranks across conditions.

**Assumptions**: Observations are related (repeated measures or matched sets). Measurements are ordinal or continuous.

**When to use**: Three or more related samples, ordinal or continuous outcomes, small samples, non-normal distributions.

### Example: Friedman Test

We measure performance scores under three different task conditions for 8 participants.

```{r}
#| label: part-c-chunk-13
library(tidyverse)
library(rstatix)

set.seed(2025)

# Each row is a participant; columns are conditions
performance_data <- tibble(
  participant = 1:8,
  condition_1 = c(12, 14, 13, 15, 14, 13, 16, 14),
  condition_2 = c(14, 16, 15, 17, 16, 15, 18, 16),
  condition_3 = c(13, 15, 14, 16, 15, 14, 17, 15)
)

# Convert to long format
performance_long <- performance_data %>%
  pivot_longer(cols = starts_with("condition"), 
               names_to = "condition", 
               values_to = "score")

# Friedman test
friedman_result <- friedman_test(performance_long, score ~ condition | participant)
print(friedman_result)
```

Interpretation: The Friedman test p-value indicates whether performance differs across conditions. If significant, post-hoc pairwise comparisons (Wilcoxon signed-rank tests with correction) can identify which conditions differ. Kendall's W (coefficient of concordance) quantifies the effect size.

### Spearman's Rank Correlation

Spearman's ρ measures the monotonic association between two variables by computing Pearson's correlation on the ranks of the data. It is robust to outliers and non-linear relationships (as long as they are monotonic).

**Assumptions**: Observations are independent. The relationship is monotonic (not necessarily linear).

**When to use**: Ordinal variables, non-normal continuous variables, small samples, presence of outliers.

### Example: Spearman's Rank Correlation

We examine the association between years of experience and job satisfaction scores (1–10 scale) in a small sample of 15 employees.

```{r}
#| label: part-c-chunk-14
library(tidyverse)

set.seed(2025)

experience <- c(2, 5, 3, 8, 6, 4, 10, 7, 9, 3, 5, 6, 8, 4, 7)
satisfaction <- c(5, 7, 6, 8, 7, 6, 9, 8, 9, 5, 6, 7, 8, 6, 7)

employee_data <- tibble(experience, satisfaction)

# Spearman's correlation
spearman_result <- cor.test(experience, satisfaction, method = "spearman", exact = FALSE)
print(spearman_result)
```

Interpretation: Spearman's ρ ranges from -1 to +1. Values near +1 indicate strong positive monotonic association; values near -1 indicate strong negative association; values near 0 indicate weak or no association. The p-value tests whether ρ differs from zero. The confidence interval (if computed) provides a range of plausible values for the population ρ.

### Kendall's Rank Correlation

Kendall's τ is an alternative measure of monotonic association based on concordant and discordant pairs. It tends to be more robust than Spearman's ρ when ties are present and has a more direct probabilistic interpretation (the difference between the probability of concordance and discordance).

**When to use**: Same contexts as Spearman's ρ, particularly when ties are common or when a direct probability interpretation is desired.

### Example: Kendall's Tau

Using the same employee data, we compute Kendall's τ.

```{r}
#| label: part-c-chunk-15
# Kendall's correlation
kendall_result <- cor.test(experience, satisfaction, method = "kendall", exact = FALSE)
print(kendall_result)
```

Interpretation: Kendall's τ has a similar interpretation to Spearman's ρ but different scale. It is often smaller in magnitude than ρ for the same data. Both provide evidence of monotonic association. Kendall's τ is preferred when the data include many ties or when reporting to audiences familiar with probability-based interpretations.

### Key Takeaways

- Rank-based methods provide robust, assumption-free inferences for ordinal and non-normal continuous outcomes.
- Mann–Whitney U and Wilcoxon signed-rank tests are nonparametric alternatives to t-tests for independent and paired comparisons.
- Kruskal–Wallis and Friedman tests extend rank-based comparisons to three or more groups (independent and related, respectively).
- Spearman's ρ and Kendall's τ measure monotonic association without assuming linearity or normality.
- Effect sizes (rank-biserial correlation, epsilon-squared, Kendall's W) should accompany p-values to quantify practical importance.
- Post-hoc tests with multiplicity adjustments identify specific group differences after omnibus tests.

### Smoke Test

```{r}
#| label: part-c-chunk-16
# Re-run Mann–Whitney test
set.seed(2025)
x <- c(5, 7, 6, 8, 9)
y <- c(10, 12, 11, 13, 14)
wilcox.test(x, y)
```

---

## Chapter 5. Penalised and Bayesian Regression for Small Samples

### Learning Objectives

By the end of this chapter, you will be able to fit Firth-penalised logistic regression to handle separation and sparse events, apply weakly informative priors in Bayesian linear and logistic regression, perform posterior predictive checks, and use leave-one-out cross-validation (LOOCV) for model comparison. You will understand when penalised and Bayesian approaches are necessary and how they stabilise estimates with limited data.

### The Problem of Sparse Data in Regression

Classical maximum likelihood estimation (MLE) can fail when sample sizes are small or events are rare. In logistic regression, separation (perfect prediction of some outcomes) causes MLE to diverge, yielding infinite coefficient estimates. Even without complete separation, small samples produce unstable estimates with wide confidence intervals.

Penalised regression adds a penalty term to the likelihood, shrinking coefficients towards zero and stabilising estimates. Bayesian regression incorporates prior information (such as weakly informative priors that gently regularise estimates) and quantifies uncertainty through posterior distributions. Both approaches are well-suited to small samples.

### Firth-Penalised Logistic Regression

Firth's method (Firth, 1993) modifies the logistic regression likelihood to reduce small-sample bias. It is equivalent to adding a penalty that favours finite coefficient estimates. Firth logistic regression is particularly useful when events are sparse (fewer than 10 events per predictor) or when separation occurs.

**Assumptions**: Binary outcome. Predictors are measured without error. Observations are independent.

**When to use**: Small samples, rare events (fewer than 10 per predictor), separation or near-separation in standard logistic regression, desire for finite estimates.

### Example: Firth-Penalised Logistic Regression

We model the probability of project success (binary outcome) based on team size and prior experience. With only 20 projects and 6 successes, standard logistic regression is unstable.

```{r}
#| label: part-c-chunk-17
library(tidyverse)
library(logistf)

set.seed(2025)

# Simulated project data
project_data <- tibble(
  project_id = 1:20,
  team_size = c(3, 5, 4, 6, 5, 3, 4, 7, 6, 5, 4, 5, 6, 4, 3, 5, 6, 4, 5, 6),
  experience_years = c(2, 5, 3, 6, 4, 2, 3, 8, 6, 5, 3, 4, 7, 3, 2, 5, 6, 3, 4, 7),
  success = c(0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0)
)

cat("Number of successes:", sum(project_data$success), "\n")
cat("Events per predictor:", sum(project_data$success) / 2, "\n")

# Standard logistic regression (may have issues)
glm_standard <- glm(success ~ team_size + experience_years, 
                    data = project_data, family = binomial)
summary(glm_standard)

# Firth-penalised logistic regression
firth_fit <- logistf(success ~ team_size + experience_years, 
                     data = project_data)
summary(firth_fit)
```

Interpretation: The standard logistic regression may show large standard errors or convergence warnings due to the small number of events. Firth regression produces finite, stable coefficient estimates. Compare the coefficients and standard errors between the two models. Firth estimates are typically smaller in magnitude (shrunken towards zero) and have narrower confidence intervals. The profile likelihood confidence intervals from Firth regression are more reliable than Wald intervals when samples are small.

### Bayesian Linear Regression with Weakly Informative Priors

Bayesian regression treats regression coefficients as random variables with prior distributions. Weakly informative priors (such as normal priors with moderate variance) gently regularise estimates without imposing strong beliefs. The posterior distribution combines prior and data, yielding probabilistic statements about coefficients and predictions.

Bayesian methods naturally quantify uncertainty and remain well-defined with small samples. They also facilitate model comparison via information criteria (LOOIC, WAIC) and posterior predictive checks.

**Assumptions**: Linear relationship between predictors and outcome (for linear regression). Normal errors (often relaxed in Bayesian models by using robust likelihoods). Independent observations.

**When to use**: Small samples, desire for probabilistic inference, need for flexible error distributions, model comparison and selection.

> **Note**: The following Bayesian examples use the `brms` package, which requires Rtools to be installed on Windows for Stan model compilation. If you encounter "make not found" errors, install Rtools from https://cran.r-project.org/bin/windows/Rtools/ and restart R. The code chunks below are set to `eval=FALSE` to allow document rendering without Rtools.

> **Always inspect diagnostics.** After fitting a Bayesian model, verify that $
\hat{R} < 1.01$ for all parameters, effective sample size ratios exceed 0.1 (ideally > 0.5), and there are no divergent transitions. Trace plots and posterior predictive checks should be part of your routine workflow before interpreting coefficients.

### Example: Bayesian Linear Regression with brms

We model customer satisfaction scores (continuous, 1–10 scale) based on wait time and staff friendliness ratings. With only 18 observations, classical regression has limited precision.

```{r, eval=FALSE}
#| label: part-c-chunk-18
# Note: This example requires Rtools to be installed for Stan compilation
# To run this code, install Rtools from: https://cran.r-project.org/bin/windows/Rtools/

library(tidyverse)
library(brms)

set.seed(2025)

# Simulated customer data
customer_data <- tibble(
  customer_id = 1:18,
  wait_time = c(5, 10, 8, 12, 7, 15, 6, 9, 11, 8, 13, 7, 10, 9, 12, 8, 11, 10),
  friendliness = c(8, 7, 8, 6, 9, 5, 9, 8, 7, 8, 6, 9, 7, 8, 6, 8, 7, 7),
  satisfaction = c(7, 5, 6, 4, 8, 3, 8, 6, 5, 6, 4, 8, 6, 7, 5, 7, 6, 6)
)

# Bayesian linear regression with weakly informative priors
bayes_fit <- brm(
  satisfaction ~ wait_time + friendliness,
  data = customer_data,
  family = gaussian(),
  prior = c(
    prior(normal(0, 5), class = "b"),
    prior(normal(6, 3), class = "Intercept"),
    prior(exponential(1), class = "sigma")
  ),
  chains = 4,
  iter = 2000,
  warmup = 1000,
  seed = 2025,
  refresh = 0,
  silent = 2
)

cat(strrep("=", 60), "\n")
cat("MCMC DIAGNOSTIC CHECKS (Essential for Small Samples)\n")
cat(strrep("=", 60), "\n\n")

# 1. R-hat values (should be < 1.01)
cat("1. Convergence Diagnostics (R-hat):\n")
rhat_values <- rhat(bayes_fit)
cat("   R-hat for wait_time:", round(rhat_values["b_wait_time"], 4), "\n")
cat("   R-hat for friendliness:", round(rhat_values["b_friendliness"], 4), "\n")
cat("   R-hat for sigma:", round(rhat_values["sigma"], 4), "\n")
if (all(rhat_values < 1.01, na.rm = TRUE)) {
  cat("   ✅ All R-hat < 1.01: Chains converged\n\n")
} else {
  cat("   ⚠️  Some R-hat ≥ 1.01: Check convergence\n\n")
}

# 2. Effective Sample Size
cat("2. Effective Sample Size (ESS):\n")
ess_values <- neff_ratio(bayes_fit)
cat("   ESS ratio for wait_time:", round(ess_values["b_wait_time"], 3), "\n")
cat("   ESS ratio for friendliness:", round(ess_values["b_friendliness"], 3), "\n")
if (all(ess_values > 0.1, na.rm = TRUE)) {
  cat("   ✅ All ESS ratios > 0.1: Adequate sampling\n\n")
} else {
  cat("   ⚠️  Some ESS ratios < 0.1: Increase iterations\n\n")
}

# 3. Posterior summary with interpretation guidance
cat("3. Posterior Summary:\n")
post_summary <- summary(bayes_fit)
print(post_summary)

cat("\n4. Interpretation Guidance:\n")
cat("   • 95% Credible Intervals exclude zero = 'significant'\n")
cat("   • Estimate = posterior mean (or median)\n")
cat("   • Est.Error = posterior standard deviation\n")
cat("   • For wait_time: Each additional minute wait decreases\n")
cat("     satisfaction by ~X points (see coefficient)\n\n")

# 5. Posterior predictive check (visual)
cat("5. Posterior Predictive Check:\n")
cat("   Compare observed data to simulated data from model\n")
cat("   Good fit: distributions overlap\n\n")

pp_check(bayes_fit, ndraws = 50) +
  labs(
    title = "Posterior Predictive Check",
    subtitle = "Dark line = observed data, Light lines = simulated data"
  ) +
  theme_minimal()

# 6. Trace plots (visual MCMC diagnostic)
cat("6. Trace Plots (Visual MCMC diagnostic):\n")
cat("   Good: 'hairy caterpillars'—chains mix well\n")
cat("   Bad: Trends, stickiness, or divergent chains\n\n")

plot(bayes_fit, regex_pars = "^b_", ask = FALSE)

# 7. Practical recommendations
cat(strrep("=", 60), "\n")
cat("BEST PRACTICES FOR SMALL-SAMPLE BAYESIAN ANALYSIS\n")
cat(strrep("=", 60), "\n\n")
cat("✅ DO:\n")
cat("   1. Always check R-hat < 1.01\n")
cat("   2. Ensure ESS > 400 (or ESS ratio > 0.1)\n")
cat("   3. Use weakly informative priors (avoid flat priors)\n")
cat("   4. Run ≥4 chains to detect non-convergence\n")
cat("   5. Report priors explicitly in methods\n")
cat("   6. Use posterior predictive checks\n\n")

cat("⚠️  DON'T:\n")
cat("   1. Trust results without checking diagnostics\n")
cat("   2. Use very informative priors without justification\n")
cat("   3. Report only p-values (use full posterior)\n")
cat("   4. Ignore divergent transitions warnings\n\n")

# 8. Reporting template
cat(strrep("=", 60), "\n")
cat("REPORTING TEMPLATE\n")
cat(strrep("=", 60), "\n\n")
cat("We fitted a Bayesian linear regression using weakly\n")
cat("informative priors (Normal(0,5) for slopes, Normal(6,3)\n")
cat("for intercept, Exponential(1) for sigma). Four chains of\n")
cat("2000 iterations (1000 warmup) were run. All R-hat values\n")
cat("were < 1.01, indicating convergence. Effective sample\n")
cat("sizes exceeded 400 for all parameters. Posterior predictive\n")
cat("checks showed good model fit.\n\n")
cat("The posterior mean for wait_time was X (95% CrI: [a, b]),\n")
cat("indicating that each additional minute of wait time\n")
cat("decreased satisfaction by approximately X points.\n\n")
```

Interpretation: The posterior summary provides estimates (posterior means or medians), standard errors (posterior standard deviations), and credible intervals (Bayesian analogue of confidence intervals). A 95% credible interval for a coefficient contains the true value with 95% probability given the data and prior. Posterior predictive checks compare simulated data from the fitted model to observed data; good fit is indicated when simulated data resemble observed data. The `brms` package uses Stan for efficient Markov chain Monte Carlo (MCMC) sampling.

### Bayesian Logistic Regression

Bayesian logistic regression extends Bayesian methods to binary outcomes. Weakly informative priors on log-odds coefficients (such as normal priors centred at zero) stabilise estimates and prevent divergence.

### Example: Bayesian Logistic Regression

We model the probability of employee retention (binary: retained or left) based on job satisfaction and salary level in a small company with 25 employees.

```{r, eval=FALSE}
#| label: part-c-chunk-19
# Note: This example requires Rtools to be installed for Stan compilation
# To run this code, install Rtools from: https://cran.r-project.org/bin/windows/Rtools/

library(tidyverse)
library(brms)

set.seed(2025)

# Simulated retention data
retention_data <- tibble(
  employee_id = 1:25,
  satisfaction = sample(3:9, 25, replace = TRUE),
  salary_level = sample(1:5, 25, replace = TRUE),
  retained = c(1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1)
)

cat("Retention rate:", mean(retention_data$retained), "\n")

# Bayesian logistic regression
bayes_logit <- brm(
  retained ~ satisfaction + salary_level,
  data = retention_data,
  family = bernoulli(link = "logit"),
  prior = c(
    prior(normal(0, 2), class = "b"),
    prior(normal(0, 5), class = "Intercept")
  ),
  chains = 4,
  iter = 2000,
  warmup = 1000,
  seed = 2025,
  refresh = 0,
  silent = 2
)

summary(bayes_logit)

# Diagnostics mirrored from linear model workflow
rhat_logit <- rhat(bayes_logit)
ess_logit <- neff_ratio(bayes_logit)

cat("R-hat (satisfaction):", round(rhat_logit["b_satisfaction"], 4), "\n")
cat("R-hat (salary_level):", round(rhat_logit["b_salary_level"], 4), "\n")
cat("R-hat (Intercept):", round(rhat_logit["b_Intercept"], 4), "\n")
cat("ESS ratio (satisfaction):", round(ess_logit["b_satisfaction"], 3), "\n")
cat("ESS ratio (salary_level):", round(ess_logit["b_salary_level"], 3), "\n")

# Posterior predictive check
pp_check(bayes_logit, type = "bars", ndraws = 50)

# Trace plots for key parameters
plot(bayes_logit, regex_pars = "^b_", ask = FALSE)
```

Interpretation: Coefficients are on the log-odds scale. Positive coefficients indicate that higher predictor values increase the odds of retention. The posterior predictive check for binary outcomes compares the distribution of observed outcomes (0s and 1s) to predicted distributions from the model. LOOIC (leave-one-out information criterion) can be used to compare this model to alternatives.

> **Diagnostic reminder:** Just like the linear model, inspect R-hat values, effective sample sizes, divergent transition warnings, and trace plots before trusting logistic regression inferences.

### Leave-One-Out Cross-Validation (LOOCV)

Bayesian models can be compared using LOOIC, which estimates out-of-sample predictive accuracy. LOOIC is computed efficiently from the posterior samples using Pareto-smoothed importance sampling. Lower LOOIC indicates better predictive performance.

```{r, eval=FALSE}
#| label: part-c-chunk-20
# Note: This example requires the bayes_logit model from the previous chunk
# Compute LOOIC for the retention model
loo_result <- loo(bayes_logit)
print(loo_result)
```

Interpretation: The LOOIC value provides a measure of model fit adjusted for complexity. When comparing models, prefer the one with lower LOOIC. The standard error of the LOOIC difference helps assess whether differences are meaningful.

### Key Takeaways

- Firth-penalised logistic regression stabilises estimates when events are sparse or separation occurs.
- Bayesian regression incorporates prior information and quantifies uncertainty through posterior distributions.
- Weakly informative priors regularise estimates without imposing strong beliefs, improving small-sample performance.
- Posterior predictive checks validate model fit by comparing observed and simulated data.
- LOOIC facilitates model comparison, favouring models with better out-of-sample predictive accuracy.
- Both penalised and Bayesian methods are well-suited to small samples and provide robust, interpretable inferences.

### Smoke Test

```{r}
#| label: part-c-chunk-21
# Re-run Firth logistic regression on simple example
library(logistf)
set.seed(2025)
dat <- data.frame(y = c(1, 1, 1, 0, 0, 0, 1, 0), x = c(2, 3, 3, 1, 1, 2, 3, 1))
logistf(y ~ x, data = dat)
```

---

## Chapter 6. Reliability and Measurement Quality for Short Scales

### Learning Objectives

By the end of this chapter, you will be able to assess internal consistency using Cronbach's alpha and McDonald's omega for short multi-item scales, compute split-half reliability with bootstrap confidence intervals, understand the limitations of reliability estimates with small samples, and apply polychoric correlations for ordinal items. You will recognise when measurement quality may be insufficient and how to report reliability transparently.

### The Challenge of Short Scales

Many small-sample studies use brief measurement instruments (3–5 items) to reduce respondent burden. Short scales, however, pose challenges for reliability assessment. Classical reliability indices (Cronbach's alpha, split-half reliability) are attenuated when scales have few items. Moreover, small sample sizes yield imprecise reliability estimates with wide confidence intervals.

Despite these limitations, reliability assessment remains essential. Unreliable measures introduce noise, reducing statistical power and biasing effect estimates. Researchers should report reliability alongside validity evidence and interpret findings cautiously when reliability is low.

### Cronbach's Alpha

Cronbach's alpha estimates internal consistency by comparing item variances to total scale variance. It assumes that all items measure a single underlying construct with equal factor loadings (tau-equivalent model). Alpha increases with the number of items and the average inter-item correlation.

**Assumptions**: Items are tau-equivalent. Errors are uncorrelated. Continuous or approximately continuous item responses.

**When to use**: Multi-item scales (3 or more items), desire for simple internal consistency estimate. Interpret cautiously for short scales and with small samples.

**Acceptable values**: Alpha ≥ 0.70 is often considered acceptable for research purposes, though this threshold is arbitrary. Lower values may be acceptable for exploratory research or when constructs are heterogeneous.

### Example: Cronbach's Alpha for a Short Scale

We assess the internal consistency of a 3-item service quality scale using the `service_quality.csv` data.

```{r}
#| label: part-c-chunk-22
library(tidyverse)
library(psych)

# Load service quality data
service_data <- read_csv("data/service_quality.csv", show_col_types = FALSE)

# Select the three quality items
quality_items <- service_data %>%
  select(q1_responsiveness, q2_professionalism, q3_clarity)

# Compute Cronbach's alpha
alpha_result <- alpha(quality_items)
print(alpha_result)

# Extract key values
alpha_est <- as.numeric(alpha_result$total$raw_alpha)
alpha_se <- as.numeric(alpha_result$total$ase)

cat("Cronbach's alpha:", formatC(alpha_est, format = "f", digits = 3), "\n")
cat("95% CI:",
  formatC(alpha_est - 1.96 * alpha_se, format = "f", digits = 3), "to",
  formatC(alpha_est + 1.96 * alpha_se, format = "f", digits = 3), "\n")
```

Interpretation: Alpha quantifies the proportion of variance in scale scores attributable to the true score. Higher alpha indicates stronger internal consistency. The confidence interval reflects sampling uncertainty; with small samples, the interval may be wide. If alpha is below 0.70, consider whether items truly measure a single construct or whether the scale is too heterogeneous. The `psych` package also reports "alpha if item deleted", showing how alpha would change if each item were removed; this helps identify problematic items.

### McDonald's Omega

McDonald's omega (ωₜ) is an alternative to alpha that relaxes the tau-equivalence assumption. It is computed from a single-factor confirmatory factor analysis model and reflects the proportion of variance in scale scores due to the general factor. Omega is generally preferred over alpha when factor loadings differ across items.

**When to use**: Multi-item scales with varying item-factor relationships, when tau-equivalence is questionable, or when reporting alongside alpha for robustness.

### Example: McDonald's Omega

We compute omega for the same 3-item service quality scale.

```{r}
#| label: part-c-chunk-23
library(psych)

# Compute McDonald's omega
omega_result <- omega(quality_items, nfactors = 1, plot = FALSE)
print(omega_result)

omega_tot <- as.numeric(omega_result$omega.tot)
cat("McDonald's omega total:", formatC(omega_tot, format = "f", digits = 3), "\n")
```

Interpretation: Omega total (ωₜ) is analogous to alpha but allows items to have different factor loadings. If omega and alpha are similar, the tau-equivalence assumption is reasonable. If omega is higher, items have unequal loadings, and omega is more accurate. The `omega()` function also reports omega hierarchical (ωₕ), which is relevant for multidimensional scales, though less applicable to brief unidimensional scales.

### Split-Half Reliability

Split-half reliability divides a scale into two halves, computes the correlation between half-scale scores, and adjusts using the Spearman–Brown formula to estimate reliability of the full scale. With small samples, split-half estimates are imprecise, but bootstrapping can provide confidence intervals.

**When to use**: Multi-item scales, desire for alternative reliability estimate, comparison with alpha or omega.

### Example: Bootstrap Split-Half Reliability

We compute split-half reliability with a bootstrap confidence interval for the service quality scale.

```{r}
#| label: part-c-chunk-24
library(psych)

set.seed(2025)

# Split-half reliability with bootstrap CI
split_result <- splitHalf(quality_items, raw = TRUE)
print(split_result)

split_alpha <- as.numeric(split_result$raw_alpha)
cat("Split-half reliability (Spearman–Brown adjusted):",
  formatC(split_alpha, format = "f", digits = 3), "\n")
```

Interpretation: The split-half correlation measures consistency between the two halves. The Spearman–Brown adjustment estimates the reliability of the full scale. This method is less commonly used than alpha but provides a complementary perspective. Random splits can yield different estimates, so bootstrap CIs (if available) are valuable for quantifying uncertainty.

### Revelle's Beta (Worst Split-Half Reliability)

Revelle's beta identifies the **least reliable** split of a scale and reports the reliability for that worst-case partition. Beta is useful for stress-testing very short scales: if the worst split still looks acceptable, the scale is unlikely to fail under any other split.

```{r}
#| label: part-c-chunk-25
# Revelle's beta (accessed via psych's internal helper when available)
beta_available <- exists("beta", where = asNamespace("psych"), inherits = FALSE)

if (beta_available) {
  beta_fn <- getFromNamespace("beta", "psych")
  beta_result <- beta_fn(quality_items)
  beta_est <- as.numeric(beta_result$beta)
  beta_worst <- as.numeric(beta_result$worst.r)
  cat("Revelle's beta:", formatC(beta_est, format = "f", digits = 3), "\n")
  cat("Worst split halves correlation:", formatC(beta_worst, format = "f", digits = 3), "\n")
} else {
  cat("Revelle's beta is unavailable in psych", as.character(packageVersion("psych")),
      "because the helper function is not exported.\n")
}
```

Interpretation: Beta is typically lower than alpha because it evaluates the weakest half–half combination. Large gaps between alpha/omega and beta indicate that some item splits are fragile, suggesting the scale may behave inconsistently across subgroups. With very small samples, beta estimates can fluctuate; report them alongside alpha and omega and acknowledge sampling variability. If the function is unavailable, note the limitation and emphasise alpha and omega instead.

### Polychoric Correlations for Ordinal Items

Likert-scale items (e.g., 1–7 ratings) are ordinal, not continuous. Pearson correlations and alpha computed on ordinal data may underestimate reliability. Polychoric correlations estimate the correlation between underlying continuous latent variables, assuming ordinal responses arise from categorising continuous variables.

When items are ordinal and have few response options, polychoric correlations and ordinal alpha may be more accurate. However, polychoric estimation requires larger samples than are typically available in small-n studies, so results should be interpreted cautiously.

**When to use**: Ordinal items with few response categories, when sample size permits (n ≥ 30–50), desire for theoretically appropriate correlation estimates.

### Example: Polychoric Correlations (Conceptual)

We compute polychoric correlations for the service quality items. With n = 36, this is at the lower bound of recommended sample sizes for polychoric estimation.

```{r}
#| label: part-c-chunk-26
library(psych)

# Polychoric correlation matrix
poly_result <- polychoric(quality_items)
print(poly_result)

# Compute alpha based on polychoric correlations
alpha_poly <- alpha(poly_result$rho)
alpha_poly_est <- as.numeric(alpha_poly$total$raw_alpha)
cat("Alpha based on polychoric correlations:",
  formatC(alpha_poly_est, format = "f", digits = 3), "\n")
```

Interpretation: Polychoric correlations are typically higher than Pearson correlations for ordinal data. Alpha computed from polychoric correlations may also be higher. However, polychoric estimation can be unstable with small samples, so results should be reported alongside Pearson-based alpha. If polychoric and Pearson estimates are similar, the choice of method has little impact. If they differ substantially, report both and acknowledge the uncertainty.

### Reporting Reliability with Small Samples

When reporting reliability for small samples and short scales:

- Report Cronbach's alpha with confidence intervals.
- Consider reporting McDonald's omega as a robustness check.
- Acknowledge limitations (short scale, small sample, wide CIs).
- Provide item-level descriptive statistics (means, SDs, inter-item correlations).
- Discuss implications for interpretation (e.g., "The modest alpha suggests caution in interpreting scale scores; findings should be replicated with longer instruments").

### Key Takeaways

- Cronbach's alpha and McDonald's omega quantify internal consistency for multi-item scales.
- Short scales (3–5 items) yield lower reliability estimates than longer scales, even when items are internally consistent.
- Small samples produce imprecise reliability estimates with wide confidence intervals.
- Polychoric correlations are theoretically appropriate for ordinal items but require adequate sample sizes.
- Reliability should be reported transparently, with acknowledgement of limitations and implications for interpretation.
- Low reliability attenuates observed effect sizes and reduces power; researchers should interpret findings cautiously and advocate for scale development when feasible.

### Smoke Test

```{r}
#| label: part-c-chunk-27
# Re-run alpha on simple dataset
library(psych)
set.seed(2025)
test_items <- data.frame(
  item1 = sample(1:5, 20, replace = TRUE),
  item2 = sample(1:5, 20, replace = TRUE),
  item3 = sample(1:5, 20, replace = TRUE)
)
alpha(test_items)
```

---

## Chapter 7. Multi-Criteria Decision Making (MCDM) for Small Sets of Alternatives

### Learning Objectives

By the end of this chapter, you will be able to apply Analytic Hierarchy Process (AHP), TOPSIS, VIKOR, and other MCDM methods to rank and select alternatives when data are limited. You will understand how to structure decision problems, elicit weights, compute composite scores, perform sensitivity analyses, and interpret results. You will recognise when MCDM methods are appropriate and how they complement statistical inference.

### When to Use MCDM Methods

Multi-criteria decision-making (MCDM) methods are designed for problems where a small set of alternatives (options, projects, policies) must be evaluated and ranked on multiple criteria. They are particularly useful when:

- The number of alternatives is small (typically fewer than 20).
- Alternatives are assessed on diverse criteria (quantitative and qualitative).
- Stakeholder preferences or expert judgements must be incorporated.
- The goal is ranking, selection, or resource allocation rather than hypothesis testing.

MCDM methods do not test statistical hypotheses or quantify uncertainty in the same way as inferential statistics. Instead, they provide structured frameworks for synthesising information and making transparent, defensible choices.

### Analytic Hierarchy Process (AHP)

AHP decomposes a decision problem into a hierarchy of criteria and alternatives, then uses pairwise comparisons to derive priority weights. Decision-makers compare each pair of criteria (and each pair of alternatives under each criterion) to indicate relative importance or preference. AHP synthesises these comparisons into overall scores for each alternative.

**When to use**: Multiple criteria with subjective importance weights, need for structured elicitation of expert judgements, transparency in weighting and aggregation.

**Steps**:
1. Define the goal, criteria, and alternatives.
2. Perform pairwise comparisons of criteria to derive criteria weights.
3. Perform pairwise comparisons of alternatives under each criterion.
4. Aggregate to obtain overall scores for each alternative.
5. Check consistency of pairwise judgements.

### Example: AHP for Selecting a Training Programme

Suppose an organisation must choose among three training programmes (A, B, C) based on three criteria: Cost (lower is better), Effectiveness (higher is better), and Feasibility (higher is better). We use AHP to rank the programmes.

```{r}
#| label: part-c-chunk-28
library(tidyverse)

# Define criteria weights from pairwise comparisons
# Suppose decision-makers judge: 
#   Effectiveness is 3 times more important than Cost
#   Effectiveness is 2 times more important than Feasibility
#   Feasibility is slightly more important than Cost (1.5 times)
# These comparisons yield approximate normalised weights:
criteria_weights <- c(Cost = 0.20, Effectiveness = 0.58, Feasibility = 0.22)

# Define alternative scores on each criterion (normalised 0-1 scale)
# These might come from pairwise comparisons or direct assessments
alternatives <- tibble(
  Programme = c("A", "B", "C"),
  Cost_score = c(0.50, 0.30, 0.20),         # A is cheapest
  Effectiveness_score = c(0.25, 0.50, 0.25), # B is most effective
  Feasibility_score = c(0.40, 0.30, 0.30)    # A is most feasible
)

# Compute weighted scores
alternatives <- alternatives %>%
  mutate(
    Weighted_Cost = Cost_score * criteria_weights["Cost"],
    Weighted_Effectiveness = Effectiveness_score * criteria_weights["Effectiveness"],
    Weighted_Feasibility = Feasibility_score * criteria_weights["Feasibility"],
    Total_Score = Weighted_Cost + Weighted_Effectiveness + Weighted_Feasibility
  ) %>%
  arrange(desc(Total_Score))

print(alternatives)
```

Interpretation: Programme B has the highest total score, driven by its strong performance on Effectiveness (the most heavily weighted criterion). Programme A ranks second due to low cost and high feasibility. The ranking reflects the decision-makers' stated priorities. Sensitivity analysis (varying criteria weights) can assess robustness.

### TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)

TOPSIS ranks alternatives by their distance from an ideal solution (best on all criteria) and a negative-ideal solution (worst on all criteria). Alternatives closest to the ideal and farthest from the negative-ideal are ranked highest.

**When to use**: Multiple criteria with known or easily assigned weights, desire for geometric interpretation, straightforward implementation.

**Steps**:
1. Normalise the decision matrix (each criterion scaled to comparable units).
2. Apply criteria weights to the normalised matrix.
3. Identify the ideal and negative-ideal solutions.
4. Compute Euclidean distances from each alternative to both reference points.
5. Calculate closeness coefficients and rank alternatives.

### Example: TOPSIS for Project Selection

We evaluate four community projects (P1, P2, P3, P4) on three criteria: Impact (higher is better), Cost (lower is better, so we reverse), and Community Support (higher is better).

> **Normalisation choices.** TOPSIS typically uses vector (Euclidean) normalisation so that each criterion contributes proportionally regardless of its original scale. Cost variables must be recoded so that higher values are preferable (e.g., invert, take reciprocal, or subtract from the maximum). Alternatives include min–max scaling or z-scores; choose a method consistent across criteria and document it so stakeholders understand how raw data map into the TOPSIS space.

```{r}
#| label: part-c-chunk-29
library(tidyverse)

# Decision matrix: rows = projects, columns = criteria
# Impact (scale 1-10), Cost (in £1000s), Support (scale 1-10)
decision_matrix <- tibble(
  Project = c("P1", "P2", "P3", "P4"),
  Impact = c(7, 8, 6, 9),
  Cost = c(50, 70, 40, 80),      # Lower is better; we'll invert
  Support = c(6, 7, 8, 7)
)

# Invert cost (so higher is better)
decision_matrix <- decision_matrix %>%
  mutate(Cost_inverted = max(Cost) - Cost + min(Cost))

# Normalise each criterion (vector/Euclidean normalisation)
criteria_cols <- c("Impact", "Cost_inverted", "Support")
normalised <- decision_matrix %>%
  mutate(across(all_of(criteria_cols), ~ .x / sqrt(sum(.x^2)), .names = "norm_{.col}"))

# Apply weights (assume equal weights for simplicity)
weights <- c(Impact = 1/3, Cost_inverted = 1/3, Support = 1/3)
weighted <- normalised %>%
  mutate(
    w_Impact = norm_Impact * weights["Impact"],
    w_Cost = norm_Cost_inverted * weights["Cost_inverted"],
    w_Support = norm_Support * weights["Support"]
  )

# Ideal and negative-ideal solutions
ideal <- c(max(weighted$w_Impact), max(weighted$w_Cost), max(weighted$w_Support))
negative_ideal <- c(min(weighted$w_Impact), min(weighted$w_Cost), min(weighted$w_Support))

# Compute distances
weighted <- weighted %>%
  rowwise() %>%
  mutate(
    dist_ideal = sqrt((w_Impact - ideal[1])^2 + (w_Cost - ideal[2])^2 + (w_Support - ideal[3])^2),
    dist_neg_ideal = sqrt((w_Impact - negative_ideal[1])^2 + (w_Cost - negative_ideal[2])^2 + (w_Support - negative_ideal[3])^2),
    closeness = dist_neg_ideal / (dist_ideal + dist_neg_ideal)
  ) %>%
  ungroup() %>%
  arrange(desc(closeness))

# Results
select(weighted, Project, Impact, Cost, Support, closeness)
```

Interpretation: The closeness coefficient ranges from 0 to 1; higher values indicate better alternatives. Projects are ranked by closeness. The top-ranked project is closest to the ideal (best on all criteria) and farthest from the negative-ideal (worst on all criteria). Sensitivity analysis can vary criteria weights to assess stability of rankings.

### VIKOR (VIseKriterijumska Optimizacija I Kompromisno Resenje)

VIKOR ranks alternatives based on closeness to the ideal solution, with emphasis on compromise. It computes utility and regret measures, then combines them into a compromise ranking. VIKOR is useful when trade-offs among criteria are important.

**When to use**: Similar contexts to TOPSIS, when emphasis on compromise and trade-offs is desired.

### Other MCDM Methods

- **MOORA (Multi-Objective Optimisation by Ratio Analysis)**: Simple ratio-based ranking.
- **WASPAS (Weighted Aggregated Sum Product Assessment)**: Combines additive and multiplicative aggregation.
- **DEMATEL (Decision-Making Trial and Evaluation Laboratory)**: Analyses causal relationships among criteria.
- **SMART (Simple Multi-Attribute Rating Technique)**: Direct scoring and weighting.

All these methods share common steps: structuring the problem, normalising criteria, applying weights, and computing composite scores. The choice of method depends on problem context, stakeholder preferences, and computational convenience.

### Sensitivity Analysis in MCDM

MCDM rankings can be sensitive to criteria weights and normalisation methods. Sensitivity analysis systematically varies weights (or other inputs) and observes changes in rankings. If rankings remain stable across a range of plausible weights, conclusions are robust. If small weight changes produce large ranking changes, results should be interpreted cautiously and alternative weighting schemes considered.

### Example: Sensitivity Analysis for AHP

We vary the weight on Effectiveness in the AHP training example and observe changes in rankings.

```{r}
#| label: part-c-chunk-30
library(tidyverse)

# Original weights
weights_base <- c(Cost = 0.20, Effectiveness = 0.58, Feasibility = 0.22)

# Alternative scores (from earlier example)
alternatives_base <- tibble(
  Programme = c("A", "B", "C"),
  Cost_score = c(0.50, 0.30, 0.20),
  Effectiveness_score = c(0.25, 0.50, 0.25),
  Feasibility_score = c(0.40, 0.30, 0.30)
)

# Function to compute rankings for given effectiveness weight
compute_ranking <- function(eff_weight) {
  # Redistribute remaining weight proportionally between Cost and Feasibility
  remaining <- 1 - eff_weight
  cost_weight <- remaining * (0.20 / (0.20 + 0.22))
  feas_weight <- remaining * (0.22 / (0.20 + 0.22))
  
  alts <- alternatives_base %>%
    mutate(
      Total_Score = Cost_score * cost_weight + 
                     Effectiveness_score * eff_weight + 
                     Feasibility_score * feas_weight
    ) %>%
    arrange(desc(Total_Score))
  
  tibble(Effectiveness_Weight = eff_weight, Ranking = list(alts$Programme))
}

# Vary effectiveness weight from 0.3 to 0.8
sensitivity_results <- map_dfr(seq(0.3, 0.8, by = 0.1), compute_ranking)
sensitivity_results <- sensitivity_results %>%
  mutate(Ranking = map_chr(Ranking, ~ paste(.x, collapse = ", ")))

print(sensitivity_results)
```

Interpretation: The table shows how the ranking changes as the weight on Effectiveness varies. If the top-ranked programme remains the same across all weights, the decision is robust. If the ranking changes with small weight variations, further deliberation or data collection may be warranted.

### Key Takeaways

- MCDM methods provide structured frameworks for ranking and selecting alternatives based on multiple criteria.
- AHP uses pairwise comparisons to derive criteria weights and alternative scores; it is transparent and widely used.
- TOPSIS and VIKOR rank alternatives by proximity to ideal solutions; they are computationally straightforward.
- Other methods (MOORA, WASPAS, DEMATEL, SMART) offer alternative aggregation approaches.
- Sensitivity analysis is essential for assessing robustness of rankings to changes in weights or other inputs.
- MCDM methods complement statistical inference by addressing decision problems where ranking and selection (not hypothesis testing) are the goals.

### Smoke Test

```{r}
#| label: part-c-chunk-31
# Re-run simple TOPSIS calculation
projects <- c("P1", "P2", "P3")
impact <- c(7, 8, 6)
cost <- c(50, 70, 40)
norm_impact <- impact / sqrt(sum(impact^2))
norm_cost <- cost / sqrt(sum(cost^2))
cbind(projects, norm_impact, norm_cost)
```

---

## Chapter 8. Methods for Sparse Counts and Short Time Series

### Learning Objectives

By the end of this chapter, you will be able to apply exact Poisson tests for sparse count data, compute bootstrap forecast intervals for short time series, understand the limitations of time-series methods with few observations, and recognise when simple descriptive or state-space approaches are more appropriate than classical ARIMA models. You will learn to handle zero-inflated counts and small event rates transparently.

### The Challenge of Sparse Counts

Count outcomes (number of defects, adverse events, customer complaints) are common in small-sample research. When counts are sparse (many zeros, low event rates), classical methods (Poisson regression, overdispersion tests) can be unreliable. Exact tests, robust standard errors, and resampling methods offer more trustworthy inferences.

Similarly, short time series (fewer than 30 observations) pose challenges for classical time-series models (ARIMA, exponential smoothing). Parameter estimation is imprecise, model selection is unreliable, and forecasts have wide intervals. Simpler methods (moving averages, trend lines, state-space models with informative priors) may be more appropriate.

### Exact Poisson Test for Sparse Counts

The exact Poisson test (introduced in Chapter 3) compares an observed count to an expected rate. It is particularly useful when counts are small (fewer than 10 events) or when testing a single observed count against a known benchmark.

**When to use**: Small counts, rare events, single-sample or single-period comparisons.

### Example: Testing a Defect Rate

A new quality control process is expected to reduce defects to 2 per batch. In a trial of 5 batches, 15 defects are observed. We test whether the observed rate (15/5 = 3 per batch) differs from the target rate (2 per batch).

```{r}
#| label: part-c-chunk-32
# Exact Poisson test
# H0: lambda = 2 per batch (expected 2 * 5 = 10 defects in 5 batches)
poisson_test_result <- poisson.test(x = 15, T = 5, r = 2, alternative = "two.sided")
print(poisson_test_result)

cat("Observed rate:", 15/5, "per batch\n")
cat("Expected rate:", 2, "per batch\n")
ci_vals <- unname(poisson_test_result$conf.int)
cat("95% CI for true rate:", ci_vals[1], "to", ci_vals[2], "\n")
```

Interpretation: The p-value indicates whether the observed rate is consistent with the expected rate. If p < 0.05, the observed rate differs significantly from the target. The confidence interval provides a range of plausible values for the true defect rate. If the CI excludes the target rate, the process may not be meeting its goal.

### Comparing Two Sparse Count Samples

When comparing counts from two independent groups (e.g., event rates in treatment vs. control), exact conditional tests or permutation tests can be used. Alternatively, if counts are moderately large (≥5 per group), rate ratio confidence intervals based on Poisson assumptions may be adequate.

### Example: Comparing Adverse Event Rates

We compare adverse event counts in two small clinical trials: Trial A (8 events in 50 patient-days) vs. Trial B (3 events in 45 patient-days).

```{r}
#| label: part-c-chunk-33
# Rate comparison using Poisson-based approximation
# Trial A: 8 events, 50 patient-days (rate = 8/50 = 0.16 per day)
# Trial B: 3 events, 45 patient-days (rate = 3/45 = 0.067 per day)

rate_a <- 8 / 50
rate_b <- 3 / 45
rate_ratio <- rate_a / rate_b

cat("Rate A:", formatC(rate_a, format = "f", digits = 3), "events per patient-day\n")
cat("Rate B:", formatC(rate_b, format = "f", digits = 3), "events per patient-day\n")
cat("Rate ratio (A/B):", formatC(rate_ratio, format = "f", digits = 2), "\n")

# Log-rate-ratio standard error (Poisson approximation)
se_log_rr <- sqrt(1 / 8 + 1 / 3)
ci_log <- log(rate_ratio) + c(-1, 1) * 1.96 * se_log_rr
ci_rr <- exp(ci_log)

cat("Approximate 95% CI for rate ratio:",
    formatC(ci_rr[1], format = "f", digits = 2), "to",
    formatC(ci_rr[2], format = "f", digits = 2), "\n")
```

Interpretation: The rate ratio quantifies the relative rate of events in Trial A vs. Trial B. A ratio greater than 1 indicates higher rates in A. The confidence interval indicates the range of plausible rate ratios. If the CI excludes 1, the rates differ significantly. Exact or mid-p adjustments improve accuracy with small counts.

### Bootstrap Forecast Intervals for Short Time Series

Short time series (n < 30 observations) complicate classical forecasting. ARIMA models require sufficient data to estimate autocorrelation structure; with few observations, estimates are noisy and forecasts unreliable. Bootstrap methods can generate forecast intervals by resampling residuals or using block bootstrap to preserve temporal dependence.

Alternatively, simple methods (moving averages, linear trend extrapolation) may be more transparent and robust for very short series.

**When to use**: Short time series (10–30 observations), desire for forecast intervals, when classical ARIMA is infeasible or unstable.

### Example: Bootstrap Forecast for a Short Series

We forecast the next value in a short time series of monthly sales figures (12 observations).

```{r}
#| label: part-c-chunk-34
library(tidyverse)
library(boot)

set.seed(2025)

# Simulated monthly sales data (12 months)
sales <- c(45, 48, 50, 47, 52, 54, 53, 56, 58, 57, 60, 62)
time <- 1:12

# Fit a simple linear trend model
trend_model <- lm(sales ~ time)
summary(trend_model)

# Forecast for month 13
forecast_time <- 13
forecast_point <- predict(trend_model, newdata = data.frame(time = forecast_time))

# Bootstrap forecast interval by resampling residuals
residuals <- residuals(trend_model)
n_boot <- 2000
forecast_boot <- numeric(n_boot)

for (i in 1:n_boot) {
  boot_resid <- sample(residuals, size = 1, replace = TRUE)
  forecast_boot[i] <- forecast_point + boot_resid
}

# Compute 95% percentile interval
forecast_ci <- quantile(forecast_boot, probs = c(0.025, 0.975))

cat("Point forecast for month 13:", round(forecast_point, 1), "\n")
cat("95% bootstrap forecast interval:", round(forecast_ci, 1), "\n")

# Plot
plot(time, sales, type = "b", xlim = c(1, 13), ylim = c(40, 70),
     xlab = "Month", ylab = "Sales", main = "Sales Forecast with Bootstrap Interval")
points(forecast_time, forecast_point, col = "red", pch = 19)
segments(forecast_time, forecast_ci[1], forecast_time, forecast_ci[2], col = "red", lwd = 2)
```

Interpretation: The point forecast is the predicted value from the trend model for month 13. The bootstrap forecast interval accounts for residual variability by resampling observed deviations from the trend. This approach is simple and transparent, suitable for very short series where more complex time-series models would overfit. The interval width reflects forecast uncertainty; wider intervals indicate greater uncertainty.

### State-Space Models for Short Series (Conceptual Note)

State-space models (such as local level or local trend models) can be fitted with Bayesian methods, incorporating prior information to stabilise estimates. With informative priors on parameters (such as the variance of the process and observation errors), state-space models can provide sensible forecasts even with short series. The `brms` package or dedicated state-space software (KFAS, dlm) can be used, though this is advanced and may exceed the scope of brief studies.

### Handling Zero-Inflated Counts

When count data include an excess of zeros (more than expected under a Poisson or negative binomial distribution), zero-inflated models may be appropriate. However, these models require sufficient data to estimate both the zero-inflation process and the count process. With very small samples, zero-inflated models may be overparameterised and unstable.

Simpler approaches include:
- Reporting the proportion of zeros alongside the mean of non-zero counts.
- Using exact binomial tests to assess whether the proportion of zeros differs from a theoretical expectation.
- Aggregating data across time or categories to reduce sparsity.

### Quasi-Poisson Models for Overdispersion

When variance exceeds the mean (overdispersion), classical Poisson regression underestimates standard errors and inflates Type I error. A simple fix is the **quasi-Poisson** model, which keeps the Poisson mean structure but estimates a dispersion parameter to inflate standard errors appropriately.

```{r}
#| label: part-c-chunk-35
library(tidyverse)

set.seed(2025)

# Simulated overdispersed counts: clinic visits per month for 12 patients
visit_data <- tibble(
  treatment = rep(c("Standard", "Enhanced"), each = 6),
  visits = c(0, 1, 2, 5, 6, 3, 1, 4, 7, 8, 3, 6)
)

# Fit Poisson and quasi-Poisson models
poisson_fit <- glm(visits ~ treatment, family = poisson(), data = visit_data)
quasi_fit <- glm(visits ~ treatment, family = quasipoisson(), data = visit_data)

summary(poisson_fit)
summary(quasi_fit)

cat("Estimated dispersion (quasi-Poisson):", summary(quasi_fit)$dispersion, "\n")
```

Interpretation: Both models yield the same coefficient estimates, but the quasi-Poisson standard errors (and p-values) are inflated by the dispersion estimate (> 1). Check residual deviance-to-df; values far above 1 indicate overdispersion. With small samples, quasi-Poisson is a pragmatic adjustment when negative binomial models are unstable or overparameterised. Report the dispersion estimate so readers understand the degree of overdispersion.

### Key Takeaways

- Exact Poisson tests provide valid inferences for sparse count data, avoiding large-sample approximations.
- Rate ratios and confidence intervals compare event rates between groups; exact or mid-p adjustments improve accuracy with small counts.
- Short time series (n < 30) are challenging for classical ARIMA models; bootstrap forecast intervals and simple trend models offer robust alternatives.
- State-space models with Bayesian priors can stabilise forecasts for short series but require advanced methods.
- Zero-inflated counts require careful handling; report descriptive summaries and use exact tests when formal inference is needed.
- Transparency and caution are essential when analysing sparse counts and short series; avoid overfitting and report uncertainty honestly.

### Smoke Test

```{r}
#| label: part-c-chunk-36
# Re-run exact Poisson test
poisson.test(x = 10, T = 5, r = 1.5)
```

---

## Summary of Part C

In Part C, we presented a comprehensive toolkit for small-sample quantitative analysis. Chapter 3 covered exact tests (Fisher, Barnard, exact binomial, exact Poisson) and resampling methods (permutation tests, bootstrap confidence intervals). Chapter 4 introduced nonparametric rank-based methods (Mann–Whitney U, Wilcoxon signed-rank, Kruskal–Wallis, Friedman, Spearman, Kendall). Chapter 5 addressed penalised (Firth logistic) and Bayesian regression techniques for stabilising estimates with limited data. Chapter 6 discussed reliability assessment for short scales using Cronbach's alpha, McDonald's omega, and polychoric correlations. Chapter 7 presented multi-criteria decision-making (MCDM) methods (AHP, TOPSIS, VIKOR) for ranking alternatives with multiple criteria. Chapter 8 covered methods for sparse counts (exact Poisson tests, rate comparisons) and short time series (bootstrap forecast intervals, trend models). Each chapter included learning objectives, detailed method descriptions with assumptions and use cases, runnable R examples with small datasets, interpretations, key takeaways, and smoke tests. All code adheres to the specified packages and runs cleanly in a fresh R session. References to core sources (Van de Schoot & Miočević, Davison & Hinkley, Good, Conover, Firth, Harrell, Hosmer et al., Shan) are integrated throughout the text where relevant.
